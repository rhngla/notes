[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "Transformers and foundational models for X\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\nRohan Gala\n\n\n\n\n\n\n  \n\n\n\n\nNotation for conditional distributions\n\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nRohan Gala\n\n\n\n\n\n\n  \n\n\n\n\nConcepts in dimensionality reduction\n\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nRohan Gala\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/spaces.html",
    "href": "posts/spaces.html",
    "title": "Concepts in dimensionality reduction",
    "section": "",
    "text": "We’ll start with some definitions - the goal is to situate ourselves with:\nUltimately we want to construct analysis tools i.e. operations to manipulate such objects constructed from data, and discover relationships without violating definitions and underlying assumptions (or at least doing so knowingly). Further, through these definitions we notice somewhat hierarchical relationships of such objects, spaces, and rules. Some definitions are more abstract, and some are special cases with additional rules. Being aware of where in the abstraction hierarchy we operate is helpful to manage complexity while analyzing data."
  },
  {
    "objectID": "posts/spaces.html#sec-metricspace",
    "href": "posts/spaces.html#sec-metricspace",
    "title": "Concepts in dimensionality reduction",
    "section": "1.1 Metric and metric space",
    "text": "1.1 Metric and metric space\nA distance metric d is defined on set M \nd:M \\times M \\rightarrow \\mathbb{R}\n\nA distance metric is sometimes called just “distance” or just “metric”. For all x,y,z \\in M the following relationships hold for any valid distance metric: \n\\begin{aligned}\nd(x, y) &\\geq 0 \\\\\nd(x, y) &=0 \\Longleftrightarrow x=y \\\\\nd(x, y) &=d(y, x) \\\\\nd(x, z) &\\leq d(x, y)+d(y, z)\n\\end{aligned}\n\\tag{1}\nA metric space is defined as the ordered pair (M,d), where M is a set, and d is a distance metric. Reference to the metric d is occasionally omitted understood from the context, and the metric space is denoted simply by (M).\n\n\nCosine distance, defined for vectors x and y in Euclidean space as 1-\\cos\\theta where \\theta = \\frac{x \\cdot y}{\\|x\\| \\|y\\|}, is not a distance metric in general (e.g. d(x, 2x) = 0, but x\\neq 2x). However, it becomes a valid distance metric if we restrict our set of vectors to only those that have a fixed norm (\\| x \\| = c)."
  },
  {
    "objectID": "posts/spaces.html#sec-field",
    "href": "posts/spaces.html#sec-field",
    "title": "Concepts in dimensionality reduction",
    "section": "1.2 Field",
    "text": "1.2 Field\nA field is a set F which includes 0 and 1 and binary operations of addition + and multiplication \\cdot such that for all elements x,y,z \\in F the following hold:\n\n\\begin{aligned}\n0 + x &= x \\\\\nx + (-x) &= 0, \\quad -x \\in F \\\\\nx + y &= y + x \\\\\nx + (y + z) &= (x + y) + z \\\\\n1 \\cdot x &= 1 \\\\\nx^{-1} \\cdot x &= 1, \\quad \\forall x \\neq 0 \\\\\nx \\cdot y &= y \\cdot x \\\\\nx \\cdot (y \\cdot z) &= (x \\cdot y) \\cdot z \\\\\nx \\cdot (y + z) &= (x \\cdot y) + (x \\cdot z) \\\\\n\\end{aligned}\n\\tag{2}\nWe typically deal with the real field \\mathbb{R} or complex field \\mathbb{C}"
  },
  {
    "objectID": "posts/spaces.html#sec-vectorspace",
    "href": "posts/spaces.html#sec-vectorspace",
    "title": "Concepts in dimensionality reduction",
    "section": "1.3 Vector space",
    "text": "1.3 Vector space\nA vector space over a field F is a set V together with operations of addition and scalar multiplication) such that for all vectors x,y,z \\in V and all scalars \\alpha, \\beta \\in \\mathbb{F} the following hold:\n\n\\begin{aligned}\nx + y &= y + x \\\\\n(x + y) + z &= x + (y + z) \\\\\nx + 0 &= x \\\\\nx + (-x) &= 0 \\\\\n\\alpha (x + y) &= \\alpha x + \\alpha y \\\\\n(\\alpha + \\beta) x &= \\alpha x + \\beta x \\\\\n\\alpha (\\beta x) &= (\\alpha \\beta) x \\\\\n1x &= x\n\\end{aligned}\n\\tag{3}"
  },
  {
    "objectID": "posts/spaces.html#sec-norm",
    "href": "posts/spaces.html#sec-norm",
    "title": "Concepts in dimensionality reduction",
    "section": "1.4 Norm",
    "text": "1.4 Norm\nA norm on a vector space V is a function w:V \\rightarrow \\mathbb{R} that satisfies: \n\\begin{aligned}\nw(x) &\\geq 0 \\\\\nw(x) &= 0 \\implies x = 0 \\\\\nw(\\alpha x) &= \\lvert\\alpha\\rvert w(x) \\\\\nw(x+y) &\\leq w(x) + w(y)\n\\end{aligned}\n\\tag{4}\n\nNorms induce a distance metric on the vector space, d (x, y) = w (x − y), and such a normed vector space is therefore also a metric space.\nNot all metric spaces are normed vector spaces. That is, there exist metric spaces that are not normed vector spaces.\nA metric can define an induced norm w over a vector space (via w(x) = d(x,0)) if it satisfies the following: \n\\begin{aligned}\nd(w,v) &= d(u+w, u+v) \\\\\nd(\\alpha u,\\alpha v) &= \\lvert\\alpha\\rvert d(u,v)\n\\end{aligned}\n\\tag{5}"
  },
  {
    "objectID": "posts/spaces.html#sec-innerproductspace",
    "href": "posts/spaces.html#sec-innerproductspace",
    "title": "Concepts in dimensionality reduction",
    "section": "1.5 Inner product space",
    "text": "1.5 Inner product space\nAn inner product space is a vector space V over the field F together with an inner product. An inner product is a map \\langle \\cdot ,\\cdot \\rangle :V \\times V \\to F, that satisfies the properties for all vectors x,y,z\\in V and all scalars a,b \\in F. \n\\begin{aligned}\n\\langle x,y\\rangle &={\\overline {\\langle y,x\\rangle }} \\\\\n\\langle ax+by,z\\rangle &=a \\langle x,z\\rangle +b\\langle y,z\\rangle \\\\\n\\langle x,x\\rangle &\\geq 0 \\\\\n\\langle x,x\\rangle = 0 &\\iff x = 0 \\\\\n\\end{aligned}\n\\tag{6}\n\nConjugate symmetry implies \\langle x,x\\rangle \\in \\mathbb{R}.\nIn many applications, F is \\mathbb{R}, and conjugate symmetry reduces to symmetry.\nEvery inner product space induces a norm, called its canonical norm: \\|x\\|={\\sqrt {\\langle x,x\\rangle }}\n\n\n\np-norm (or L_p-norm) is defined over a vector space as: \n\\| x \\| = \\sqrt[p]{\\sum_{i=1}^n x_i^p}\n\nThe distance induced by the p-norm is also known as the Minkowski distance of order p.\n\nEuclidean space\nAn n-dimensional Euclidean space is a particular metric space. In fact, it is a vector space equipped with an inner product. Vectors in Euclidean space are denoted as x \\in \\mathbb{R}^n.\n\nThe inner product for two vectors x and y is defined as \\langle x, y \\rangle = x \\cdot y, where x \\cdot y = \\sum_{i=1}^n x_i y_i.\nThe inner product induces a norm (also known as 2-norm, following the definition of the more general case of p-norm) \\| x \\| = \\sqrt{x \\cdot x}.\nThe norm induces a distance metric: d_{\\textrm{euclidean}}(x,y) = \\| x - y \\|_2.\n\nWe often assume we are working with a Euclidean metric space (\\mathbb{R}^n, d_{\\textrm{euclidean}}) without explicitly stating definitions. However, considering these definitions make it easier to understand the more general case of a Riemannian manifold."
  },
  {
    "objectID": "posts/spaces.html#reimannian-manifold",
    "href": "posts/spaces.html#reimannian-manifold",
    "title": "Concepts in dimensionality reduction",
    "section": "1.6 Reimannian Manifold",
    "text": "1.6 Reimannian Manifold\nTextbooks on differential geometry, e.g. Andrews (2020) would provide a more formal entry point and description for Manifolds. I’d like to point out connections here to Euclidean geometry to build some intuition first. In geometry taught in school, Euclidean geometry is introduced through its axioms. In particular, relaxing the fifth postulate (also referred at as the parallel postulate) leads to ideas of spherical and hyperbolic geometry, which can also be viewed as special cases of Reimannian manifolds that have constant, global curvature.\nRiemannian manifolds allow for local curvature. This local curvature is allowed to vary smoothly from one position to the next along the manifold. The local region around each position can be treated as a Euclidean space (we’ll introduce the notion of tangent space for this). We can choose a coordinate system locally to account for the local curvature, and then keep using intuitions for Euclidean spaces there.\nHowever, as we move along manifold, we’d like to modify the co-ordinate system itself to account for the local curvature. Because we are working with a smoothly varying curvature, we can do this using smooth transformations of the co-ordinate system - for which we use tools from calculus.\n\n\n\n\n\n\nImportant\n\n\n\n\n\nSections below are under construction!\n\n\n\n\nCoordinate transformations\n\nDiscuss Jacobians\nCoordinate transformations in \\mathbb{R}^n\nInvertible transformations (connection to normalizing flows)\n\n\n\nNotation\nA Riemannian manifold is a tuple (M,g) where M is a set of points and g refers to inner products on vector spaces that we define at each point p \\in M. Vectors that are tangent to the point p make up the tangent space denoted by T_{p} M.\nLet point p and a local co-ordinate system denoted as (x^1,...,x^n)\nThe set where we define each object as a differential operator \\mathbf{A} = (A^1\\frac{\\partial}{\\partial x^1},...,A^n\\frac{\\partial}{\\partial x^n}), s.t. A^i \\in \\mathbb{R}. Such a set satisfies definition of a vector space. Moreover, each object is tangent to the manifold because of the partial derivatives. Thus, we now have a way to specify objects belonging to the tangent space T_{p} M.\nConsider the change of co-ordinates from (x^1,...,x^n) to (y^1,...,y^n), where y^i = f^i(x^1,...,x^n). The Jacobian matrix is defined as J_{ij} = \\frac{\\partial y^i}{\\partial x^j}.\n\n\nThe differential operators actually define a vector field; each vector field is a vector space on itself.\nWe denote the inner product at p \\in M as the map: g_{p}: T_{p} M \\times T_{p} M \\rightarrow \\mathbb{R}.\nFor two vectors \\mathbf{A} and \\mathbf{B} in T_{p} M, \\langle \\mathbf{A}, \\mathbf{B} \\rangle _{g_{p}}, the inner product\nThrough the distance metric induced by the canonical norm, it is easy to work out that Euclidean space is a special case of a Reimannian Manifold where g_{p} is an identity matrix.\n\n\nLocal\nRecall that an inner product induces a norm, and the norm induces a distance metric. Since the inner product is locally defined at each point, we can think of the distance metric as being defined at each point. Using this local notion of distance, we can calculate distances between any two points on the manifold through integration. This we have a set of points M, where we can calculate distances between any two points\nNote the similarity to definition of a metric space, Section 1.1.\nNote that the metric tensor is an inner product on the tangent space T_{p}M. Moreover, the tangent space is a vector space. Recall that any inner product induces a norm, which in turm induces a distance metric. Thus, there is notion of a distance metric for each point on the manifold (and the reason why g_p is called a metric tensor).\n\nNote: For the Euclidean manifold, the metric tensor is the identity matrix.\n\nCategorizing geometric transforms:\n\nDiffeomorphic: Preserves angles, distances, and shape\nIsometric: Preserves angles and distances\nConformal: Preserves angles"
  },
  {
    "objectID": "posts/spaces.html#sec-highdim-curse",
    "href": "posts/spaces.html#sec-highdim-curse",
    "title": "Concepts in dimensionality reduction",
    "section": "2.1 Curse of high dimensionality",
    "text": "2.1 Curse of high dimensionality\nIntuition for properties of data in 2d / 3d space don’t carry over to high dimensional spaces.\n\nMost of the volume is at the surface of the hypersphere in high D\nThis is a consequence of the phenomena of “concentration of measure”\n\nFurther reading: Aggarwal, Hinneburg, and Keim (2001), Chapter 1 of Lee and Verleysen (2007)."
  },
  {
    "objectID": "posts/spaces.html#manifold-assumption",
    "href": "posts/spaces.html#manifold-assumption",
    "title": "Concepts in dimensionality reduction",
    "section": "2.2 Manifold assumption",
    "text": "2.2 Manifold assumption\n\nMany methods attempt to use point samples in high dimensional space to parametrize a low dimensional manifold.\n\nParaphrasing a nice introduction in Ganea (2019), we assume that data lies on a manifold of much smaller dimension (latent) than the ambient dimension (measured). This enables learning and generalization to unseen data points. We further assume that it is a metric space (Y, d_Y) and has a smooth differentiable structure that allows learning via optimization.\nThus we are interested in Riemannian manifolds of intrinsic dimension d \\leq n. The hope is we can achieve this by learning an a mapping function f : X \\rightarrow Y from the input high dimensional data space to the low-dimensional representation space."
  },
  {
    "objectID": "posts/spaces.html#dimensionality-reduction-1",
    "href": "posts/spaces.html#dimensionality-reduction-1",
    "title": "Concepts in dimensionality reduction",
    "section": "2.3 Dimensionality reduction",
    "text": "2.3 Dimensionality reduction\nThese set of notes by Drew Wilimitis also contain python implementation of common methods.\nVarious computational methods have been designed to uncover a lower dimension manifold that preserve some notion of distances and structure of data observed in high dimension.\n\nMDS: preserve pairwise distances (using some common metric, e.g. Euclidean)\nIsomap: preserve pairwise geodesic distances (evaluated through nearest neighbor hops. Assumes a metric to define nearest neighbor)\nDiffusion maps: consider notion of reachability of distant points by constructing a diffusion process (Coifman et al. 2005; De la Porte et al. 2008)\nLaplacian eigenmaps, T-sne, Umap etc. are other nonlinear methods\n\nRandom projections, random sparse projections can be considered as baselines. Constructing such transforms has links to the Johnson Lindenstrauss lemma, and there is research on fast construction of useful random transforms. e.g. transformer architectures.\nTODO: - Similarities and distances discussion in Ganea (2019) - Proof in Dasgupta and Gupta (2003) for the Johnson Lindenstrauss lemma. - Nearest component analysis - Unifying dimensionality reduction, graph partitioning, and data set parameterization - Sparse random projections - Include references from this twitter thread\n\nUsing Jaccard similarity\nConsider representations of point p embedded in two metric spaces $(X, d_{X}) and in (Y, d_{Y}). We denote the k-nearest neighbors of p in the two metric spaces as K_{X}(p) and K_{Y}(p). The Jaccard similarity between the neighborhood sets is defined as: \nJ_{XY}(p) = \\frac{|K_{X}(p) \\cap K_{Y}(p)|}{|K_{X}(p) \\cup K_{Y}(p)|}"
  },
  {
    "objectID": "posts/notation_dist.html",
    "href": "posts/notation_dist.html",
    "title": "Notation for conditional distributions",
    "section": "",
    "text": "Continuous probability distributions are commonly specified through the probability density function p(x). A distribution with parameters \\theta can be viewed as a conditional distribution p(x|\\theta). In particular, the following notation is interchangeably used in the literature: p_\\theta(x) \\equiv p(x;\\theta) \\equiv p(x|\\theta)\nAny subtlety in terms of whether the notation choice depends on whether \\theta is a parameter (i.e. a constant without a prior distribution associated with it), or is a random variable is up to the author to clarify and use consistently. See community discussions Fraïssé (2017), Learner (2011), and Bartlett (2015).\nAt times, authors also drop the dependence on \\theta and simply write p_{\\theta}(x) as p(x). This can be a source of a lot of confusion, and both authors and readers should be careful to prevent misundertanding.\n\nReferences\n\n\nBartlett, Jonathan. 2015. “Bayesian Inference: Are Parameters Fixed or Random?” 2015. https://thestatsgeek.com/2015/04/22/bayesian-inference-are-parameters-fixed-or-random/.\n\n\nFraïssé. 2017. “What Does a Semicolon Denote in the Context of Probability and Statistics?” 2017. https://math.stackexchange.com/questions/2559085/what-does-a-semicolon-denote-in-the-context-of-probability-and-statistics.\n\n\nLearner. 2011. “What Is the Difference Between the Vertical Bar and Semi-Colon Notations?” 2011. https://stats.stackexchange.com/questions/10234/what-is-the-difference-between-the-vertical-bar-and-semi-colon-notations."
  },
  {
    "objectID": "posts/transformers.html",
    "href": "posts/transformers.html",
    "title": "Transformers and foundational models for X",
    "section": "",
    "text": "Here I keep track of sources that have helped me understand this topic better.\nIn my view, the main concepts to be familiar with are:\n\nGenerative modeling for sequences\nAttention mechanism, and transformer architecture\nMulti-task learning\n\nRaschka (2023) outlines some key papers and developments in the context of large language models, and Weng (2018) provides an overview of the attention mechanism, and some historical notes for the peculiar keys, values, queries nomenclature used in Vaswani et al. (2017).\n\nGenerative sequence modeling\nLanguage has a natural sequential structure. The generative modeling task is to learn a probability distribution over sequences of words.\nLet different words be specific values of n random variables. Then the distribution we’d like to learn from data is p(s_1, s_2, ..., s_n). The sequential ordering motivates a particular factorization of this joint distribution:\np(s_1, s_2, ..., s_n) = p(s_1) p(s_2 | s_1) p(s_3 | s_1, s_2) ... p(s_n | s_1, s_2, ..., s_{n-1})\nIt’s important to understand that the notion of ‘words’ is arbitrary for such modeling. Even for language, one can break up words at the character or even byte level (e.g. see byte pair encoding). This is a crucial think to keep in mind for the notions of multi-task learning and foundational models.\nWhen generating new sequences, we are recursively sampling particular conditional distributions, e.g. p(s_i | s_1, ... s_{i-1}) to ultimately look like we are sampling from p(s_i, ... s_n | s_{i-k}, ..., s_{i-1}).\nThis looks like p(\\textrm{outputs}|\\textrm{inputs}). We’ll refer to this in the section on multi-task learning.\nPhuong and Hutter (2022) describe transformer-based-algorithms without implementation details, instead focusing on descriptions with consistent and formal notation. With the maze of transformer variants that exist, this resource offers a coherent picture of the landscape.\n\n\nAttention mechanism and transformer architecture\nAlex Smola (2019) motivates the attention mechanism with kernel regression, and Schlag, Irie, and Schmidhuber (2021) link attention to fast weight programming.\nSchlag, Irie, and Schmidhuber (2021) explore the linear version of attention in the context of associative memory and explore memory capacity of such models with different choices of kernels, activation functions, etc., connecting to a rich literature on such investigations dating all the way back to Cover (1965).\nA self-attention layer maps an input sequence of length T, denoted as (x_1,...,x_T), x_i \\in \\mathbb{R}^{d_\\textrm{in} \\times 1} to an output sequence (y_1,...,y_T), y_i \\in \\mathbb{R}^{d_\\textrm{out} \\times 1} as\n\n\\begin{align}\nk_i, v_i, q_i &= Kx_i,Vx_i,Qx_i \\\\\nA_{ij} &= \\textrm{softmax}(k_i^\\top q_j) \\\\\ny_{i} &= \\sum_j{A_{ij}v_j}\n\\end{align}\n\nK, Q \\in \\mathbb{R}^{T \\times d_\\textrm{kq}} and V \\in \\mathbb{R}^{T \\times d_\\textrm{out}}, are trainable weights. \\textrm{softmax} ensures \\sum_j{A_{ij}=1} \\space \\forall \\space i, and A \\in \\mathbb{R}_{\\geq 0}^{T \\times T} is the attention matrix.\nSetting certain entries of the A_{ij} to zero is referred to as masking, and it has the effect of disallowing particular pairwise-interactions between the input sequence elements in contributing to a particular output.\nFor language, setting the upper triangular entries (including the diagonal) to be zero is a common choice. This mask pattern ensures that y_i is only based on (x_1, ..., x_{i-1}).\nIn the absence of masking, each output takes on the form y_j = \\sum_i{g_j(x_i)}, for some function g_j. This form renders it invariant to ordering i.e. permutations of the input sequence.\nOn a related note, Zaheer et al. (2017) had shown that for universal function approximators (e.g. neural networks) h and g, a function f of the form f(x_1, ... x_n) = h(\\sum_i(g(x_i))) is a universal approximator for any permutation invariant function.\nI’ve ignored scaling and nonlinearities for simplicity in the description above. Aside from the attention mechanism, the transformer architecture includes residual connections, layer normalization, and scaling in the calculation of the attention matrix. These conceptually straightforward operations turn out to be crucial in practice to prevent gradients from exploding or vanishing.\nAt this point, I’d highly recommend the implementation walkthrough by Karpathy (2023). He’s a skilled teacher and watching him live-code is instructive.\n\n\nMulti-task learning\nIn one version (e.g. GPT and variants), these transformer models are trained to simply predict the next word. That is, the model learns distributions p(s_i | s_1, ..., s_{i-1}) for all i \\in (1,...,n).\nSo far, a clear reason for why large language models seem magical is missing. Why bother increasing size of models, why invest so much in collecting the data and compute resources?\nI think Radford et al. (2019), and later Brown et al. (2020) provide this missing reason. The main realization is that for language, the task itself is a sequence of tokens. Tasks can take the form of “translate to german” or “write python code for” etc. Here the sequence of words that specify the task are themselves part of the input sequence.\nMoreover, the underlying input does not have to be “words”, but instead can be more parcellated and abstract byte pair encodings.\nThe implication is that while we learn p(output|input), we are also learning p(output | input, task), without needing task-specific architecture or training!\nIn my view, this is a central requirement for a meaningful notion of foundational models. The implicit multi-task learning that takes place is the reason why non-trivial zero shot performance is possible on a variety of tasks, even though the task is not explicitly specified while training.\n\n\nFoundation models\nThe term “foundational models” was popularized by Bommasani et al. (2021), and an accompanying workshop at Stanford University.\nBommasani et al. (2021) describe the notion of emergence (generalization over tasks, and therefore related to zero-shot performance) and homogenization (learning something useful for a variety of downstream tasks, and therefore the pre-training is amenable to fine-tuning).\nIn my view this is essentially echoing the central observations of Radford et al. (2019) and Brown et al. (2020).\nWith the hype about large language models gripping other domains, it is worth asking if the these ideas are applicable models in other domains that do not have the special place of language.\n\n\nTransformers for single-cell resolution datasets\nThe most developed, least noisy single-cell resolution -omics datasets are from transcriptomics. For most analyses, the dataset is a table rows are cells, columns are genes, and entries are the gene expression values. Additional metadata, e.g. disease state of the donor from which the cell was sampled, perturbations applied to the cell etc. may be part of the dataset.\nTypical tasks include determining identifiable cell types, specifying differentially expressed genes across conditions (aging, disease, perturbations), inferring developmental trajectories of particular cell types, identifying gene regulatory networks etc.\nThe issue that plagues so many of these analyses are related modeling of noise in such measurements, curse of high-dimensionality, absence of established ground-truth, and often lack of a mathematically precise definition for tasks.\nThe team at OpenProblems have continued to push for standardization and benchmarking through formal descriptions of tasks, e.g. Luecken et al. (2021). Still, the output of such efforts (both in terms of benchmarking datasets and state-of-the-art models) seems to have had limited influence on subsequent academic research.\nTranscriptomic data contain neither a natural sequential ordering, nor a large enough corpus of tasks to draw parallels with emergence and homogenization described above.\nRecent papers have applied transformer architectures to single cell genomics:\n\nYang et al. (2022)\nCui et al. (2022), rebranded as Cui et al. (2023)\nShen et al. (2023)\nTheodoris et al. (2023)\nHao et al. (2023)\n\n(to be continued)\n\n\nReferences\n\n\nAlex Smola, Ashton Zhang. 2019. “A Tutorial on Attention in Deep Learning.” 2019. https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4343.\n\n\nBommasani, Rishi, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, et al. 2021. “On the Opportunities and Risks of Foundation Models.” arXiv Preprint arXiv:2108.07258.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.\n\n\nCover, Thomas M. 1965. “Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition.” IEEE Transactions on Electronic Computers, no. 3: 326–34.\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, Nan Duan, and Bo Wang. 2022. “scFormer: A Universal Representation Learning Approach for Single-Cell Data Using Transformers.” bioRxiv, 2022–11.\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, and Bo Wang. 2023. “scGPT: Towards Building a Foundation Model for Single-Cell Multi-Omics Using Generative AI.” bioRxiv, 2023–04.\n\n\nHao, Minsheng, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi Cheng, Taifeng Wang, Jianzhu Ma, Le Song, and Xuegong Zhang. 2023. “Large Scale Foundation Model on Single-Cell Transcriptomics.” bioRxiv. https://doi.org/10.1101/2023.05.29.542705.\n\n\nKarpathy, Andrej. 2023. “Neural Networks: Zero to Hero.” 2023. https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ.\n\n\nLuecken, Malte D, Daniel Bernard Burkhardt, Robrecht Cannoodt, Christopher Lance, Aditi Agrawal, Hananeh Aliee, Ann T Chen, et al. 2021. “A Sandbox for Prediction and Integration of Dna, Rna, and Proteins in Single Cells.” In Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n\n\nPhuong, Mary, and Marcus Hutter. 2022. “Formal Algorithms for Transformers.” arXiv Preprint arXiv:2207.09238.\n\n\nRadford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. “Language Models Are Unsupervised Multitask Learners.” OpenAI Blog 1 (8): 9.\n\n\nRaschka, Sebastian. 2023. “Understanding Large Language Models.” 2023. https://magazine.sebastianraschka.com/p/understanding-large-language-models.\n\n\nSchlag, Imanol, Kazuki Irie, and Jürgen Schmidhuber. 2021. “Linear Transformers Are Secretly Fast Weight Programmers.” In International Conference on Machine Learning, 9355–66. PMLR.\n\n\nShen, Hongru, Jilei Liu, Jiani Hu, Xilin Shen, Chao Zhang, Dan Wu, Mengyao Feng, et al. 2023. “Generative Pretraining from Large-Scale Transcriptomes for Single-Cell Deciphering.” Iscience 26 (5).\n\n\nTheodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina R. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023. “Transfer Learning Enables Predictions in Network Biology.” Nature. https://doi.org/10.1038/s41586-023-06139-9.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” Advances in Neural Information Processing Systems 30.\n\n\nWeng, Lilian. 2018. “Attention? Attention!” Lilianweng.github.io. https://lilianweng.github.io/posts/2018-06-24-attention/.\n\n\nYang, Fan, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lu, and Jianhua Yao. 2022. “scBERT as a Large-Scale Pretrained Deep Language Model for Cell Type Annotation of Single-Cell RNA-Seq Data.” Nature Machine Intelligence 4 (10): 852–66.\n\n\nZaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. 2017. “Deep Sets.” Advances in Neural Information Processing Systems 30."
  }
]