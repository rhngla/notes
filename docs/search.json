[
  {
    "objectID": "posts/R.html",
    "href": "posts/R.html",
    "title": "R notes",
    "section": "",
    "text": "Miscellenous notes on R.\n\nInstalling R (Macs)\nI’ve had a good experience with rig on my M3 Mac through homebrew. Multiple R versions can be installed, and Rstudio can be directed to use a particular version.\nrig rstudio 4.3-arm64\nUse renv through Rstudio to manage environments. This page has a good overview of the main commands and functionality.\n# Use this to show the paths where package files are installed\npaths &lt;- .libPaths()\n\n# One of these paths can be provided directly to install.packages() \ninstall.packages(\"NMF\", lib = paths[1])\n\n\nCompiling R packages on Mac\nInstalling the GNU Fortran compiler following these instructions offered a straightforward fix for compilation errors for various packages on my M3 Mac. Attempts to use homebrew installations of gcc and gfortran failed for me.\n\n\nWriting functions\nPlace the file containing function definitions in the working directory. Wse getwd() to check and setwd() to change the working directory as needed.\n# Contents of `funcfile.R`\ndummyfunc_a&lt;-function(x){\n  y&lt;-x^2\n  return(y)\n}\n\ndummyfunc_b&lt;-function(x){\n  y&lt;-x^3\n  return(y)\n}\nMake dummyfunc_a and dummyfunc_b available in the environment by sourcing the file:\n# works in a script or console\nsource('funcfile.R')\n\n\nUsing debug tools\n\n1. browser()\nThis is a way to put a breakpoint manually anywhere in the code. The console will show Browse[1]&gt; and you can inspect the variables at that point in the code. While in the browser mode:\n\nn steps to next line\nc to continue till next breakpoint\nQ to quits the browser mode\n\n\n\n2. debug() and undebug():\n# source the file to make the function available\nsource('funcfile.R')\n\n# tell the debug the function of interest\ndebug(dummyfunc)\n\n# run the function - this will open a browser() at the first line of that function\ndummyfunc(1)\n\n# turn off the debugger when done\nundebug(dummyfunc)\nThe same shortcut keys as for browser() work in the debug() mode as well.\n\n\n\n\n\n\nImportant\n\n\n\n\n\nRemember to invoke source('funcfile.R') each time you make changes to the function file for debug() to work correctly.\n\n\n\n\n\n3. traceback():\nThis is helpful to get a full trace of function calls to that led to an error especially while debugging complex / unfamiliar packages."
  },
  {
    "objectID": "posts/stochastic-nodes.html",
    "href": "posts/stochastic-nodes.html",
    "title": "Gradients through stochastic nodes",
    "section": "",
    "text": "Calculating gradients through nodes in a computational graph that involve sampling from a distribution (for which we want to learn parameters) is not straightforward. Intuitively, this is because samples of the distribution don’t maintain a dependency with the distribution parameters. This prevents application of the chain rule used for backpropagation to obtain gradients w.r.t the distribution parameters.\nThere are two main strategies to get around this, one involving score function estimators, and another involving reparametrization. See (Schulman et al. 2015) for a formal treatment.\nThe score function estimator-based approach is more general but also more noisy. The approach suggests using surrogate loss functions, which ultimately permit gradient updates to distribution parameters.\nThe reparametrization trick (when possible) involves reformulating the distribution such that distribution parameters become part of a deterministic function applied to samples that are obtained from a fixed distribution.\n\nThe set up\nConsider a stochastic node, where we sample values x of a random variable X from a distribution with parameters \\phi. A transformation f with parameters \\theta (e.g. neural network) is applied to the samples to construct the loss function we want to optimize.\n\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi) &= \\mathbb{E}_{x \\sim p_{X|\\phi}}[f_{\\theta}(x)] = \\int{f_{\\theta}(x)p_{\\phi}(x)dx}\n\\end{aligned}\n\\tag{1}\nTo learn parameters \\theta and \\phi, we’ll require access to updates obtained through the gradients of the loss function w.r.t these parameters.\n\n\nUpdates to \\theta\nFirst we evaluate the gradient \\nabla_{\\theta}\\mathcal{L} required to update parameters \\theta of the deterministic function f_{\\theta}: \n\\begin{aligned}\n\\nabla_{\\theta}\\mathcal{L} &= \\nabla_{\\theta}\\mathbb{E}_{x \\sim p_{X|\\phi}}[f_{\\theta}(x)] \\\\\n&=  \\int{\\nabla_{\\theta} f_{\\theta}(x) p_{\\phi}(x)dx} \\\\\n&=  \\mathbb{E}_{x \\sim p_{X|\\phi}}[\\nabla_{\\theta}f_{\\theta}(x)]\n\\end{aligned}\n\\tag{2}\nWe see that the gradient in Eq. 2 can be cast as an expectation over samples from the distribution p_{X|\\phi}. So we can simply draw samples from the distribution and estimate the gradient, because \\nabla_{\\theta}f_{\\theta} is well-defined. In the extreme case, the estimate (albeit a noisy one) is obtained simply from a single sample x. So far so good!\n\n\nUpdates to \\phi\n\n\\begin{aligned}\n\\nabla_{\\phi}\\mathcal{L} &= \\nabla_{\\phi}\\mathbb{E}_{x \\sim p_{X|\\phi}}[f_{\\theta}(x)] \\\\\n&= \\int{f_{\\theta}(x) \\nabla_{\\phi} p_{\\phi}(x)dx}\n\\end{aligned}\n\\tag{3}\nIt may be tempting think of Eq. 3 as \\mathbb{E}_{x \\sim \\nabla_{\\phi} p_{X|\\phi}}[f_{\\theta}(x)] in an analogous manner. This would be wrong, because \\nabla_{\\phi} p_{\\phi}(x) not a valid probability density function!\n\n\n⚠️ A valid probability density function should be non-negative everywhere, and integrate to unity over its domain.\nInstead we’ll use an identity that is often referred to as the log-derivative trick to rewrite Eq. 3 as follows:\n\n\nLog-derivative trick is an application of the chain rule for derivatives: \n\\begin{aligned}\n\\nabla_{x}({\\log{h(x)}}) &= \\frac{1}{h(x)}\\nabla_{x}({h(x)}) \\\\\n\\implies \\nabla_{x}({h(x)}) &= h(x)\\nabla_x{(\\log{h(x)})}\n\\end{aligned}\n\n\n\\begin{aligned}\n\\int{f_{\\theta}(x) \\nabla_{\\phi} p_{\\phi}(x)dx} &= \\int{(f_{\\theta}(x)  \\nabla_{\\phi} \\log{p_{\\phi}(x)}) p_{\\phi}(x) dx} \\\\\n\\implies \\nabla_{\\phi}\\mathcal{L} &=\\mathbb{E}_{x \\sim p_{X|\\phi}}[f_{\\theta}(x) \\nabla_{\\phi}{\\log{p_{\\phi}(x)}}]\n\\end{aligned}\n\\tag{4}\nEq. 4 shows that the gradient is now over samples from the distribution p_{X|\\phi}. As before, in the extreme case this may be estimated with a single sample.\n\nScore function estimator and surrogate loss\nIf we view p_{\\phi}(x) as a likelihood function, then \\nabla_{\\phi}{\\log{p_{\\phi}(x)}} is referred to as the score function. The estimate of the gradient includes the score function, and so it is referred to as a score function estimator.\nEq. 4 also suggests \\mathcal{L}_{\\textrm{surrogate}}=~\\mathbb{E}_{x \\sim p_{X|\\phi}}{[f_{\\theta}(x)\\log{p_{\\phi}(x)}]} as a surrogate loss function for the stochastic node to obtain the correct update for \\phi when relying on auto-differentiation libraries.\n\n\n\n\n\n\nA simple check\n\n\n\n\n\nAs a side note, let’s think of the case where f_{\\theta}(x) maps to a constant value c. Then it means parameter \\phi does not influence the loss. No matter the sample drawn, the loss does not change. We can check that the gradient w.r.t. \\phi is zero in expectation Equation 5 for a score function estimator:\n\n\\begin{aligned}\n\\nabla_{\\phi}\\mathcal{L} &= \\int{c  \\nabla_{\\phi} (\\log{p_{\\phi}(x)}) p_{\\phi}(x) dx} \\\\\n&= c \\int{\\nabla_{\\phi} p_{\\phi}(x) dx} = c \\nabla_{\\phi} \\int{p_{\\phi}(x) dx} = 0\\\\\n\\end{aligned}\n\\tag{5}\n\n\n\n\n\nThe bias and variance over different samples of the data are an important characterization of estimators. The score function is of relevance here, e.g. see these Lecture notes and discussion about the Cramér-Rao bound.\n\n\nReparametrization trick\nAnother way to estimate the gradient through stochastic nodes is to use the reparametrization trick. This is possible when the distribution p_{X|\\phi} can be composed with a deterministic function g_\\psi applied to samples z drawn from a fixed distribution p_{Z}.\n\n\\begin{aligned}\n\\mathcal{L} &= \\mathbb{E}_{x \\sim p_{X|\\phi}}[f_{\\theta}(x)] = \\mathbb{E}_{z \\sim p_{Z}}[f_{\\theta}{(g_{\\psi}(z)}]\n\\end{aligned}\n\\tag{6}\nHere the parameters to learn are \\theta and \\psi, which both correspond to deterministic functions. The gradients can now be calculated through the chain rule, similar to Eq. 2.\nWe thus have a couple of different ways to learn parameters of probability distributions using auto-differentiation frameworks, making it possible to learn non-deterministic models.\n\n\n\nReferences\n\n\nSchulman, John, Nicolas Heess, Theophane Weber, and Pieter Abbeel. 2015. “Gradient Estimation Using Stochastic Computation Graphs.” Advances in Neural Information Processing Systems 28."
  },
  {
    "objectID": "posts/notation_dist.html",
    "href": "posts/notation_dist.html",
    "title": "Notation for random variables, distributions etc.",
    "section": "",
    "text": "Distribution\nFor a continuous random variable X, distribution typically refers to the probability density function (pdf) p_X. The pdf value at a particular value ox for random variable X is denoted as p_X(x). However it is also common to see p_X(x) being referred to as the distribution.\n\n\n\nNotation\nSame as\n\n\n\n\np(x)\np_{X}(X=x)\n\n\np(x|z)\np_{X|Z}(X=x|Z=z)\n\n\np(x,z)\np_{X,Z}(X=x,Z=z)\n\n\n\n\n\nParameters or random variables\nA distribution with parameters \\theta can be denoted as p_\\theta(x). In particular, the following notation is interchangeably used in literature: p_\\theta(x) \\equiv p(x;\\theta) \\equiv p(x|\\theta)\nAny subtlety in terms of whether the notation choice depends on whether \\theta is a parameter (i.e. a constant without a prior distribution associated with it), or is a random variable is up to the author to clarify and use consistently. See community discussions Fraïssé (2017), Learner (2011), and Bartlett (2015).\nAt times, authors also drop the dependence on \\theta and simply write p_{\\theta}(x) as p(x). This can be a source of a lot of confusion, and both authors and readers should be careful to prevent misundertanding.\n\n\nReferences\n\n\nBartlett, Jonathan. 2015. “Bayesian Inference: Are Parameters Fixed or Random?” 2015. https://thestatsgeek.com/2015/04/22/bayesian-inference-are-parameters-fixed-or-random/.\n\n\nFraïssé. 2017. “What Does a Semicolon Denote in the Context of Probability and Statistics?” 2017. https://math.stackexchange.com/questions/2559085/what-does-a-semicolon-denote-in-the-context-of-probability-and-statistics.\n\n\nLearner. 2011. “What Is the Difference Between the Vertical Bar and Semi-Colon Notations?” 2011. https://stats.stackexchange.com/questions/10234/what-is-the-difference-between-the-vertical-bar-and-semi-colon-notations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "R notes\n\n\n\n\n\n\nR\n\n\n\n\n\n\n\n\n\nMar 13, 2024\n\n\nRohan Gala\n\n\n\n\n\n\n\n\n\n\n\n\nInformation, entropy, and KL divergence\n\n\n\n\n\n\nfundamentals\n\n\nmath\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nRohan Gala\n\n\n\n\n\n\n\n\n\n\n\n\nGradients through stochastic nodes\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\nRohan Gala\n\n\n\n\n\n\n\n\n\n\n\n\nTransformers\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\nRohan Gala\n\n\n\n\n\n\n\n\n\n\n\n\nNotation for random variables, distributions etc.\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nRohan Gala\n\n\n\n\n\n\n\n\n\n\n\n\nConcepts in dimensionality reduction\n\n\n\n\n\n\nmath\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nRohan Gala\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/spaces.html",
    "href": "posts/spaces.html",
    "title": "Concepts in dimensionality reduction",
    "section": "",
    "text": "We’ll start with some definitions - the goal is to situate ourselves with:\nUltimately we want to construct analysis tools i.e. operations to manipulate such objects constructed from data, and discover relationships without violating definitions and underlying assumptions (or at least doing so knowingly). Further, through these definitions we notice somewhat hierarchical relationships of such objects, spaces, and rules. Some definitions are more abstract, and some are special cases with additional rules. Being aware of where in the abstraction hierarchy we operate is helpful to manage complexity while analyzing data."
  },
  {
    "objectID": "posts/spaces.html#sec-metricspace",
    "href": "posts/spaces.html#sec-metricspace",
    "title": "Concepts in dimensionality reduction",
    "section": "1.1 Metric and metric space",
    "text": "1.1 Metric and metric space\nA distance metric d is defined on set M \nd:M \\times M \\rightarrow \\mathbb{R}\n\nA distance metric is sometimes called just “distance” or just “metric”. For all x,y,z \\in M the following relationships hold for any valid distance metric: \n\\begin{aligned}\nd(x, y) &\\geq 0 \\\\\nd(x, y) &=0 \\Longleftrightarrow x=y \\\\\nd(x, y) &=d(y, x) \\\\\nd(x, z) &\\leq d(x, y)+d(y, z)\n\\end{aligned}\n\\tag{1}\nA metric space is defined as the ordered pair (M,d), where M is a set, and d is a distance metric. Reference to the metric d is occasionally omitted understood from the context, and the metric space is denoted simply by (M).\n\n\nCosine distance, defined for vectors x and y in Euclidean space as 1-\\cos\\theta where \\theta = \\frac{x \\cdot y}{\\|x\\| \\|y\\|}, is not a distance metric in general (e.g. d(x, 2x) = 0, but x\\neq 2x). However, it becomes a valid distance metric if we restrict our set of vectors to only those that have a fixed norm (\\| x \\| = c)."
  },
  {
    "objectID": "posts/spaces.html#sec-field",
    "href": "posts/spaces.html#sec-field",
    "title": "Concepts in dimensionality reduction",
    "section": "1.2 Field",
    "text": "1.2 Field\nA field is a set F which includes 0 and 1 and binary operations of addition + and multiplication \\cdot such that for all elements x,y,z \\in F the following hold:\n\n\\begin{aligned}\n0 + x &= x \\\\\nx + (-x) &= 0, \\quad -x \\in F \\\\\nx + y &= y + x \\\\\nx + (y + z) &= (x + y) + z \\\\\n1 \\cdot x &= 1 \\\\\nx^{-1} \\cdot x &= 1, \\quad \\forall x \\neq 0 \\\\\nx \\cdot y &= y \\cdot x \\\\\nx \\cdot (y \\cdot z) &= (x \\cdot y) \\cdot z \\\\\nx \\cdot (y + z) &= (x \\cdot y) + (x \\cdot z) \\\\\n\\end{aligned}\n\\tag{2}\nWe typically deal with the real field \\mathbb{R} or complex field \\mathbb{C}"
  },
  {
    "objectID": "posts/spaces.html#sec-vectorspace",
    "href": "posts/spaces.html#sec-vectorspace",
    "title": "Concepts in dimensionality reduction",
    "section": "1.3 Vector space",
    "text": "1.3 Vector space\nA vector space over a field F is a set V together with operations of addition and scalar multiplication such that for all vectors x,y,z \\in V and all scalars \\alpha, \\beta \\in \\mathbb{F} the following hold:\n\n\\begin{aligned}\nx + y &= y + x \\\\\n(x + y) + z &= x + (y + z) \\\\\nx + 0 &= x \\\\\nx + (-x) &= 0 \\\\\n\\alpha (x + y) &= \\alpha x + \\alpha y \\\\\n(\\alpha + \\beta) x &= \\alpha x + \\beta x \\\\\n\\alpha (\\beta x) &= (\\alpha \\beta) x \\\\\n1x &= x\n\\end{aligned}\n\\tag{3}"
  },
  {
    "objectID": "posts/spaces.html#sec-norm",
    "href": "posts/spaces.html#sec-norm",
    "title": "Concepts in dimensionality reduction",
    "section": "1.4 Norm",
    "text": "1.4 Norm\nA norm on a vector space V is a function w:V \\rightarrow \\mathbb{R} that satisfies: \n\\begin{aligned}\nw(x) &\\geq 0 \\\\\nw(x) &= 0 \\implies x = 0 \\\\\nw(\\alpha x) &= \\lvert\\alpha\\rvert w(x) \\\\\nw(x+y) &\\leq w(x) + w(y)\n\\end{aligned}\n\\tag{4}\n\nHere \\lvert\\alpha\\rvert denotes the absolute value of the scalar \\alpha.\nNorms induce a distance metric on the vector space, d (x, y) = w (x − y), and vector space equipped with a norm (i.e. normed vector space) is therefore also a metric space.\nNot all metric spaces are normed vector spaces. That is, there exist metric spaces that are not normed vector spaces.\nA metric can define an induced norm w over a vector space (via w(x) = d(x,0)) if it satisfies the following: \n\\begin{aligned}\nd(w,v) &= d(u+w, u+v) \\\\\nd(\\alpha u,\\alpha v) &= \\lvert\\alpha\\rvert d(u,v)\n\\end{aligned}\n\\tag{5}"
  },
  {
    "objectID": "posts/spaces.html#sec-innerproductspace",
    "href": "posts/spaces.html#sec-innerproductspace",
    "title": "Concepts in dimensionality reduction",
    "section": "1.5 Inner product space",
    "text": "1.5 Inner product space\nAn inner product space is a vector space V over the field F equipped with an inner product. An inner product is a map \\langle \\cdot ,\\cdot \\rangle :V \\times V \\to F, that satisfies the properties for all vectors x,y,z\\in V and all scalars a,b \\in F. \n\\begin{aligned}\n\\langle x,y\\rangle &={\\overline {\\langle y,x\\rangle }} \\\\\n\\langle ax+by,z\\rangle &=a \\langle x,z\\rangle +b\\langle y,z\\rangle \\\\\n\\langle x,x\\rangle &\\geq 0 \\\\\n\\langle x,x\\rangle = 0 &\\iff x = 0 \\\\\n\\end{aligned}\n\\tag{6}\n\nConjugate symmetry implies \\langle x,x\\rangle \\in \\mathbb{R}.\nIn many applications, F is \\mathbb{R}, and conjugate symmetry reduces to symmetry.\nEvery inner product space induces a norm, called its canonical norm: \\|x\\|={\\sqrt {\\langle x,x\\rangle }}\n\n\n\np-norm (or L_p-norm) is defined over a vector space as: \n\\| x \\| = \\sqrt[p]{\\sum_{i=1}^n x_i^p}\n\nThe distance induced by the p-norm is also known as the Minkowski distance of order p.\n\nEuclidean space\nAn n-dimensional Euclidean space is a particular metric space. In fact, it is a vector space equipped with an inner product. Vectors in Euclidean space are denoted as x \\in \\mathbb{R}^n.\n\nThe inner product for two vectors x and y is defined as \\langle x, y \\rangle = x \\cdot y, where x \\cdot y = \\sum_{i=1}^n x_i y_i.\nThe inner product induces a norm (also known as 2-norm, following the definition of the more general case of p-norm) \\| x \\| = \\sqrt{x \\cdot x}.\nThe norm induces a distance metric: d_{\\textrm{euclidean}}(x,y) = \\| x - y \\|_2.\n\nWe often assume we are working with a Euclidean metric space (\\mathbb{R}^n, d_{\\textrm{euclidean}}) without explicitly stating definitions. However, considering these definitions make it easier to understand the more general case of a Riemannian manifold."
  },
  {
    "objectID": "posts/spaces.html#reimannian-manifold",
    "href": "posts/spaces.html#reimannian-manifold",
    "title": "Concepts in dimensionality reduction",
    "section": "1.6 Reimannian Manifold",
    "text": "1.6 Reimannian Manifold\nTextbooks on differential geometry, e.g. Andrews (2020) would provide a more formal entry point and description for Manifolds. I’d like to point out connections here to Euclidean geometry to build some intuition first. In geometry taught in school, Euclidean geometry is introduced through its axioms. In particular, relaxing the fifth postulate (also referred at as the parallel postulate) leads to ideas of spherical and hyperbolic geometry, which can also be viewed as special cases of Reimannian manifolds that have constant, global curvature.\nRiemannian manifolds allow for local curvature. This local curvature is allowed to vary smoothly from one position to the next along the manifold. The local region around each position can be treated as a Euclidean space (we’ll introduce the notion of tangent space for this). We can choose a coordinate system locally to account for the local curvature, and then keep using intuitions for Euclidean spaces there.\nHowever, as we move along manifold, we’d like to modify the co-ordinate system itself to account for the local curvature. Because we are working with a smoothly varying curvature, we can do this using smooth transformations of the co-ordinate system - for which we use tools from calculus.\n(To be continued …)\n\nReferences\n\n\nAndrews, Ben. 2020. “Lectures on Differential Geometry.” Australian National University. https://maths-people.anu.edu.au/~andrews/DG/."
  },
  {
    "objectID": "posts/transformers.html",
    "href": "posts/transformers.html",
    "title": "Transformers",
    "section": "",
    "text": "Here I keep track of sources that have helped me understand this topic better. I think the main concepts to be familiar with are:\n\nGenerative modeling for sequences\nAttention mechanism, and transformer architecture\nMulti-task learning\n\n\n\nRaschka (2023) outlines some key papers and developments in the context of large language models.\n\nWeng (2018) provides an overview of the attention mechanism, and some historical notes for the peculiar keys, values, queries nomenclature used in Vaswani et al. (2017).\n\nBloem (2019) provides a more pedagogical view of the transformer architecture, and has nice visualizations as well.\n\n\n\nGenerative sequence modeling\nLanguage has a natural sequential structure. The generative modeling task is to learn a probability distribution over word sequences.\nLet s_i denote the random variable for the i-th word in the sequence. Each random variable can take on values from the entire vocabulary. Then the distribution (a.k.a. generative model) we’d like to learn from data is p(s_1, s_2, ..., s_n). The sequential ordering motivates a particular factorization of this joint distribution:\np(s_1, s_2, ..., s_n) = p(s_1) p(s_2 | s_1) p(s_3 | s_1, s_2) ... p(s_n | s_1, s_2, ..., s_{n-1})\nNote that for language modeling, the vocabulary typically consists of character substrings, or even related byte level representations. As the vocabulary becomes more fine-grained, the distributions that must be learnt become more expressive. The sequence length to cover the same semantic meaning becomes larger, and the amount of data required to learn the distributions increases.\nWhen generating new sequences, one can sample successively from the conditional distributions, e.g. p(s_{i+1} | s_1, ... s_{i}) to construct a length k sequence from given a length i sequence.\nGiven a sequence of length i, constructing a length k sequence can be viewed as sampling from p(s_{i+1}, ... s_{i+k} | s_{1}, ..., s_{i}). This looks like p(\\textrm{outputs}|\\textrm{inputs}). We’ll revisit this view in the section on multi-task learning.\n\n\nPhuong and Hutter (2022) describe transformer-based-algorithms without implementation details, instead focusing on descriptions with consistent and formal notation. With the maze of transformer variants that exist, this resource offers a coherent picture of the landscape.\n\n\nAttention mechanism and transformer architecture\nHere, I’ll highlight how attention captures dependence of the output on particular entries of the input sequence. This bare-bones version of self-attention includes no bias terms, no scaling of the dot products etc., that are part of standard implementations and described well in Phuong and Hutter (2022).\n\n\nAlex Smola (2019) motivates the attention mechanism with a kernel regression view.\n\nSchlag, Irie, and Schmidhuber (2021) link attention to fast weight programming. They also explore the linear version of attention in the context of associative memory and explore memory capacity of such models with different choices of kernels, activation functions, etc., connecting to a rich literature on such investigations dating all the way back to Cover (1965).\n\nLet length of the sequence T. We denote the i-th input as {x_i \\in \\mathbb{R}^{d_{\\textrm{in}}}}. Similarly the j-th output {y_j \\in \\mathbb{R}^{d_{\\textrm{out}}}}. Matrices {K, Q \\in \\mathbb{R}^{d_\\textrm{key-query} \\times d_{\\textrm{in}}}} and {V \\in \\mathbb{R}^{d_\\textrm{out} \\times d_{\\textrm{in}}}} are trainable weights.\n\n\\begin{aligned}\nk_i, q_i, v_i &= Kx_i, Qx_i, Vx_i &\\quad k,q \\in \\mathbb{R}^{d_{\\textrm{key-query}}}, v \\in \\mathbb{R}^{d_{out}} \\\\\nA_{ij} &= k_i^\\top q_j &\\quad A \\in \\mathbb{R}^{T \\times T}\\\\\nB_{ij} &= {\\exp (A_{ij})} / {\\sum_k \\exp (A_{ik})} \\\\\ny_i &= \\sum_j{B_{ij}v_j}\n\\end{aligned}\n\nA is known as the attention matrix. Setting certain entries of the A_{ij} to zero is referred to as masking, and it has the effect of preventing multiplicative interaction between the i-th and j-th inputs in constructing the i-th output.\nFor language, setting the upper triangular entries (excluding the diagonal) to zero is a common choice. This mask pattern ensures that y_i is only based on previous and current inputs of the sequence, i.e. (x_1, ..., x_{i}) (causality for temporal sequences). Since the product of lower triangular matrices remains lower triangular, stacking attention layers with this mask pattern retains such dependence throughout - which can be useful depending on the use case.\nChoice of particular masking patterns and sparsity of the attention matrix (and of K, Q, V matrices) are important questions for specific applications.\nIn the absence of masking, each output takes on the form y_j  = \\sum_i{g_j(x_i)}, for some function g_j. This form renders it invariant to ordering i.e. permutations of the input sequence.\n\n\nZaheer et al. (2017) have shown that for universal function approximators (e.g. neural networks) h and g, a function f of the form f(x_1, ... x_n) = h(\\sum_i{g(x_i)}) is a universal approximator for any permutation invariant function.\nOther than self-attention, the decoder-only transformer architecture includes residual connections, layer normalization, weight sharing across embedding laters etc. These conceptually straightforward operations turn out to be crucial in practice to prevent gradients from exploding or vanishing (see e.g. Zhang et al. (2022)), and to reduce memory footprint of large models.\nThe transformer in Vaswani et al. (2017) also has an encoding arm that is used for cross-attention, without masking. This makes intuitive sense for tasks like translation where the sequence of words in english may not be relevant for sequence of words in a german translation. In general, cross-attention is used to utilize context from a different source.\nSee Weng (2023) for a more comprehensive review of innovations in transformer-like models.\n\n\nI highly recommend the GPT implementation walkthrough by Karpathy (2023a). He’s a skilled teacher and watching him live-code is instructive.\n\n\nMulti-task learning\nIn one class of transformers (e.g. GPT and variants), the model is trained to simply predict the next word. That is, the model learns distributions p(s_i | s_1, ..., s_{i-1}) for all i \\in (1,...,n).\nSo far, a clear reason for why large language models seem magical is missing. Why bother increasing size of models, why invest so much in collecting the data and compute resources?\nI think Radford et al. (2019), and later Brown et al. (2020) provide the motivation. The main realization is that for language, the task itself is a sequence of tokens. Tasks can take the form of “translate to german” or “write python code for” etc. Here the sequence of words that specify the task are themselves part of the input sequence. Moreover, the underlying input can be more parcellated and abstract, e.g. byte pair encodings.\nThe implication is that while we learn p(\\textrm{output}|\\textrm{input}), we are also learning {p(\\textrm{output} | \\textrm{input}, \\textrm{task})}, without requiring task-specific architecture or training.\nIn my view, this is a central requirement for a meaningful notion of foundational models. The implicit multi-task learning that takes place is the reason why non-trivial zero shot performance may be possible on a variety of tasks, even though the task is not explicitly specified while training.\n\n\nFoundational models\nThe term “foundational models” was popularized by Bommasani et al. (2021), and an accompanying workshop at Stanford University.\nBommasani et al. (2021) describe the notion of emergence (generalization over tasks, and therefore related to zero-shot performance) and homogenization (learning something useful for a variety of downstream tasks, and therefore the pre-training is amenable to fine-tuning). These essentially echo the central observations of Radford et al. (2019) and Brown et al. (2020).\nThe idea of fine-tuning and using pre-trained model components on related datasets or tasks (e.g. transfer learning) has been a motivation behind efforts to curate such models in various domains, even before large language models became common e.g. https://pytorch.org/hub/. Unlike the multi-task learning in large language models, the scope of tasks for which pre-trained models might be useful is much more constrained.\n\n\nParting notes\nThe output of language models can be directly parsed and evaluated for quality by humans. This fact is exploited for public facing language models such as chatGPT. In this talk titled the State of GPT (2023b), Karpathy describes finetuning, reward modeling, and reinforcement learning using the reward model; all of these hinge on the fact that humans can understand and evaluate outputs of such language models.\nScientific applications will certainly find use for the attention mechanism and the transformer architecture. However, claims in papers from scientific domains that draw analogies with language modeling and co-opt jargon around foundational models deserve a more critical look.\n\n\nReferences\n\n\nAlex Smola, Ashton Zhang. 2019. “A Tutorial on Attention in Deep Learning.” 2019. https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4343.\n\n\nBloem, Peter. 2019. “Transformers from Scratch.” 2019. https://peterbloem.nl/blog/transformers.\n\n\nBommasani, Rishi, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, et al. 2021. “On the Opportunities and Risks of Foundation Models.” arXiv Preprint arXiv:2108.07258.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.\n\n\nCover, Thomas M. 1965. “Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition.” IEEE Transactions on Electronic Computers, no. 3: 326–34.\n\n\nKarpathy, Andrej. 2023a. “Neural Networks: Zero to Hero.” 2023. https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ.\n\n\n———. 2023b. “State of GPT.” Youtube. 2023. https://www.youtube.com/watch?v=bZQun8Y4L2A.\n\n\nPhuong, Mary, and Marcus Hutter. 2022. “Formal Algorithms for Transformers.” arXiv Preprint arXiv:2207.09238.\n\n\nRadford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. “Language Models Are Unsupervised Multitask Learners.” OpenAI Blog 1 (8): 9.\n\n\nRaschka, Sebastian. 2023. “Understanding Large Language Models.” 2023. https://magazine.sebastianraschka.com/p/understanding-large-language-models.\n\n\nSchlag, Imanol, Kazuki Irie, and Jürgen Schmidhuber. 2021. “Linear Transformers Are Secretly Fast Weight Programmers.” In International Conference on Machine Learning, 9355–66. PMLR.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” Advances in Neural Information Processing Systems 30.\n\n\nWeng, Lilian. 2018. “Attention? Attention!” 2018. https://lilianweng.github.io/posts/2018-06-24-attention/.\n\n\n———. 2023. “The Transformer Family Version 2.0.” Lilianweng.github.io, January. https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/.\n\n\nZaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. 2017. “Deep Sets.” Advances in Neural Information Processing Systems 30.\n\n\nZhang, Lily, Veronica Tozzo, John Higgins, and Rajesh Ranganath. 2022. “Set Norm and Equivariant Skip Connections: Putting the Deep in Deep Sets.” In International Conference on Machine Learning, 26559–74. PMLR."
  },
  {
    "objectID": "posts/information.html",
    "href": "posts/information.html",
    "title": "Information, entropy, and KL divergence",
    "section": "",
    "text": "Information and entropy are important concepts in modern statistical learning. The origins of these closely related concepts can be traced back to physical systems. Classical thermodynamics and heat engines motivate a continuous perspective, and Shannon’s information theory for data encoding and decoding motivate a discrete perspective for these concepts.\nWhile these physical systems can help build intuition, leaning hard on analogies can make it confusing for the problem at hand. Here I present a self-contained introduction to these concepts without reference to physical systems.\nI’ll assume continuous values for our random variables throughout. For the discrete case, the only change is to think of p_X and q_X as a probability mass functions (rather than a density functions), and the rest of the description on this page holds.\n\n\n\nTable 1: Notation and definitions\n\n\n\n\n\n\n\n\n\n\nQuantity\nSymbol\nDescription\n\n\n\n\nrandom variable\nX\nFunction that maps events to values\n\n\nmodel\nq_X\nA (controllable) probability density function\n\n\ndensity\nq_X(x)\nProbability density at a particular value x\n\n\ndraw\nx\\sim q_X\nA value for a random variable, drawn from the model q_X\n\n\nobservation\nx\\sim p_X\nAn observed value for a random variable from p_X\n\n\n\n\n\n\nNote:\n\ndraw, model and density specifically refer to quantities related to q_X as shown in the table. The plain-text model, density etc. refer to the general concepts.\nWe’ll drop the subscript X from the distributions and use p = p_X and q = q_X to simplify notation.\n\n\nSet up\n\nWe are given observations x from a distribution p.\nWe cannot directly access p, only observations from it.\nWe want to find a model q that approximates p well.\nAssume that model q can be tuned via parameters \\theta ^\\dagger.\n\n\n\nLikelihood and information\n\n\nGiven an observation x:\n\nWe can interpret q(x) as how likely an observation is under the model.\nSo we refer to q(x) as the likelihood.\nFollowing this, the quantity -\\log{q(x)} is called negative log-likelihood.\n\n\n\n{^\\dagger} The \\theta dependence can be explicitly denoted in the model as q_{\\theta}, or the density (and therefore likelihood) as q(x;\\theta). Likelihood can thus be viewed as a function of \\theta. The semi-colon indicates that \\theta refers to parameters, and not a random variable that comes with its own distribution.\n\nIn the Bayesian view, \\theta itself may be considered as a particular value of a random variable \\Theta, and this may be indicated in the distribution as q_{X|\\Theta}.\n\n\n\nFurther, if density q(x) is small, we can interpret the observation x as surprising.\nThe smaller the value of q(x), the larger is the value of -\\log{q(x)}.\nLarge values of negative log-likelihood thus convey surprise.\nIf we had access to p, we could similarly calculate surprise in observations under p.\nThe quantity -\\log{p(x)} is known as information.\n\n\n\n\nThe quantities -\\log{q(x)} and -\\log{p(x)} look identical, yet have different names.\n-\\log{p(x)} tells us something about the information content of an observation.\n-\\log{q(x)} conveys something about our approximation q (and therefore \\theta).\nThis is why it’s helpful to retain the terminology, and interpret it as such.\n\n\n\nWhy use the \\log operation to define information? A cryptic, partial answer: independence of distributions \\implies addition of information (and of log-likelihoods).\n\n\nEntropy\n\n\n\nAs a description of the model, we can calculate the expected surprise over samples x \\sim q.\nExpected surprise is simply the avg. negative log-likelihood, as above.\nExpected surprise with samples: E_{x \\sim q}({-\\log q(x)}).\nExpected surprise with observations: E_{x \\sim p}({-\\log q(x)}).\n\n\n\nExpectation is usually an operator applied to a function of a random variable, indicated as E[f(X)]. The distribution over which the expectation is calculated is often implicit.\n\nHere we treat expectation as a function of values specified in the argument, drawn from a distribution specified in the subscript, E_{x \\sim p}(f(x)). This is a commonly adopted convention that I’ll follow here.\n\nExpected surprise with samples is called entropy of q.\nExpected surprise with observation is called cross-entropy of q w.r.t. p.\nMinimizing cross-entropy is simply minimizing the expected surprise in observations, and is one way to tune q through \\theta.\nThis is equivalent to maximizing the likelihood of observations under q!\n\n\n\n\n\n\n\nMonte Carlo estimate of Expectations\n\n\n\n\n\n\nMonte Carlo integration enables approximation of integrals over regions as sums over samples drawn from those regions.\nIn particular, for regions that make up a valid probability distribution p:\n\n\\begin{aligned}\n\\int f(x) p(x) dx \\approx \\frac{1}{N} \\sum_{i=1}^N f(x_i) \\\\\n\\end{aligned}\n\nWe can thus express expectation over a probability distribution as a sum over samples!\nThis requires that samples are drawn i.i.d.^{\\dagger\\dagger} according to the distribution p.\nThe larger N is, the better the approximation is.\nWe often formulate problems so that p is some simple, low-dimensional distribution so that a relatively small number of samples can approximate such integrals well.\nThe subject efficient sampling from p of various classes (e.g. no explicit functional form, high dimensional etc.) is a subject of active research.\n\n\n\nExample:\n\nConsider cross entropy, where the integral is over the distribution p.\nIn this case, we don’t have access to p directly, only observations.\nNevertheless, we’ll assume observations are i.i.d. according to p.\nUsing i to index such observations x_i \\sim p, we have an estimator of the cross-entropy as a sum over observations:\n\n\\begin{aligned}\nE_{x \\sim p}({-\\log q(x)}) &= \\int p(x) (-\\log q(x)) dx \\\\\n&\\approx \\frac{1}{N} \\sum_{i=1}^N f(x_i)\n\\end{aligned}\n^{\\dagger\\dagger} i.i.d: independent and identically distributed.\n\n\n\n\n\nKL Divergence and Gibbs inequality\n\n\n\nQuantities related to information\n\n\n\n\n\n\nQuantity\nRefers to\n\n\n\n\nH(q) = E_{x \\sim q}({-\\log q(x)})\nEntropy of q\n\n\nH(p) = E_{x \\sim p}({-\\log p(x)})\nEntropy of p\n\n\nH(p, q) = E_{x \\sim p}({-\\log q(x)})\nCross entropy (q w.r.t. p)\n\n\nD_\\textrm{KL}(p | q) = E_{x \\sim p}({-\\log q(x)} -(-\\log p(x)))\nKL Divergence of q w.r.t. p\n\n\nD_\\textrm{KL}(p | q) = H(p,q) - H(p)\nRelative entropy interpretation\n\n\nD_\\textrm{KL}(p | q) = H(p,q) - H(p) \\geq 0\nGibbs inequality\n\n\n\n\n\n\nGibb’s inequality is a consequence of Jensen’s inequality (see proof below).\nIt shows that cross-entropy is always greater than or equal to entropy.\nThe cross-entropy is minimized when q = p.\nKL divergence is zero when q = p.\nD_\\textrm{KL}(p | q) is often presented as E_{x \\sim p}(\\log \\frac{p(x)}{q(x)}) where the connection to entropy is harder to see.\n\n\n\n\n\n\n\nGibb’s inequality proof\n\n\n\n\n\n\\begin{aligned}\nD_\\textrm{KL}(p | q) &= E_{x \\sim p}({-\\log q(x)} -(-\\log p(x))) \\\\\n&= E_{x \\sim p}(-\\log \\frac{q(x)}{p(x)}) \\\\\n\\end{aligned}\n\nJensen’s inequality: For a convex function \\phi: E_{x \\sim p}(\\phi(x))~\\geq~\\phi(E_{x \\sim p}(x)).\n-\\log is a convex function.\n\\begin{aligned}\nE_{x \\sim p}(-\\log \\frac{q(x)}{p(x)})~&\\geq -\\log E_{x \\sim p}(\\frac{q(x)}{p(x)}) \\\\\n&\\geq -\\log \\int p(x) \\frac{q(x)}{p(x)} dx \\\\\n&\\geq -\\log \\int q(x) dx \\\\\n&\\geq -\\log (1) \\\\\n&\\geq 0 \\\\\n\\implies D_\\textrm{KL}(p | q) &\\geq 0\n\\end{aligned}\n\nFor the discrete case, the integral is replaced by a sum.\n\n\n\n\n\n\n\nA recap and some intuitions\n\nWe can’t access p directly, only observations from it.\nWe’re proposing the model q as an approximation to p.\nMaximizing likelihood, minimizing cross-entropy, and minimizing KL divergence are closely related concepts to find a q that approximates p.\n\n\n\nRe. entropy:\n\nThe lower the entropy, the less surprising samples are on average.\nSampling from a sharply peaked distribution will be confined mostly to a small region.\nSuch samples will have high likelihood (by definition).\nSo a sharply peaked distribution has low entropy.\nIn statistical mechanics, structure in a system is related to ‘peaky’ distributions.\nA more structured system means the related distribution has lower entropy.\nConversely, a more disordered system means the related distribution has higher entropy.\n\n\n\nA numerical example with Gaussians:\n\nWe consider p = \\mathcal{N}(0, 1)\nq is also gaussian, but parameters \\mu or \\sigma are varied.\nHere we don’t explore algorithms to find q.\nThe goal is simply to compare entropy-based quantities, which serve as a basis for such algorithms.\n\n\n\n\nIn the left plot, we fix \\sigma = 1 and vary \\mu for q.\nEntropy is related to shape of distributions.\n\\sigma determines the shape of the Gaussian.\nTherefore, H(q) remains unchanged.\nThe minimum occurs at \\mu = 0, i.e. where q matches p.\nThe difference between H(p,q) and D_\\textrm{KL}(p | q) at the minimum is H(p).\n\n\n\n\nIn the right plot, we fix \\mu = 1 and vary \\sigma for q.\nEntropy H(q) is small when q is sharply peaked (i.e. small \\sigma).\nAs \\sigma inreases, q spreads out and H(q) becomes larger.\nThe minimum occurs at \\sigma = 1, i.e. where q matches p.\nH(p,q) - D_\\textrm{KL}(p | q) = H(p) at the minimum.\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport torch\n\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", font_scale=0.8, rc=custom_params)\n\n# p as a fixed Gaussian\nmu_p, std_p = 0.0, 1.0\np = torch.distributions.Normal(loc=mu_p, scale=std_p)\nx_obs = p.sample((10000,))\n\n# H(p) calculations:\nHp_theory = p.entropy()\nHp_emp = -p.log_prob(x_obs).mean()\n\n# q as Gaussians with varying mu\nDpq = []\nHpq = []\nHq = []\nq_mu = np.arange(-4, 4, 0.4)\nq_sd = 1.0\nfor mu in q_mu:\n    q = torch.distributions.Normal(loc=mu, scale=q_sd)\n    Dpq.append(torch.distributions.kl_divergence(p, q))\n    Hpq.append(-q.log_prob(x_obs).mean())\n    Hq.append(q.entropy())\n\nf, ax = plt.subplots(1, 2, figsize=(6, 3))\nax[0].plot(q_mu[[1, -1]], [Hp_emp, Hp_emp], '-b', label=r'$H(p)$')\nax[0].plot(q_mu, Hq, '.r', label=r'$H(q)$')\nax[0].plot(q_mu, Dpq, '-g', label=r'$D_{KL}(p,q)$')\nax[0].plot(q_mu, Hpq, '-k', alpha=0.5, label=r'$H(p,q)$')\nax[0].set(xlabel=r'$\\mu$', \n          ylabel='Entropy [nats]',\n          title=r'$p=\\mathcal{N}(0,1), q=\\mathcal{N}(\\mu, 1)$')\nax[0].grid()\n\n# q as Gaussians with varying sigma\nDpq = []\nHpq = []\nHq = []\nq_sd = np.arange(0.5, 2, 0.05)\nq_mu = 0.0\nfor sd in q_sd:\n    q = torch.distributions.Normal(loc=q_mu, scale=sd)\n    Dpq.append(torch.distributions.kl_divergence(p, q))\n    Hpq.append(-q.log_prob(x_obs).mean())\n    Hq.append(q.entropy())\nax[1].plot(q_sd[[1, -1]], [Hp_emp, Hp_emp], '-b', label=r'$H(p)$')\nax[1].plot(q_sd, Hq, '.r', label=r'$H(q)$')\nax[1].plot(q_sd, Dpq, '-g', label=r'$D_{KL}(p,q)$')\nax[1].plot(q_sd, Hpq, '-k', alpha=0.5, label=r'$H(p,q)$')\nax[1].set(xlabel=r'$\\sigma$', \n          ylabel='Entropy [nats]',\n          title=r'$p=\\mathcal{N}(0,1), q=\\mathcal{N}(0,\\sigma)$')\nax[1].grid()\nax[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAcknowledgments\nAnna Grim, Łukasz Kuśmierz, Priyanka Kulkarni and Shreyas Potnis for helpful comments and suggestions."
  }
]