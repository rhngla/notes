[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes",
    "section": "",
    "text": "Information, entropy, and KL divergence\n\n\n\n\n\n\n\nfundamentals\n\n\nmath\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nJul 13, 2023\n\n\nRohan Gala\n\n\n\n\n\n\n  \n\n\n\n\nGradients through stochastic nodes\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nJul 3, 2023\n\n\nRohan Gala\n\n\n\n\n\n\n  \n\n\n\n\nTransformers and foundational models for X\n\n\n\n\n\n\n\nmachine learning\n\n\n\n\n\n\n\n\n\n\n\nJun 20, 2023\n\n\nRohan Gala\n\n\n\n\n\n\n  \n\n\n\n\nNotation for conditional distributions\n\n\n\n\n\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nApr 27, 2023\n\n\nRohan Gala\n\n\n\n\n\n\n  \n\n\n\n\nConcepts in dimensionality reduction\n\n\n\n\n\n\n\nmath\n\n\nmachine-learning\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nRohan Gala\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/spaces.html",
    "href": "posts/spaces.html",
    "title": "Concepts in dimensionality reduction",
    "section": "",
    "text": "We’ll start with some definitions - the goal is to situate ourselves with:\nUltimately we want to construct analysis tools i.e. operations to manipulate such objects constructed from data, and discover relationships without violating definitions and underlying assumptions (or at least doing so knowingly). Further, through these definitions we notice somewhat hierarchical relationships of such objects, spaces, and rules. Some definitions are more abstract, and some are special cases with additional rules. Being aware of where in the abstraction hierarchy we operate is helpful to manage complexity while analyzing data."
  },
  {
    "objectID": "posts/spaces.html#sec-metricspace",
    "href": "posts/spaces.html#sec-metricspace",
    "title": "Concepts in dimensionality reduction",
    "section": "1.1 Metric and metric space",
    "text": "1.1 Metric and metric space\nA distance metric d is defined on set M \nd:M \\times M \\rightarrow \\mathbb{R}\n\nA distance metric is sometimes called just “distance” or just “metric”. For all x,y,z \\in M the following relationships hold for any valid distance metric: \n\\begin{aligned}\nd(x, y) &\\geq 0 \\\\\nd(x, y) &=0 \\Longleftrightarrow x=y \\\\\nd(x, y) &=d(y, x) \\\\\nd(x, z) &\\leq d(x, y)+d(y, z)\n\\end{aligned}\n\\tag{1}\nA metric space is defined as the ordered pair (M,d), where M is a set, and d is a distance metric. Reference to the metric d is occasionally omitted understood from the context, and the metric space is denoted simply by (M).\n\n\nCosine distance, defined for vectors x and y in Euclidean space as 1-\\cos\\theta where \\theta = \\frac{x \\cdot y}{\\|x\\| \\|y\\|}, is not a distance metric in general (e.g. d(x, 2x) = 0, but x\\neq 2x). However, it becomes a valid distance metric if we restrict our set of vectors to only those that have a fixed norm (\\| x \\| = c)."
  },
  {
    "objectID": "posts/spaces.html#sec-field",
    "href": "posts/spaces.html#sec-field",
    "title": "Concepts in dimensionality reduction",
    "section": "1.2 Field",
    "text": "1.2 Field\nA field is a set F which includes 0 and 1 and binary operations of addition + and multiplication \\cdot such that for all elements x,y,z \\in F the following hold:\n\n\\begin{aligned}\n0 + x &= x \\\\\nx + (-x) &= 0, \\quad -x \\in F \\\\\nx + y &= y + x \\\\\nx + (y + z) &= (x + y) + z \\\\\n1 \\cdot x &= 1 \\\\\nx^{-1} \\cdot x &= 1, \\quad \\forall x \\neq 0 \\\\\nx \\cdot y &= y \\cdot x \\\\\nx \\cdot (y \\cdot z) &= (x \\cdot y) \\cdot z \\\\\nx \\cdot (y + z) &= (x \\cdot y) + (x \\cdot z) \\\\\n\\end{aligned}\n\\tag{2}\nWe typically deal with the real field \\mathbb{R} or complex field \\mathbb{C}"
  },
  {
    "objectID": "posts/spaces.html#sec-vectorspace",
    "href": "posts/spaces.html#sec-vectorspace",
    "title": "Concepts in dimensionality reduction",
    "section": "1.3 Vector space",
    "text": "1.3 Vector space\nA vector space over a field F is a set V together with operations of addition and scalar multiplication) such that for all vectors x,y,z \\in V and all scalars \\alpha, \\beta \\in \\mathbb{F} the following hold:\n\n\\begin{aligned}\nx + y &= y + x \\\\\n(x + y) + z &= x + (y + z) \\\\\nx + 0 &= x \\\\\nx + (-x) &= 0 \\\\\n\\alpha (x + y) &= \\alpha x + \\alpha y \\\\\n(\\alpha + \\beta) x &= \\alpha x + \\beta x \\\\\n\\alpha (\\beta x) &= (\\alpha \\beta) x \\\\\n1x &= x\n\\end{aligned}\n\\tag{3}"
  },
  {
    "objectID": "posts/spaces.html#sec-norm",
    "href": "posts/spaces.html#sec-norm",
    "title": "Concepts in dimensionality reduction",
    "section": "1.4 Norm",
    "text": "1.4 Norm\nA norm on a vector space V is a function w:V \\rightarrow \\mathbb{R} that satisfies: \n\\begin{aligned}\nw(x) &\\geq 0 \\\\\nw(x) &= 0 \\implies x = 0 \\\\\nw(\\alpha x) &= \\lvert\\alpha\\rvert w(x) \\\\\nw(x+y) &\\leq w(x) + w(y)\n\\end{aligned}\n\\tag{4}\n\nNorms induce a distance metric on the vector space, d (x, y) = w (x − y), and such a normed vector space is therefore also a metric space.\nNot all metric spaces are normed vector spaces. That is, there exist metric spaces that are not normed vector spaces.\nA metric can define an induced norm w over a vector space (via w(x) = d(x,0)) if it satisfies the following: \n\\begin{aligned}\nd(w,v) &= d(u+w, u+v) \\\\\nd(\\alpha u,\\alpha v) &= \\lvert\\alpha\\rvert d(u,v)\n\\end{aligned}\n\\tag{5}"
  },
  {
    "objectID": "posts/spaces.html#sec-innerproductspace",
    "href": "posts/spaces.html#sec-innerproductspace",
    "title": "Concepts in dimensionality reduction",
    "section": "1.5 Inner product space",
    "text": "1.5 Inner product space\nAn inner product space is a vector space V over the field F together with an inner product. An inner product is a map \\langle \\cdot ,\\cdot \\rangle :V \\times V \\to F, that satisfies the properties for all vectors x,y,z\\in V and all scalars a,b \\in F. \n\\begin{aligned}\n\\langle x,y\\rangle &={\\overline {\\langle y,x\\rangle }} \\\\\n\\langle ax+by,z\\rangle &=a \\langle x,z\\rangle +b\\langle y,z\\rangle \\\\\n\\langle x,x\\rangle &\\geq 0 \\\\\n\\langle x,x\\rangle = 0 &\\iff x = 0 \\\\\n\\end{aligned}\n\\tag{6}\n\nConjugate symmetry implies \\langle x,x\\rangle \\in \\mathbb{R}.\nIn many applications, F is \\mathbb{R}, and conjugate symmetry reduces to symmetry.\nEvery inner product space induces a norm, called its canonical norm: \\|x\\|={\\sqrt {\\langle x,x\\rangle }}\n\n\n\np-norm (or L_p-norm) is defined over a vector space as: \n\\| x \\| = \\sqrt[p]{\\sum_{i=1}^n x_i^p}\n\nThe distance induced by the p-norm is also known as the Minkowski distance of order p.\n\nEuclidean space\nAn n-dimensional Euclidean space is a particular metric space. In fact, it is a vector space equipped with an inner product. Vectors in Euclidean space are denoted as x \\in \\mathbb{R}^n.\n\nThe inner product for two vectors x and y is defined as \\langle x, y \\rangle = x \\cdot y, where x \\cdot y = \\sum_{i=1}^n x_i y_i.\nThe inner product induces a norm (also known as 2-norm, following the definition of the more general case of p-norm) \\| x \\| = \\sqrt{x \\cdot x}.\nThe norm induces a distance metric: d_{\\textrm{euclidean}}(x,y) = \\| x - y \\|_2.\n\nWe often assume we are working with a Euclidean metric space (\\mathbb{R}^n, d_{\\textrm{euclidean}}) without explicitly stating definitions. However, considering these definitions make it easier to understand the more general case of a Riemannian manifold."
  },
  {
    "objectID": "posts/spaces.html#reimannian-manifold",
    "href": "posts/spaces.html#reimannian-manifold",
    "title": "Concepts in dimensionality reduction",
    "section": "1.6 Reimannian Manifold",
    "text": "1.6 Reimannian Manifold\nTextbooks on differential geometry, e.g. Andrews (2020) would provide a more formal entry point and description for Manifolds. I’d like to point out connections here to Euclidean geometry to build some intuition first. In geometry taught in school, Euclidean geometry is introduced through its axioms. In particular, relaxing the fifth postulate (also referred at as the parallel postulate) leads to ideas of spherical and hyperbolic geometry, which can also be viewed as special cases of Reimannian manifolds that have constant, global curvature.\nRiemannian manifolds allow for local curvature. This local curvature is allowed to vary smoothly from one position to the next along the manifold. The local region around each position can be treated as a Euclidean space (we’ll introduce the notion of tangent space for this). We can choose a coordinate system locally to account for the local curvature, and then keep using intuitions for Euclidean spaces there.\nHowever, as we move along manifold, we’d like to modify the co-ordinate system itself to account for the local curvature. Because we are working with a smoothly varying curvature, we can do this using smooth transformations of the co-ordinate system - for which we use tools from calculus.\n\n\n\n\n\n\nImportant\n\n\n\n\n\nSections below are under construction!\n\n\n\n\nCoordinate transformations\n\nDiscuss Jacobians\nCoordinate transformations in \\mathbb{R}^n\nInvertible transformations (connection to normalizing flows)\n\n\n\nNotation\nA Riemannian manifold is a tuple (M,g) where M is a set of points and g refers to inner products on vector spaces that we define at each point p \\in M. Vectors that are tangent to the point p make up the tangent space denoted by T_{p} M.\nLet point p and a local co-ordinate system denoted as (x^1,...,x^n)\nThe set where we define each object as a differential operator \\mathbf{A} = (A^1\\frac{\\partial}{\\partial x^1},...,A^n\\frac{\\partial}{\\partial x^n}), s.t. A^i \\in \\mathbb{R}. Such a set satisfies definition of a vector space. Moreover, each object is tangent to the manifold because of the partial derivatives. Thus, we now have a way to specify objects belonging to the tangent space T_{p} M.\nConsider the change of co-ordinates from (x^1,...,x^n) to (y^1,...,y^n), where y^i = f^i(x^1,...,x^n). The Jacobian matrix is defined as J_{ij} = \\frac{\\partial y^i}{\\partial x^j}.\n\n\nThe differential operators actually define a vector field; each vector field is a vector space on itself.\nWe denote the inner product at p \\in M as the map: g_{p}: T_{p} M \\times T_{p} M \\rightarrow \\mathbb{R}.\nFor two vectors \\mathbf{A} and \\mathbf{B} in T_{p} M, \\langle \\mathbf{A}, \\mathbf{B} \\rangle _{g_{p}}, the inner product\nThrough the distance metric induced by the canonical norm, it is easy to work out that Euclidean space is a special case of a Reimannian Manifold where g_{p} is an identity matrix.\n\n\nLocal\nRecall that an inner product induces a norm, and the norm induces a distance metric. Since the inner product is locally defined at each point, we can think of the distance metric as being defined at each point. Using this local notion of distance, we can calculate distances between any two points on the manifold through integration. This we have a set of points M, where we can calculate distances between any two points\nNote the similarity to definition of a metric space, Section 1.1.\nNote that the metric tensor is an inner product on the tangent space T_{p}M. Moreover, the tangent space is a vector space. Recall that any inner product induces a norm, which in turm induces a distance metric. Thus, there is notion of a distance metric for each point on the manifold (and the reason why g_p is called a metric tensor).\n\nNote: For the Euclidean manifold, the metric tensor is the identity matrix.\n\nCategorizing geometric transforms:\n\nDiffeomorphic: Preserves angles, distances, and shape\nIsometric: Preserves angles and distances\nConformal: Preserves angles"
  },
  {
    "objectID": "posts/spaces.html#sec-highdim-curse",
    "href": "posts/spaces.html#sec-highdim-curse",
    "title": "Concepts in dimensionality reduction",
    "section": "2.1 Curse of high dimensionality",
    "text": "2.1 Curse of high dimensionality\nIntuition for properties of data in 2d / 3d space don’t carry over to high dimensional spaces.\n\nMost of the volume is at the surface of the hypersphere in high D\nThis is a consequence of the phenomena of “concentration of measure”\n\nFurther reading: Aggarwal, Hinneburg, and Keim (2001), Chapter 1 of Lee and Verleysen (2007)."
  },
  {
    "objectID": "posts/spaces.html#manifold-assumption",
    "href": "posts/spaces.html#manifold-assumption",
    "title": "Concepts in dimensionality reduction",
    "section": "2.2 Manifold assumption",
    "text": "2.2 Manifold assumption\n\nMany methods attempt to use point samples in high dimensional space to parametrize a low dimensional manifold.\n\nParaphrasing a nice introduction in Ganea (2019), we assume that data lies on a manifold of much smaller dimension (latent) than the ambient dimension (measured). This enables learning and generalization to unseen data points. We further assume that it is a metric space (Y, d_Y) and has a smooth differentiable structure that allows learning via optimization.\nThus we are interested in Riemannian manifolds of intrinsic dimension d \\leq n. The hope is we can achieve this by learning an a mapping function f : X \\rightarrow Y from the input high dimensional data space to the low-dimensional representation space."
  },
  {
    "objectID": "posts/spaces.html#dimensionality-reduction-1",
    "href": "posts/spaces.html#dimensionality-reduction-1",
    "title": "Concepts in dimensionality reduction",
    "section": "2.3 Dimensionality reduction",
    "text": "2.3 Dimensionality reduction\nThese set of notes by Drew Wilimitis also contain python implementation of common methods.\nVarious computational methods have been designed to uncover a lower dimension manifold that preserve some notion of distances and structure of data observed in high dimension.\n\nMDS: preserve pairwise distances (using some common metric, e.g. Euclidean)\nIsomap: preserve pairwise geodesic distances (evaluated through nearest neighbor hops. Assumes a metric to define nearest neighbor)\nDiffusion maps: consider notion of reachability of distant points by constructing a diffusion process (Coifman et al. 2005; De la Porte et al. 2008)\nLaplacian eigenmaps, T-sne, Umap etc. are other nonlinear methods\n\nRandom projections, random sparse projections can be considered as baselines. Constructing such transforms has links to the Johnson Lindenstrauss lemma, and there is research on fast construction of useful random transforms. e.g. transformer architectures.\nTODO: - Similarities and distances discussion in Ganea (2019) - Proof in Dasgupta and Gupta (2003) for the Johnson Lindenstrauss lemma. - Nearest component analysis - Unifying dimensionality reduction, graph partitioning, and data set parameterization - Sparse random projections - Include references from this twitter thread\n\nUsing Jaccard similarity\nConsider representations of point p embedded in two metric spaces $(X, d_{X}) and in (Y, d_{Y}). We denote the k-nearest neighbors of p in the two metric spaces as K_{X}(p) and K_{Y}(p). The Jaccard similarity between the neighborhood sets is defined as: \nJ_{XY}(p) = \\frac{|K_{X}(p) \\cap K_{Y}(p)|}{|K_{X}(p) \\cup K_{Y}(p)|}"
  },
  {
    "objectID": "posts/notation_dist.html",
    "href": "posts/notation_dist.html",
    "title": "Notation for conditional distributions",
    "section": "",
    "text": "Continuous probability distributions are commonly specified through the probability density function p(x). A distribution with parameters \\theta can be viewed as a conditional distribution p(x|\\theta). In particular, the following notation is interchangeably used in the literature: p_\\theta(x) \\equiv p(x;\\theta) \\equiv p(x|\\theta)\nAny subtlety in terms of whether the notation choice depends on whether \\theta is a parameter (i.e. a constant without a prior distribution associated with it), or is a random variable is up to the author to clarify and use consistently. See community discussions Fraïssé (2017), Learner (2011), and Bartlett (2015).\nAt times, authors also drop the dependence on \\theta and simply write p_{\\theta}(x) as p(x). This can be a source of a lot of confusion, and both authors and readers should be careful to prevent misundertanding.\n\nReferences\n\n\nBartlett, Jonathan. 2015. “Bayesian Inference: Are Parameters Fixed or Random?” 2015. https://thestatsgeek.com/2015/04/22/bayesian-inference-are-parameters-fixed-or-random/.\n\n\nFraïssé. 2017. “What Does a Semicolon Denote in the Context of Probability and Statistics?” 2017. https://math.stackexchange.com/questions/2559085/what-does-a-semicolon-denote-in-the-context-of-probability-and-statistics.\n\n\nLearner. 2011. “What Is the Difference Between the Vertical Bar and Semi-Colon Notations?” 2011. https://stats.stackexchange.com/questions/10234/what-is-the-difference-between-the-vertical-bar-and-semi-colon-notations."
  },
  {
    "objectID": "posts/transformers.html",
    "href": "posts/transformers.html",
    "title": "Transformers and foundational models for X",
    "section": "",
    "text": "Here I keep track of sources that have helped me understand this topic better.\nIn my view, the main concepts to be familiar with are:\n\nGenerative modeling for sequences\nAttention mechanism, and transformer architecture\nMulti-task learning\n\n\n\nRaschka (2023) outlines some key papers and developments in the context of large language models.\n\nWeng (2018) provides an overview of the attention mechanism, and some historical notes for the peculiar keys, values, queries nomenclature used in Vaswani et al. (2017).\n\nBloem (2019) provides a more pedagogical view of the transformer architecture, and has nice visualizations as well.\n\n\n\nGenerative sequence modeling\nLanguage has a natural sequential structure. The generative modeling task is to learn a probability distribution over sequences of words.\nLet different words be specific values of n random variables. Then the distribution we’d like to learn from data is p(s_1, s_2, ..., s_n). The sequential ordering motivates a particular factorization of this joint distribution:\np(s_1, s_2, ..., s_n) = p(s_1) p(s_2 | s_1) p(s_3 | s_1, s_2) ... p(s_n | s_1, s_2, ..., s_{n-1})\nIt’s important to understand that the notion of ‘words’ is arbitrary for such modeling. Even for language, one can break up words at the character or even byte level. This idea is important for notions of multi-task learning and foundational models.\nWhen generating new sequences, we are recursively sampling particular conditional distributions, e.g. p(s_i | s_1, ... s_{i-1}) to ultimately look like we are sampling from p(s_i, ... s_n | s_{i-k}, ..., s_{i-1}).\nThis looks like p(\\textrm{outputs}|\\textrm{inputs}). We’ll revisit this view in the section on multi-task learning.\n\n\nPhuong and Hutter (2022) describe transformer-based-algorithms without implementation details, instead focusing on descriptions with consistent and formal notation. With the maze of transformer variants that exist, this resource offers a coherent picture of the landscape.\n\n\nAttention mechanism and transformer architecture\nI mainly want to highlight how attention captures dependence of the output on particular entries of the input sequence. This bare-bones version of self-attention includes no bias terms, no scaling of the dot products etc., that are part of standard implementations and described well in Phuong and Hutter (2022).\n\n\nAlex Smola (2019) motivates the attention mechanism with kernel regression.\n\nSchlag, Irie, and Schmidhuber (2021) link attention to fast weight programming. They also explore the linear version of attention in the context of associative memory and explore memory capacity of such models with different choices of kernels, activation functions, etc., connecting to a rich literature on such investigations dating all the way back to Cover (1965).\n\nLet length of the sequence T. We denote the i-th input as {x_i \\in \\mathbb{R}^{d_{\\textrm{in}}}}. Similarly the j-th output {y_j \\in \\mathbb{R}^{d_{\\textrm{out}}}}. Matrices {K, Q \\in \\mathbb{R}^{d_\\textrm{key-query} \\times d_{\\textrm{in}}}} and {V \\in \\mathbb{R}^{d_\\textrm{out} \\times d_{\\textrm{in}}}} are trainable weights.\n\n\\begin{aligned}\nk_i, q_i, v_i &= Kx_i, Qx_i, Vx_i &\\quad k,q \\in \\mathbb{R}^{d_{\\textrm{key-query}}}, v \\in \\mathbb{R}^{d_{out}} \\\\\nA_{ij} &= k_i^\\top q_j &\\quad A \\in \\mathbb{R}^{T \\times T}\\\\\nB_{ij} &= {\\exp (A_{ij})} / {\\sum_k \\exp (A_{ik})} \\\\\ny_i &= \\sum_j{B_{ij}v_j}\n\\end{aligned}\n\nA is known as the attention matrix. Setting certain entries of the A_{ij} to zero is referred to as masking, and it has the effect of preventing multiplicative interaction between the i-th and j-th inputs in constructing the i-th output.\nFor language, setting the upper triangular entries (excluding the diagonal) to zero is a common choice. This mask pattern ensures that y_i is only based on previous and current inputs of the sequence, i.e. (x_1, ..., x_{i}) (causality for temporal sequences). Since the product of lower triangular matrices remains lower triangular, stacking attention layers with this mask pattern retains such dependence throughout - which can be useful depending on the use case.\nI expect that choice of particular masking patterns and sparsity of the attention matrix (and of K, Q, V matrices) are important questions for specific applications.\nIn the absence of masking, each output takes on the form y_j = \\sum_i{g_j(x_i)}, for some function g_j. This form renders it invariant to ordering i.e. permutations of the input sequence.\n\n\nZaheer et al. (2017) have shown that for universal function approximators (e.g. neural networks) h and g, a function f of the form f(x_1, ... x_n) = h(\\sum_i{g(x_i)}) is a universal approximator for any permutation invariant function.\nOther than self-attention, the decoder-only transformer architecture includes residual connections, layer normalization. These conceptually straightforward operations turn out to be crucial in practice to prevent gradients from exploding or vanishing. Language models also share weights between the embedding layer (vocabulary_size x embedding_dimension). It remains to be seen if such tricks could be helpful for other applications.\nThe transformer in Vaswani et al. (2017) also has an encoding arm that is used for cross-attention, without masking. This makes intuitive sense for tasks like translation where the sequence of words in english may not be relevant for sequence of words in a german translation. In general, cross-attention is used to provide context from a different source.\n\n\nI highly recommend the GPT implementation walkthrough by Karpathy (2023a). He’s a skilled teacher and watching him live-code is instructive.\n\n\nMulti-task learning\nIn one version (e.g. GPT and variants), these transformer models are trained to simply predict the next word. That is, the model learns distributions p(s_i | s_1, ..., s_{i-1}) for all i \\in (1,...,n).\nSo far, a clear reason for why large language models seem magical is missing. Why bother increasing size of models, why invest so much in collecting the data and compute resources?\nI think Radford et al. (2019), and later Brown et al. (2020) provide this missing reason. The main realization is that for language, the task itself is a sequence of tokens. Tasks can take the form of “translate to german” or “write python code for” etc. Here the sequence of words that specify the task are themselves part of the input sequence.\nMoreover, the underlying input does not have to be “words”, but instead can be more parcellated and abstract, e.g. byte pair encodings.\nThe implication is that while we learn p(\\textrm{output}|\\textrm{input}), we are also learning {p(\\textrm{output} | \\textrm{input}, \\textrm{task})}, without needing task-specific architecture or training!\nIn my view, this is a central requirement for a meaningful notion of foundational models. The implicit multi-task learning that takes place is the reason why non-trivial zero shot performance is possible on a variety of tasks, even though the task is not explicitly specified while training.\n\n\nFoundational models\nThe term “foundational models” was popularized by Bommasani et al. (2021), and an accompanying workshop at Stanford University.\nBommasani et al. (2021) describe the notion of emergence (generalization over tasks, and therefore related to zero-shot performance) and homogenization (learning something useful for a variety of downstream tasks, and therefore the pre-training is amenable to fine-tuning). These essentially echo the central observations of Radford et al. (2019) and Brown et al. (2020).\nThe idea of fine-tuning and using pre-trained model components on related datasets or tasks (e.g. transfer learning) has been a motivation behind efforts to curate such models in various domains, even before large language models became common e.g. https://pytorch.org/hub/. Unlike the multi-task learning in large language models, the scope of tasks for which pre-trained models may be useful in other domains is usually limited.\n\n\nTowards scientific applications\nThe output of language models can be directly parsed by humans. Let’s say we ask a language model to write science fiction based on a fictional planet, for which we make up the rules of physics. The output from these models can be directly parsed, and evaluated for quality.\nThis fact is exploited for public facing language models such as chatGPT. In this talk titled the (State of GPT Karpathy 2023b), Karpathy describes finetuning, reward modeling, and reinforcement learning using the reward model; all of these hinge on the fact that humans can understand and evaluate outputs of such language models.\nThis is usually not the case for scientific applications. Models trained on vast amounts of data can certainly make predictions, but when to trust and invest in such predictions (for scientific applications) is generally a very hard, unsolved problem for now in my opinion.\n\n\nTransformers for single-cell resolution datasets\nThe most developed, least noisy single-cell resolution -omics datasets are from transcriptomics. For most analyses, the dataset is a table where rows are cells, columns are genes, and entries are the gene expression values. Additional metadata, e.g. disease state of the donor from which the cell was sampled, perturbations applied to the cell etc. may be part of the dataset.\nTypical tasks include determining identifiable cell types, specifying differentially expressed genes across conditions (aging, disease, perturbations), inferring developmental trajectories of particular cell types, identifying gene regulatory networks etc.\nThe issue that plagues so many of these analyses are related modeling of noise in such measurements, curse of high-dimensionality, absence of established ground-truth, and lack of mathematically precise definition of tasks.\n\n\nThe team at OpenProblems have continued to push for standardization and benchmarking through formal descriptions of tasks, e.g. Luecken et al. (2021). Still, the output of such efforts (both in terms of benchmarking datasets and state-of-the-art models) seems to have had limited influence on subsequent academic research.\nTranscriptomic data contain neither a natural sequential ordering, nor a large enough corpus of tasks to draw parallels with emergence and homogenization described above.\nRecent studies applying transformer-based models to single cell genomics consider similar pre-training tasks and model architectures.\n\nLe et al. (2021)\nYang et al. (2022)\nConnell, Khan, and Keiser (2022)\nShen et al. (2023)\nMa et al. (2023)\nCui et al. (2022), rebranded as Cui et al. (2023)\nTheodoris et al. (2023)\nHao et al. (2023)\nGong et al. (2023)\n\n(to be continued)\n\n\nReferences\n\n\nAlex Smola, Ashton Zhang. 2019. “A Tutorial on Attention in Deep Learning.” 2019. https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4343.\n\n\nBloem, Peter. 2019. “Transformers from Scratch.” 2019. https://peterbloem.nl/blog/transformers.\n\n\nBommasani, Rishi, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, et al. 2021. “On the Opportunities and Risks of Foundation Models.” arXiv Preprint arXiv:2108.07258.\n\n\nBrown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” Advances in Neural Information Processing Systems 33: 1877–1901.\n\n\nConnell, William, Umair Khan, and Michael J. Keiser. 2022. “A Single-Cell Gene Expression Language Model.” arXiv Preprint arXiv:2210.14330. https://doi.org/10.48550/arXiv.2210.14330.\n\n\nCover, Thomas M. 1965. “Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition.” IEEE Transactions on Electronic Computers, no. 3: 326–34.\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, Nan Duan, and Bo Wang. 2022. “scFormer: A Universal Representation Learning Approach for Single-Cell Data Using Transformers.” bioRxiv, 2022–11.\n\n\nCui, Haotian, Chloe Wang, Hassaan Maan, and Bo Wang. 2023. “scGPT: Towards Building a Foundation Model for Single-Cell Multi-Omics Using Generative AI.” bioRxiv, 2023–04.\n\n\nGong, Jing, Minsheng Hao, Xin Zeng, Chiming Liu, Jianzhu Ma, Xingyi Cheng, Taifeng Wang, Xuegong Zhang, and Le Song. 2023. “xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data.” bioRxiv, 2023–03.\n\n\nHao, Minsheng, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi Cheng, Taifeng Wang, Jianzhu Ma, Le Song, and Xuegong Zhang. 2023. “Large Scale Foundation Model on Single-Cell Transcriptomics.” bioRxiv. https://doi.org/10.1101/2023.05.29.542705.\n\n\nKarpathy, Andrej. 2023a. “Neural Networks: Zero to Hero.” 2023. https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ.\n\n\n———. 2023b. “State of GPT.” 2023. https://www.youtube.com/watch?v=bZQun8Y4L2A.\n\n\nLe, Nguyen Quoc Khanh, Quang-Thai Ho, Trinh-Trung-Duong Nguyen, and Yu-Yen Ou. 2021. “A Transformer Architecture Based on BERT and 2D Convolutional Neural Network to Identify DNA Enhancers from Sequence Information.” Briefings in Bioinformatics 22 (5): bbab005.\n\n\nLuecken, Malte D, Daniel Bernard Burkhardt, Robrecht Cannoodt, Christopher Lance, Aditi Agrawal, Hananeh Aliee, Ann T Chen, et al. 2021. “A Sandbox for Prediction and Integration of Dna, Rna, and Proteins in Single Cells.” In Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2).\n\n\nMa, Anjun, Xiaoying Wang, Jingxian Li, Cankun Wang, Tong Xiao, Yuntao Liu, Hao Cheng, et al. 2023. “Single-Cell Biological Network Inference Using a Heterogeneous Graph Transformer.” Nature Communications 14 (1): 964.\n\n\nPhuong, Mary, and Marcus Hutter. 2022. “Formal Algorithms for Transformers.” arXiv Preprint arXiv:2207.09238.\n\n\nRadford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. “Language Models Are Unsupervised Multitask Learners.” OpenAI Blog 1 (8): 9.\n\n\nRaschka, Sebastian. 2023. “Understanding Large Language Models.” 2023. https://magazine.sebastianraschka.com/p/understanding-large-language-models.\n\n\nSchlag, Imanol, Kazuki Irie, and Jürgen Schmidhuber. 2021. “Linear Transformers Are Secretly Fast Weight Programmers.” In International Conference on Machine Learning, 9355–66. PMLR.\n\n\nShen, H., J. Liu, J. Hu, X. Shen, C. Zhang, D. Wu, M. Feng, et al. 2023. “Generative Pretraining from Large-Scale Transcriptomes for Single-Cell Deciphering.” iScience 26 (5).\n\n\nTheodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina R. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023. “Transfer Learning Enables Predictions in Network Biology.” Nature. https://doi.org/10.1038/s41586-023-06139-9.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” Advances in Neural Information Processing Systems 30.\n\n\nWeng, Lilian. 2018. “Attention? Attention!” Lilianweng.github.io. https://lilianweng.github.io/posts/2018-06-24-attention/.\n\n\nYang, Fan, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lu, and Jianhua Yao. 2022. “scBERT as a Large-Scale Pretrained Deep Language Model for Cell Type Annotation of Single-Cell RNA-Seq Data.” Nature Machine Intelligence 4 (10): 852–66.\n\n\nZaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. 2017. “Deep Sets.” Advances in Neural Information Processing Systems 30."
  },
  {
    "objectID": "posts/stochastic-nodes.html",
    "href": "posts/stochastic-nodes.html",
    "title": "Gradients through stochastic nodes",
    "section": "",
    "text": "Calculating gradients through nodes in a computational graph that involve sampling from a distribution (for which we want to learn parameters) is non-trivial. Intuitively, this is because the samples of the distribution don’t maintain a dependency with the distribution parameters. This prevents application of the chain rule used for backpropagation to obtain gradients w.r.t the distribution parameters.\nThere are two main strategies to get around this, one involving score function estimators, and another involving reparametrization. See Schulman et al. (2015) for a more formal treatment.\nRoughly, the score function estimator-based approach is more general but more noisy. It leads to a surrogate loss function specified at the stochastic node, which permits gradient updates to the parameters of the distribution.\nThe reparametrization “trick” (when possible) allows reformulating the distribution such that distribution parameters become part of a deterministic function applied to samples obtained from a fixed distribution.\n\nThe set up\nConsider a stochastic node, where we sample values x of a random variable X from a distribution p_{X|\\phi} and \\phi are distribution parameters. Note that \\phi which themselves be specified by an upstream neural network.\nLet’s also assume a deterministic and differentiable function f_{\\theta} with parameters \\theta applied to samples x, and we are interested in parameters \\theta and \\phi that optimize an objective that consists of an average over the samples drawn from p_{X|\\phi} as follows:\n\n\\begin{aligned}\n\\mathcal{L}(\\theta, \\phi) &= \\mathbb{E}_{x \\sim p_{X|\\phi}}[f_{\\theta}(x)] = \\int{f_{\\theta}(x)p_{\\phi}(x)dx}\n\\end{aligned}\n\\tag{1}\n\n\nUpdates to \\theta\nFirst we evaluate the gradient \\nabla_{\\theta}\\mathcal{L} required to update parameters \\theta of the deterministic function f_{\\theta}: \n\\begin{aligned}\n\\nabla_{\\theta}\\mathcal{L} &= \\nabla_{\\theta}\\mathbb{E}_{x \\sim p_{X|\\phi}}[f_{\\theta}(x)] \\\\\n&=  \\int{\\nabla_{\\theta} f_{\\theta}(x) p_{\\phi}(x)dx} \\\\\n&=  \\mathbb{E}_{x \\sim p_{X|\\phi}}[\\nabla_{\\theta}f_{\\theta}(x)]\n\\end{aligned}\n\\tag{2}\nWe see that the gradient in Eq. 2 can be cast as an expectation over samples from the distribution p_{X|\\phi}. So we can simply draw samples from the distribution and estimate the gradient, because \\nabla_{\\theta}f_{\\theta} is well-defined. In the extreme case, the estimate (albeit a noisy one) is obtained simply from a single sample x. So far so good.\n\n\nUpdates to \\phi\n\n\\begin{aligned}\n\\nabla_{\\phi}\\mathcal{L} &= \\nabla_{\\phi}\\mathbb{E}_{x \\sim p_{X|\\phi}}[f_{\\theta}(x)] \\\\\n&= \\int{f_{\\theta}(x) \\nabla_{\\phi} p_{\\phi}(x)dx}\n\\end{aligned}\n\\tag{3}\nIt may be tempting think of Eq. 3 as \\mathbb{E}_{x \\sim \\nabla_{\\phi} p_{X|\\phi}}[f_{\\theta}(x)] in an analogous manner - this would be wrong, because \\nabla_{\\phi} p_{\\phi}(x) not a valid probability density function.\n\n\nA valid probability density function should be non-negative everywhere, and integrate to unity over its domain.\nInstead we’ll use an identity that is often referred to as the log-derivative trick to rewrite Eq. 3 as follows:\n\n\nLog-derivative trick: \n\\nabla_{x}h(x) = h(x)\\nabla_x{\\log(x)}\n\n\n\\begin{aligned}\n\\int{f_{\\theta}(x) \\nabla_{\\phi} p_{\\phi}(x)dx} &= \\int{(f_{\\theta}(x)  \\nabla_{\\phi} \\log{p_{\\phi}(x)}) p_{\\phi}(x) dx} \\\\\n\\implies \\nabla_{\\phi}\\mathcal{L} &=\\mathbb{E}_{x \\sim p_{X|\\phi}}[f_{\\theta}(x) \\nabla_{\\phi}{\\log{p_{\\phi}(x)}}]\n\\end{aligned}\n\\tag{4}\nEq. 4 shows that the gradient is once again over samples from the distribution p_{X|\\phi}. As before, in the extreme case this may be estimated with a single sample.\nIf we view p_{\\phi}(x) as a likelihood function, then \\nabla_{\\phi}{\\log{p_{\\phi}(x)}} is referred to as the score function. The estimate of the gradient includes the score function, and so it is referred to as a score function estimator.\nThis estimator suggests \\mathcal{L}_{\\textrm{surrogate}}=~\\mathbb{E}_{x \\sim p_{X|\\phi}}{[f_{\\theta}(x)\\log{p_{\\phi}(x)}]} as a surrogate loss function where f_{\\theta}(x) is treated as a constant (because it does not depend on \\phi).\nIt is worth thinking about the extreme case, when f_{\\theta}(x) itself is a constant independent of x. This would suggest that the parameter \\phi does not influence the loss, and so we would expect the gradient for \\phi to be zero in expectation.\nThis is indeed the case,\n\n\nThe bias and variance over different samples of the data are an important characterization of estimators. The score function is of relevance here, e.g. see these Lecture notes and discussion about the Cramér-Rao bound.\nAnother way to estimate the gradient through stochastic nodes is to use the reparametrization trick. This is possible when the distribution p_{X|\\phi} can be composed with a deterministic function g_\\psi applied to samples z drawn from a fixed distribution p_{Z}.\n\n\nReferences\n\n\nSchulman, John, Nicolas Heess, Theophane Weber, and Pieter Abbeel. 2015. “Gradient Estimation Using Stochastic Computation Graphs.” Advances in Neural Information Processing Systems 28."
  },
  {
    "objectID": "posts/information.html",
    "href": "posts/information.html",
    "title": "Information, entropy, and KL divergence",
    "section": "",
    "text": "Information and entropy are important concepts in modern statistical learning. The origins of these closely related concepts can be traced back to physical systems. Classical thermodynamics and heat engines motivate a continuous perspective, and Shannon’s information theory for data encoding and decoding motivate a discrete perspective for these concepts.\nWhile these physical systems can help build intuition, leaning hard on analogies can make it confusing for the problem at hand. Here I present a self-contained introduction to these concepts without reference to physical systems.\nI’ll assume continuous values for our random variables throughout. For the discrete case, the only change is to think of p_X as a probability mass function (rather than a density function), and the rest of the description on this page holds.\n\n\nTable 1: Notation and definitions\n\n\n\n\n\n\n\nQuantity\nRefers to\nDescription\n\n\n\n\nrandom variable\nX\nFunction that maps events to values\n\n\ndistribution\nq_X\nA particular probability density function\n\n\ndensity\nq_X(x)\nProbability density of at a particular value x, as specified by q_X\n\n\nsample\nx\\sim q_X\nA value for a random variable, drawn from the underlying distribution q_X\n\n\nobservation\nx\\sim p_X\nAn observed value for a random variable. We typically don’t know and cannot directly access p_X, but want to approximate it with q_X\n\n\n\n\nNote:\n\nI’ll treat sample, distribution and density specifically to referring quantities related to q_X as shown in the table. The plain-text sample, distribution, density etc. will refer to the general concepts.\nWe’ll also drop the subscript X from the distributions and use p = p_X and q = q_X since there are no further dependencies to consider (e.g. no conditional distributions).\n\n\nLikelihood and information\n\n\n\nWe are given observations, x \\sim p(x).\nWe assume an underlying distribution q.\nWe interpret density q(x) as how likely an observation is under the distribution.\nSo we refer to q(x) as the likelihood.\nFollowing this, the expression \\log q(x) is called log-likelihood.\n\n\n\n\nFurther, if density q(x) is small, we say the observation is unlikely.\nThe more unlikely the observation, the more surprising it is.\nThe smaller the value of q(x), the larger is the value of -\\log q(x).\nLarge values of negative log-likelihood thus convey surprise.\nFormally, the negative log-likelihood is called information.\nNotion of surprise requires context of an underlying distribution!\n\n\n\nWhy use the \\log operation to define information? A cryptic, partial answer: independence of distributions \\implies addition of information.\n\n\n\nLet’s play a similar game with samples, x \\sim q\nMany samples have larger values of q(x)\nSome samples will have small values of q(x)\nSo analogous to observations, some samples can be viewed as surprising.\n\n\n\nEntropy\n\n\n\nAs a summary statistic, we calculate the expected surprise over samples\nExpected surprise is simply the avg. negative log-likelihood, as above.\nExpected surprise with samples: E_{x \\sim q}({-\\log q(x)})\nExpected surprise with observations: E_{x \\sim p}({-\\log q(x)})\n\n\n\nExpectation is usually an operator applied to random variable, indicated as E[X]. Here we treat it as a function of values. This is a commonly adopted convention that I’ll follow here.\n\n\n\nExpected surprise with samples is called entropy of q\nExpected surprise with observation is called cross-entropy of q w.r.t. p\nMinimizing cross-entropy is simply minimizing the expected surprise in observations, and is one way to find q.\nThis is equivalent to maximizing the likelihood of observations under q!\n\n\n\nKL Divergence and Gibbs inequality\n\n\n\nQuantities related to information\n\n\n\n\n\n\nQuantity\nRefers to\n\n\n\n\nH(q) = E_{x \\sim q}({-\\log q(x)})\nEntropy of q\n\n\nH(p) = E_{x \\sim p}({-\\log p(x)})\nEntropy of p\n\n\nH(p, q) = E_{x \\sim p}({-\\log q(x)})\nCross entropy (q w.r.t. p)\n\n\nD_\\textrm{KL}(p | q) = E_{x \\sim p}({-\\log q(x)} -(-\\log p(x)))\nKL Divergence of q w.r.t. p\n\n\nD_\\textrm{KL}(p | q) = H(p,q) - H(p)\nRelative entropy interpretation\n\n\nD_\\textrm{KL}(p | q) = H(p,q) - H(p) \\geq 0\nGibbs inequality\n\n\n\n\n\n\nGibb’s inequality is a consequence of Jensen’s inequality (see proof below).\nIt shows that cross-entropy is always greater than or equal to entropy.\nThe cross-entropy is minimized when q_X = p_X.\nKL divergence is zero when q_X = p_X.\nD_\\textrm{KL}(p | q) is often presented as E_{x \\sim p}(\\log \\frac{p(x)}{q(x)}) where the connection to entropy is harder to see.\n\n\n\n\n\n\n\nGibb’s inequality proof\n\n\n\n\n\n\\begin{aligned}\nD_\\textrm{KL}(p | q) &= E_{x \\sim p}({-\\log q(x)} -(-\\log p(x))) \\\\\n&= E_{x \\sim p}(-\\log \\frac{q(x)}{p(x)}) \\\\\n\\end{aligned}\n\nJensen’s inequality: For a convex function \\phi: E_{x~p}(\\phi(x))~\\geq~\\phi(E_{x \\sim p}(x).\n-\\log is a convex function.\n\\begin{aligned}\nE_{x \\sim p}(-\\log \\frac{q(x)}{p(x)})~&\\geq -\\log E_{x \\sim p}(\\frac{q(x)}{p(x)}) \\\\\n&\\geq -\\log \\int p(x) \\frac{q(x)}{p(x)} dx \\\\\n&\\geq -\\log \\int q(x) dx \\\\\n&\\geq -\\log (1) \\\\\n&\\geq 0 \\\\\n\\implies D_\\textrm{KL}(p | q) &\\geq 0\n\\end{aligned}\n\nFor the discrete case, the integral is replaced by a sum.\n\n\n\n\n\n\n\nA recap and some intuitions\n\nWe can’t access p directly, only observations from it.\nWe’re proposing the distribution q as an approximation to p.\nMaximizing likelihood, minimizing cross-entropy, and minimizing KL divergence are closely related concepts to find a q that approximates p.\n\n\n\n\nRe. entropy:\nThe lower the entropy, the less surprising are samples.\nA sharply peaked distribution will lead to samples from a small region.\nMoreover, each of those samples will have high likelihood (by definition).\nSo a sharply peaked distribution has low entropy\nLoosely, a more structured distribution \\implies lower entropy\nConversely, a more disordered distribution \\implies higher entropy"
  }
]