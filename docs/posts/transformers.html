<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.333">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rohan Gala">
<meta name="dcterms.date" content="2023-06-01">

<title>notes - Transformers and foundational models for X</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../about.qmd" rel="" target="">
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/rhngla" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Transformers and foundational models for X</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">math</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Rohan Gala </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 1, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#learning-resources" id="toc-learning-resources" class="nav-link active" data-scroll-target="#learning-resources">Learning resources</a></li>
  <li><a href="#generative-sequence-modeling" id="toc-generative-sequence-modeling" class="nav-link" data-scroll-target="#generative-sequence-modeling">Generative sequence modeling</a></li>
  <li><a href="#transformers-in-scrna-seq" id="toc-transformers-in-scrna-seq" class="nav-link" data-scroll-target="#transformers-in-scrna-seq">Transformers in scRNA-seq</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Here I keep track of sources that have helped me understand this topic better.</p>
<section id="learning-resources" class="level3">
<h3 class="anchored" data-anchor-id="learning-resources">Learning resources</h3>
<p>In my view, the main concepts to be familiar with are:</p>
<ul>
<li>Generative modeling for sequences</li>
<li>Attention mechanism, and transformer architecture</li>
<li>Multi-task learning</li>
</ul>
<p><span class="citation" data-cites="raschka2023llm">Raschka (<a href="#ref-raschka2023llm" role="doc-biblioref">2023</a>)</span> outlines some key papers and developments in the context of large language models, and <span class="citation" data-cites="weng2018attention">Weng (<a href="#ref-weng2018attention" role="doc-biblioref">2018</a>)</span> provides an overview of the attention mechanism, and some historical notes for the peculiar keys, values, queries nomenclature used in the transformer.</p>
<p>Smola <span class="citation" data-cites="smola2019attn">(<a href="#ref-smola2019attn" role="doc-biblioref">2019</a>)</span> motivates the attention mechanism with kernel regression, providing some intuition for why it works.</p>
<p><span class="citation" data-cites="schlag2021linear">Schlag, Irie, and Schmidhuber (<a href="#ref-schlag2021linear" role="doc-biblioref">2021</a>)</span> provide a clear and concise description of the attention mechanism, and could be a good starting point for that reason. They also explore models with linear version attention, in the context of associative memory and explore capacity of such models with different choices of kernels, activation functions, and update rules.</p>
<p>On the practical side, the implementation walkthrough by Karpathy <span class="citation" data-cites="karpathy2023llm">(<a href="#ref-karpathy2023llm" role="doc-biblioref">2023</a>)</span> is excellent. He’s a skilled teacher and watching an expert like him live-code is instructive.</p>
<p><span class="citation" data-cites="phuong2022formal">Phuong and Hutter (<a href="#ref-phuong2022formal" role="doc-biblioref">2022</a>)</span> describe transformer-based-algorithms without implementation details, instead focusing on descriptions with consistent and formal notation. With the maze of transformer variants that exist, this resource offers a coherent picture of the landscape.</p>
</section>
<section id="generative-sequence-modeling" class="level3">
<h3 class="anchored" data-anchor-id="generative-sequence-modeling">Generative sequence modeling</h3>
<p>Language has a natural sequential structure. The generative modeling task is to learn a probability distribution over sequences of words.</p>
<p>Let different words be specific values of <span class="math inline">n</span> random variables. Then the distribution we’d like to learn from data is <span class="math inline">p(s_1, s_2, ..., s_n)</span>. The sequential ordering motivates a particular factorization of this joint distribution:</p>
<p><span class="math display">p(s_1, s_2, ..., s_n) = p(s_1) p(s_2 | s_1) p(s_3 | s_1, s_2) ... p(s_n | s_1, s_2, ..., s_{n-1})</span></p>
<p>It’s important to understand that the notion of ‘words’ is arbitrary for such modeling. Even for language, once can break up words at the character or even byte level (e.g.&nbsp;see <a href="https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt">byte pair encoding</a>). This is a crucial think to keep in mind for the notions of multi-task learning and foundational models.</p>
<p>When generating new sequences, we are recursively sampling particular conditional distributions, e.g.&nbsp;<span class="math inline">p(s_i | s_1, ... s_{i-1})</span> to ultimately look like we are sampling from <span class="math inline">p(s_i, ... s_n | s_{i-k}, ..., s_{i-1})</span>.</p>
<p>This looks like <span class="math inline">p(\textrm{outputs}|\textrm{inputs})</span>. We’ll refer to this in the section on multi-task learning.</p>
<section id="attention-mechanism-and-transformer-architecture" class="level4">
<h4 class="anchored" data-anchor-id="attention-mechanism-and-transformer-architecture">Attention mechanism and transformer architecture</h4>
<p>A self-attention layer maps an input sequence <span class="math inline">\{x_i\}^T_{i=1}</span>, <span class="math inline">x_i \in \mathbb{R}^{d \times 1}</span> to an output sequence <span class="math inline">\{y_i\}^T_{i=1}</span>, <span class="math inline">y_i \in \mathbb{R}^{d_\textrm{value} \times 1}</span> as</p>
<p><span class="math display">
\begin{align}
k_i, v_i, q_i &amp;= Kx_i,Vx_i,Qx_i \\
A_{ij} &amp;= \textrm{softmax}(k_i^\top q_j) \\
y_{i} &amp;= \sum_j{A_{ij}v_j}
\end{align}
</span></p>
<p>I’ve ignored scaling and nonlinearities for simplicity here.</p>
<p>The <span class="math inline">\textrm{softmax}</span> operation is applied along the T dimension, and <span class="math inline">K</span>, <span class="math inline">V</span>, <span class="math inline">Q</span> are trainable weight matrices. <span class="math inline">A \in \mathbb{R}_{\geq 0}^{T \times T}</span> is the attention matrix. Setting certain entries of the <span class="math inline">A_{ij}</span> to be zero is referred to as masking, and it has the effect of disallowing certain interactions between the input sequence elements in predicting a particular output.</p>
<p>For language, setting the upper triangular entries (including the diagonal) to be zero is a common choice. This mask pattern ensures that <span class="math inline">y_i</span> is only based on <span class="math inline">\{x_1, ..., x_{i-1}\}</span>.</p>
<p>In the absence of masking, each output takes on the form <span class="math inline">y_j = \sum_i{g_j(x_i)}</span>, for some function <span class="math inline">g_j</span>, which renders it invariant to the ordering of the input sequence i.e.&nbsp;permutation invariance.</p>
<p>On a related note, <span class="citation" data-cites="zaheer2017deep">Zaheer et al. (<a href="#ref-zaheer2017deep" role="doc-biblioref">2017</a>)</span> had shown that if <span class="math inline">h</span> and <span class="math inline">g</span> are universal function approximators (e.g.&nbsp;neural networks), <span class="math inline">f(x_1, ... x_n) = h(\sum_i(g(x_i)))</span> approximates for any permutation invariant function.</p>
<p>Aside from the attention mechanism, the transformer architecture includes residual connections, layer normalization, and scaling in the calculation of the attention matrix. These are all simple enough, but turn out to be important to prevent gradients from exploding or vanishing in practice.</p>
</section>
<section id="multi-task-learning" class="level4">
<h4 class="anchored" data-anchor-id="multi-task-learning">Multi-task learning</h4>
<p>In one version (e.g.&nbsp;GPT and variants), these transformer models are trained to simply predict the next word. That is, the model learns distributions <span class="math inline">p(s_i | s_1, ..., s_{i-1})</span> for all <span class="math inline">i \in \{1,...,n\}</span>.</p>
<p>So far, the recipe seems straightforward, and still doesn’t provide an intuitive understanding for why large language models seem magical. <span class="citation" data-cites="radford2019language">Radford et al. (<a href="#ref-radford2019language" role="doc-biblioref">2019</a>)</span> fill this gap. The idea is that for language, the task itself is a sequence of tokens.</p>
<p>Tasks can take the form of “translate to hindi” or “write python code for” etc. Here the sequence of words that specify the task are themselves part of the input sequence. The implication is that while we learn <span class="math inline">p(output|input)</span>, we are also learning <span class="math inline">p(output | input, task)</span>, without needing task-specific architecture or training!</p>
<p>Moreover, the underlying input does not have to be “words”, but instead can be more parcellated and abstract <a href="https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt">byte pair encodings</a>.</p>
<p>In my view, this is key to the idea of foundational models. The implicit multi-task learning that takes place is the reason why non-trivial zero shot performance is possible on a variety of tasks, even though the task is not explicitly specified while training.</p>
</section>
</section>
<section id="transformers-in-scrna-seq" class="level3">
<h3 class="anchored" data-anchor-id="transformers-in-scrna-seq">Transformers in scRNA-seq</h3>
<p>The transformer architectures are now being applied to single cell genomics datasets.</p>
<p>The dataset is a table of, in which each row is a cell, and each column is a gene. Additional metadata, e.g.&nbsp;disease state of the donor from which the cell was sampled, perturbations applied to the cell etc. may be part of the dataset.</p>
<p>The goal is to learn interpretable representations of cells that may be analyzed to uncover identifiable clusters (“cell types”).</p>
<p>Crucially, genes have no natural, fixed ordering as in other sequential data (language, music etc.).</p>
<p>Recent papers that have applied transformer architectures to single cell genomics include <span class="citation" data-cites="yang2022scbert">Theodoris et al. (<a href="#ref-theodoris2023" role="doc-biblioref">2023</a>)</span>.</p>
<p>(To be continued)</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-smola2019attn" class="csl-entry" role="listitem">
Alex Smola, Ashton Zhang. 2019. <span>“A Tutorial on Attention in Deep Learning.”</span> 2019. <a href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4343">https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4343</a>.
</div>
<div id="ref-cui2022scformer" class="csl-entry" role="listitem">
Cui, Haotian, Chloe Wang, Hassaan Maan, Nan Duan, and Bo Wang. 2022. <span>“scFormer: A Universal Representation Learning Approach for Single-Cell Data Using Transformers.”</span> <em>bioRxiv</em>, 2022–11.
</div>
<div id="ref-cui2023scgpt" class="csl-entry" role="listitem">
Cui, Haotian, Chloe Wang, Hassaan Maan, and Bo Wang. 2023. <span>“scGPT: Towards Building a Foundation Model for Single-Cell Multi-Omics Using Generative AI.”</span> <em>bioRxiv</em>, 2023–04.
</div>
<div id="ref-karpathy2023llm" class="csl-entry" role="listitem">
Karpathy, Andrej. 2023. <span>“Neural Networks: Zero to Hero.”</span> 2023. <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ</a>.
</div>
<div id="ref-phuong2022formal" class="csl-entry" role="listitem">
Phuong, Mary, and Marcus Hutter. 2022. <span>“Formal Algorithms for Transformers.”</span> <em>arXiv Preprint arXiv:2207.09238</em>.
</div>
<div id="ref-radford2019language" class="csl-entry" role="listitem">
Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. <span>“Language Models Are Unsupervised Multitask Learners.”</span> <em>OpenAI Blog</em> 1 (8): 9.
</div>
<div id="ref-raschka2023llm" class="csl-entry" role="listitem">
Raschka, Sebastian. 2023. <span>“Understanding Large Language Models.”</span> 2023. <a href="https://magazine.sebastianraschka.com/p/understanding-large-language-models">https://magazine.sebastianraschka.com/p/understanding-large-language-models</a>.
</div>
<div id="ref-schlag2021linear" class="csl-entry" role="listitem">
Schlag, Imanol, Kazuki Irie, and Jürgen Schmidhuber. 2021. <span>“Linear Transformers Are Secretly Fast Weight Programmers.”</span> In <em>International Conference on Machine Learning</em>, 9355–66. PMLR.
</div>
<div id="ref-shen2023generative" class="csl-entry" role="listitem">
Shen, Hongru, Jilei Liu, Jiani Hu, Xilin Shen, Chao Zhang, Dan Wu, Mengyao Feng, et al. 2023. <span>“Generative Pretraining from Large-Scale Transcriptomes for Single-Cell Deciphering.”</span> <em>Iscience</em> 26 (5).
</div>
<div id="ref-theodoris2023" class="csl-entry" role="listitem">
Theodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina R. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023. <span>“Transfer Learning Enables Predictions in Network Biology.”</span> <em>Nature</em>. <a href="https://doi.org/10.1038/s41586-023-06139-9">https://doi.org/10.1038/s41586-023-06139-9</a>.
</div>
<div id="ref-weng2018attention" class="csl-entry" role="listitem">
Weng, Lilian. 2018. <span>“Attention? Attention!”</span> <em>Lilianweng.github.io</em>. <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">https://lilianweng.github.io/posts/2018-06-24-attention/</a>.
</div>
<div id="ref-yang2022scbert" class="csl-entry" role="listitem">
Yang, Fan, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lu, and Jianhua Yao. 2022. <span>“scBERT as a Large-Scale Pretrained Deep Language Model for Cell Type Annotation of Single-Cell RNA-Seq Data.”</span> <em>Nature Machine Intelligence</em> 4 (10): 852–66.
</div>
<div id="ref-zaheer2017deep" class="csl-entry" role="listitem">
Zaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. 2017. <span>“Deep Sets.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>