<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rohan Gala">
<meta name="dcterms.date" content="2023-06-20">

<title>notes - Transformers</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../about.qmd" rel="" target="">
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/rhngla" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Transformers</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">machine learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Rohan Gala </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 20, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">January 10, 2024</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#generative-sequence-modeling" id="toc-generative-sequence-modeling" class="nav-link active" data-scroll-target="#generative-sequence-modeling">Generative sequence modeling</a></li>
  <li><a href="#attention-mechanism-and-transformer-architecture" id="toc-attention-mechanism-and-transformer-architecture" class="nav-link" data-scroll-target="#attention-mechanism-and-transformer-architecture">Attention mechanism and transformer architecture</a></li>
  <li><a href="#multi-task-learning" id="toc-multi-task-learning" class="nav-link" data-scroll-target="#multi-task-learning">Multi-task learning</a></li>
  <li><a href="#foundational-models" id="toc-foundational-models" class="nav-link" data-scroll-target="#foundational-models">Foundational models</a></li>
  <li><a href="#parting-notes" id="toc-parting-notes" class="nav-link" data-scroll-target="#parting-notes">Parting notes</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<p>Here I keep track of sources that have helped me understand this topic better. I think the main concepts to be familiar with are:</p>
<ul>
<li><a href="#generative-sequence-modeling">Generative modeling for sequences</a></li>
<li><a href="#attention-mechanism-and-transformer-architecture">Attention mechanism, and transformer architecture</a></li>
<li><a href="#multi-task-learning">Multi-task learning</a></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="citation" data-cites="raschka2023llm">Raschka (<a href="#ref-raschka2023llm" role="doc-biblioref">2023</a>)</span> outlines some key papers and developments in the context of large language models.</p>
</div><div class="">
<p><span class="citation" data-cites="weng2018attention">Weng (<a href="#ref-weng2018attention" role="doc-biblioref">2018</a>)</span> provides an overview of the attention mechanism, and some historical notes for the peculiar keys, values, queries nomenclature used in <span class="citation" data-cites="vaswani2017attention">Vaswani et al. (<a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span>.</p>
</div><div class="">
<p><span class="citation" data-cites="bloem2019transformers">Bloem (<a href="#ref-bloem2019transformers" role="doc-biblioref">2019</a>)</span> provides a more pedagogical view of the transformer architecture, and has nice visualizations as well.</p>
</div></div>

<section id="generative-sequence-modeling" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="generative-sequence-modeling">Generative sequence modeling</h3>
<p>Language has a natural sequential structure. The generative modeling task is to learn a probability distribution over word sequences.</p>
<p>Let <span class="math inline">s_i</span> denote the random variable for the <span class="math inline">i</span>-th word in the sequence. Each random variable can take on values from the entire vocabulary. Then the distribution (a.k.a. generative model) we’d like to learn from data is <span class="math inline">p(s_1, s_2, ..., s_n)</span>. The sequential ordering motivates a particular factorization of this joint distribution:</p>
<p><span class="math display">p(s_1, s_2, ..., s_n) = p(s_1) p(s_2 | s_1) p(s_3 | s_1, s_2) ... p(s_n | s_1, s_2, ..., s_{n-1})</span></p>
<p>Note that for language modeling, the vocabulary typically consists of character substrings, or even related byte level representations. As the vocabulary becomes more fine-grained, the distributions that must be learnt become more expressive. The sequence length to cover the same semantic meaning becomes larger, and the amount of data required to learn the distributions increases.</p>
<p>When generating new sequences, one can sample successively from the conditional distributions, e.g.&nbsp;<span class="math inline">p(s_{i+1} | s_1, ... s_{i})</span> to construct a length <span class="math inline">k</span> sequence from given a length <span class="math inline">i</span> sequence.</p>
<p>Given a sequence of length <span class="math inline">i</span>, constructing a length <span class="math inline">k</span> sequence can be viewed as sampling from <span class="math inline">p(s_{i+1}, ... s_{i+k} | s_{1}, ..., s_{i})</span>. This looks like <span class="math inline">p(\textrm{outputs}|\textrm{inputs})</span>. We’ll revisit this view in the section on multi-task learning.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="citation" data-cites="phuong2022formal">Phuong and Hutter (<a href="#ref-phuong2022formal" role="doc-biblioref">2022</a>)</span> describe transformer-based-algorithms without implementation details, instead focusing on descriptions with consistent and formal notation. With the maze of transformer variants that exist, this resource offers a coherent picture of the landscape.</p>
</div></div></section>
<section id="attention-mechanism-and-transformer-architecture" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="attention-mechanism-and-transformer-architecture">Attention mechanism and transformer architecture</h3>
<p>Here, I’ll highlight how attention captures dependence of the output on particular entries of the input sequence. This bare-bones version of self-attention includes no bias terms, no scaling of the dot products etc., that are part of standard implementations and described well in <span class="citation" data-cites="phuong2022formal">Phuong and Hutter (<a href="#ref-phuong2022formal" role="doc-biblioref">2022</a>)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="citation" data-cites="smola2019attn">Alex Smola (<a href="#ref-smola2019attn" role="doc-biblioref">2019</a>)</span> motivates the attention mechanism with a kernel regression view.</p>
</div><div class="">
<p><span class="citation" data-cites="schlag2021linear">Schlag, Irie, and Schmidhuber (<a href="#ref-schlag2021linear" role="doc-biblioref">2021</a>)</span> link attention to fast weight programming. They also explore the linear version of attention in the context of associative memory and explore memory capacity of such models with different choices of kernels, activation functions, etc., connecting to a rich literature on such investigations dating all the way back to <span class="citation" data-cites="cover1965geometrical">Cover (<a href="#ref-cover1965geometrical" role="doc-biblioref">1965</a>)</span>.</p>
</div></div>
<p>Let length of the sequence <span class="math inline">T</span>. We denote the <span class="math inline">i</span>-th input as <span class="math inline">{x_i \in \mathbb{R}^{d_{\textrm{in}}}}</span>. Similarly the <span class="math inline">j</span>-th output <span class="math inline">{y_j \in \mathbb{R}^{d_{\textrm{out}}}}</span>. Matrices <span class="math inline">{K, Q \in \mathbb{R}^{d_\textrm{key-query} \times d_{\textrm{in}}}}</span> and <span class="math inline">{V \in \mathbb{R}^{d_\textrm{out} \times d_{\textrm{in}}}}</span> are trainable weights.</p>
<p><span class="math display">
\begin{aligned}
k_i, q_i, v_i &amp;= Kx_i, Qx_i, Vx_i &amp;\quad k,q \in \mathbb{R}^{d_{\textrm{key-query}}}, v \in \mathbb{R}^{d_{out}} \\
A_{ij} &amp;= k_i^\top q_j &amp;\quad A \in \mathbb{R}^{T \times T}\\
B_{ij} &amp;= {\exp (A_{ij})} / {\sum_k \exp (A_{ik})} \\
y_i &amp;= \sum_j{B_{ij}v_j}
\end{aligned}
</span></p>
<p><span class="math inline">A</span> is known as the attention matrix. Setting certain entries of the <span class="math inline">A_{ij}</span> to zero is referred to as masking, and it has the effect of preventing multiplicative interaction between the <span class="math inline">i</span>-th and <span class="math inline">j</span>-th inputs in constructing the <span class="math inline">i</span>-th output.</p>
<p>For language, setting the upper triangular entries (excluding the diagonal) to zero is a common choice. This mask pattern ensures that <span class="math inline">y_i</span> is only based on previous and current inputs of the sequence, i.e.&nbsp;<span class="math inline">(x_1, ..., x_{i})</span> (causality for temporal sequences). Since the product of lower triangular matrices remains lower triangular, stacking attention layers with this mask pattern retains such dependence throughout - which can be useful depending on the use case.</p>
<p>Choice of particular masking patterns and sparsity of the attention matrix (and of <span class="math inline">K</span>, <span class="math inline">Q</span>, <span class="math inline">V</span> matrices) are important questions for specific applications.</p>
<p>In the absence of masking, each output takes on the form <span class="math inline">y_j = \sum_i{g_j(x_i)}</span>, for some function <span class="math inline">g_j</span>. This form renders it invariant to ordering i.e.&nbsp;permutations of the input sequence.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="citation" data-cites="zaheer2017deep">Zaheer et al. (<a href="#ref-zaheer2017deep" role="doc-biblioref">2017</a>)</span> have shown that for universal function approximators (e.g.&nbsp;neural networks) <span class="math inline">h</span> and <span class="math inline">g</span>, a function <span class="math inline">f</span> of the form <span class="math display">f(x_1, ... x_n) = h(\sum_i{g(x_i)})</span> is a universal approximator for any permutation invariant function.</p>
</div></div><p>Other than self-attention, the decoder-only transformer architecture includes residual connections, layer normalization, weight sharing across embedding laters etc. These conceptually straightforward operations turn out to be crucial in practice to prevent gradients from exploding or vanishing (see e.g. <span class="citation" data-cites="zhang2022set">Zhang et al. (<a href="#ref-zhang2022set" role="doc-biblioref">2022</a>)</span>), and to reduce memory footprint of large models.</p>
<p>The transformer in <span class="citation" data-cites="vaswani2017attention">Vaswani et al. (<a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span> also has an encoding arm that is used for cross-attention, without masking. This makes intuitive sense for tasks like translation where the sequence of words in english may not be relevant for sequence of words in a german translation. In general, cross-attention is used to utilize context from a different source.</p>
<p>See <span class="citation" data-cites="weng2023transformer">Weng (<a href="#ref-weng2023transformer" role="doc-biblioref">2023</a>)</span> for a more comprehensive review of innovations in transformer-like models.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>I highly recommend the GPT implementation walkthrough by Karpathy <span class="citation" data-cites="karpathy2023llm">(<a href="#ref-karpathy2023llm" role="doc-biblioref">2023a</a>)</span>. He’s a skilled teacher and watching him live-code is instructive.</p>
</div></div></section>
<section id="multi-task-learning" class="level3">
<h3 class="anchored" data-anchor-id="multi-task-learning">Multi-task learning</h3>
<p>In one class of transformers (e.g.&nbsp;GPT and variants), the model is trained to simply predict the next word. That is, the model learns distributions <span class="math inline">p(s_i | s_1, ..., s_{i-1})</span> for all <span class="math inline">i \in (1,...,n)</span>.</p>
<p>So far, a clear reason for why large language models seem magical is missing. Why bother increasing size of models, why invest so much in collecting the data and compute resources?</p>
<p>I think <span class="citation" data-cites="radford2019language">Radford et al. (<a href="#ref-radford2019language" role="doc-biblioref">2019</a>)</span>, and later <span class="citation" data-cites="brown2020language">Brown et al. (<a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span> provide the motivation. The main realization is that for language, the task itself is a sequence of tokens. Tasks can take the form of “translate to german” or “write python code for” etc. Here the sequence of words that specify the task are themselves part of the input sequence. Moreover, the underlying input can be more parcellated and abstract, e.g.&nbsp;<a href="https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt">byte pair encodings</a>.</p>
<p>The implication is that while we learn <span class="math inline">p(\textrm{output}|\textrm{input})</span>, we are also learning <span class="math inline">{p(\textrm{output} | \textrm{input}, \textrm{task})}</span>, without requiring task-specific architecture or training.</p>
<p>In my view, this is a central requirement for a meaningful notion of foundational models. The implicit multi-task learning that takes place is the reason why non-trivial zero shot performance may be possible on a variety of tasks, even though the task is not explicitly specified while training.</p>
</section>
<section id="foundational-models" class="level3">
<h3 class="anchored" data-anchor-id="foundational-models">Foundational models</h3>
<p>The term “foundational models” was popularized by <span class="citation" data-cites="bommasani2021opportunities">Bommasani et al. (<a href="#ref-bommasani2021opportunities" role="doc-biblioref">2021</a>)</span>, and an accompanying <a href="https://crfm.stanford.edu/workshop.html">workshop at Stanford University</a>.</p>
<p><span class="citation" data-cites="bommasani2021opportunities">Bommasani et al. (<a href="#ref-bommasani2021opportunities" role="doc-biblioref">2021</a>)</span> describe the notion of emergence (generalization over tasks, and therefore related to zero-shot performance) and homogenization (learning something useful for a variety of downstream tasks, and therefore the pre-training is amenable to fine-tuning). These essentially echo the central observations of <span class="citation" data-cites="radford2019language">Radford et al. (<a href="#ref-radford2019language" role="doc-biblioref">2019</a>)</span> and <span class="citation" data-cites="brown2020language">Brown et al. (<a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span>.</p>
<p>The idea of fine-tuning and using pre-trained model components on related datasets or tasks (e.g.&nbsp;transfer learning) has been a motivation behind efforts to curate such models in various domains, even before large language models became common e.g.&nbsp;<a href="https://pytorch.org/hub/">https://pytorch.org/hub/</a>. Unlike the multi-task learning in large language models, the scope of tasks for which pre-trained models might be useful is much more constrained.</p>
</section>
<section id="parting-notes" class="level3">
<h3 class="anchored" data-anchor-id="parting-notes">Parting notes</h3>
<p>The output of language models can be directly parsed and evaluated for quality by humans. This fact is exploited for public facing language models such as chatGPT. In this talk titled the <em>State of GPT</em> <span class="citation" data-cites="karpathy2023stategpt">(<a href="#ref-karpathy2023stategpt" role="doc-biblioref">2023b</a>)</span>, Karpathy describes finetuning, reward modeling, and reinforcement learning using the reward model; all of these hinge on the fact that humans can understand and evaluate outputs of such language models.</p>
<p>Scientific applications will certainly find use for the attention mechanism and the transformer architecture. However, claims in papers from scientific domains that draw analogies with language modeling and co-opt jargon around foundational models deserve a more critical look.</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-smola2019attn" class="csl-entry" role="listitem">
Alex Smola, Ashton Zhang. 2019. <span>“A Tutorial on Attention in Deep Learning.”</span> 2019. <a href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4343">https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4343</a>.
</div>
<div id="ref-bloem2019transformers" class="csl-entry" role="listitem">
Bloem, Peter. 2019. <span>“Transformers from Scratch.”</span> 2019. <a href="https://peterbloem.nl/blog/transformers">https://peterbloem.nl/blog/transformers</a>.
</div>
<div id="ref-bommasani2021opportunities" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, et al. 2021. <span>“On the Opportunities and Risks of Foundation Models.”</span> <em>arXiv Preprint arXiv:2108.07258</em>.
</div>
<div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div>
<div id="ref-cover1965geometrical" class="csl-entry" role="listitem">
Cover, Thomas M. 1965. <span>“Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition.”</span> <em>IEEE Transactions on Electronic Computers</em>, no. 3: 326–34.
</div>
<div id="ref-karpathy2023llm" class="csl-entry" role="listitem">
Karpathy, Andrej. 2023a. <span>“Neural Networks: Zero to Hero.”</span> 2023. <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ</a>.
</div>
<div id="ref-karpathy2023stategpt" class="csl-entry" role="listitem">
———. 2023b. <span>“State of GPT.”</span> Youtube. 2023. <a href="https://www.youtube.com/watch?v=bZQun8Y4L2A">https://www.youtube.com/watch?v=bZQun8Y4L2A</a>.
</div>
<div id="ref-phuong2022formal" class="csl-entry" role="listitem">
Phuong, Mary, and Marcus Hutter. 2022. <span>“Formal Algorithms for Transformers.”</span> <em>arXiv Preprint arXiv:2207.09238</em>.
</div>
<div id="ref-radford2019language" class="csl-entry" role="listitem">
Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. <span>“Language Models Are Unsupervised Multitask Learners.”</span> <em>OpenAI Blog</em> 1 (8): 9.
</div>
<div id="ref-raschka2023llm" class="csl-entry" role="listitem">
Raschka, Sebastian. 2023. <span>“Understanding Large Language Models.”</span> 2023. <a href="https://magazine.sebastianraschka.com/p/understanding-large-language-models">https://magazine.sebastianraschka.com/p/understanding-large-language-models</a>.
</div>
<div id="ref-schlag2021linear" class="csl-entry" role="listitem">
Schlag, Imanol, Kazuki Irie, and Jürgen Schmidhuber. 2021. <span>“Linear Transformers Are Secretly Fast Weight Programmers.”</span> In <em>International Conference on Machine Learning</em>, 9355–66. PMLR.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-weng2018attention" class="csl-entry" role="listitem">
Weng, Lilian. 2018. <span>“Attention? Attention!”</span> 2018. <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">https://lilianweng.github.io/posts/2018-06-24-attention/</a>.
</div>
<div id="ref-weng2023transformer" class="csl-entry" role="listitem">
———. 2023. <span>“The Transformer Family Version 2.0.”</span> <em>Lilianweng.github.io</em>, January. <a href="https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/">https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/</a>.
</div>
<div id="ref-zaheer2017deep" class="csl-entry" role="listitem">
Zaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. 2017. <span>“Deep Sets.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-zhang2022set" class="csl-entry" role="listitem">
Zhang, Lily, Veronica Tozzo, John Higgins, and Rajesh Ranganath. 2022. <span>“Set Norm and Equivariant Skip Connections: Putting the Deep in Deep Sets.”</span> In <em>International Conference on Machine Learning</em>, 26559–74. PMLR.
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>