<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.333">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rohan Gala">
<meta name="dcterms.date" content="2023-06-20">

<title>notes - Transformers and foundational models for X</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css">

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">notes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../about.qmd" rel="" target="">
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/rhngla" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Transformers and foundational models for X</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">machine learning</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Rohan Gala </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">June 20, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">July 3, 2023</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#generative-sequence-modeling" id="toc-generative-sequence-modeling" class="nav-link active" data-scroll-target="#generative-sequence-modeling">Generative sequence modeling</a></li>
  <li><a href="#attention-mechanism-and-transformer-architecture" id="toc-attention-mechanism-and-transformer-architecture" class="nav-link" data-scroll-target="#attention-mechanism-and-transformer-architecture">Attention mechanism and transformer architecture</a></li>
  <li><a href="#multi-task-learning" id="toc-multi-task-learning" class="nav-link" data-scroll-target="#multi-task-learning">Multi-task learning</a></li>
  <li><a href="#foundational-models" id="toc-foundational-models" class="nav-link" data-scroll-target="#foundational-models">Foundational models</a></li>
  <li><a href="#towards-scientific-applications" id="toc-towards-scientific-applications" class="nav-link" data-scroll-target="#towards-scientific-applications">Towards scientific applications</a></li>
  <li><a href="#transformers-for-single-cell-resolution-datasets" id="toc-transformers-for-single-cell-resolution-datasets" class="nav-link" data-scroll-target="#transformers-for-single-cell-resolution-datasets">Transformers for single-cell resolution datasets</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<p>Here I keep track of sources that have helped me understand this topic better.</p>
<p>In my view, the main concepts to be familiar with are:</p>
<ul>
<li><a href="#generative-sequence-modeling">Generative modeling for sequences</a></li>
<li><a href="#attention-mechanism-and-transformer-architecture">Attention mechanism, and transformer architecture</a></li>
<li><a href="#multi-task-learning">Multi-task learning</a></li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="citation" data-cites="raschka2023llm">Raschka (<a href="#ref-raschka2023llm" role="doc-biblioref">2023</a>)</span> outlines some key papers and developments in the context of large language models.</p>
</div><div class="">
<p><span class="citation" data-cites="weng2018attention">Weng (<a href="#ref-weng2018attention" role="doc-biblioref">2018</a>)</span> provides an overview of the attention mechanism, and some historical notes for the peculiar keys, values, queries nomenclature used in <span class="citation" data-cites="vaswani2017attention">Vaswani et al. (<a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span>.</p>
</div><div class="">
<p><span class="citation" data-cites="bloem2019transformers">Bloem (<a href="#ref-bloem2019transformers" role="doc-biblioref">2019</a>)</span> provides a more pedagogical view of the transformer architecture, and has nice visualizations as well.</p>
</div></div>

<section id="generative-sequence-modeling" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="generative-sequence-modeling">Generative sequence modeling</h3>
<p>Language has a natural sequential structure. The generative modeling task is to learn a probability distribution over sequences of words.</p>
<p>Let different words be specific values of <span class="math inline">n</span> random variables. Then the distribution we’d like to learn from data is <span class="math inline">p(s_1, s_2, ..., s_n)</span>. The sequential ordering motivates a particular factorization of this joint distribution:</p>
<p><span class="math display">p(s_1, s_2, ..., s_n) = p(s_1) p(s_2 | s_1) p(s_3 | s_1, s_2) ... p(s_n | s_1, s_2, ..., s_{n-1})</span></p>
<p>It’s important to understand that the notion of ‘words’ is arbitrary for such modeling. Even for language, one can break up words at the character or even byte level. This idea is important for notions of multi-task learning and foundational models.</p>
<p>When generating new sequences, we are recursively sampling particular conditional distributions, e.g.&nbsp;<span class="math inline">p(s_i | s_1, ... s_{i-1})</span> to ultimately look like we are sampling from <span class="math inline">p(s_i, ... s_n | s_{i-k}, ..., s_{i-1})</span>.</p>
<p>This looks like <span class="math inline">p(\textrm{outputs}|\textrm{inputs})</span>. We’ll revisit this view in the section on multi-task learning.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="citation" data-cites="phuong2022formal">Phuong and Hutter (<a href="#ref-phuong2022formal" role="doc-biblioref">2022</a>)</span> describe transformer-based-algorithms without implementation details, instead focusing on descriptions with consistent and formal notation. With the maze of transformer variants that exist, this resource offers a coherent picture of the landscape.</p>
</div></div></section>
<section id="attention-mechanism-and-transformer-architecture" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="attention-mechanism-and-transformer-architecture">Attention mechanism and transformer architecture</h3>
<p>I mainly want to highlight how attention captures dependence of the output on particular entries of the input sequence. This bare-bones version of self-attention includes no bias terms, no scaling of the dot products etc., that are part of standard implementations and described well in <span class="citation" data-cites="phuong2022formal">Phuong and Hutter (<a href="#ref-phuong2022formal" role="doc-biblioref">2022</a>)</span>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="citation" data-cites="smola2019attn">Alex Smola (<a href="#ref-smola2019attn" role="doc-biblioref">2019</a>)</span> motivates the attention mechanism with kernel regression.</p>
</div><div class="">
<p><span class="citation" data-cites="schlag2021linear">Schlag, Irie, and Schmidhuber (<a href="#ref-schlag2021linear" role="doc-biblioref">2021</a>)</span> link attention to fast weight programming. They also explore the linear version of attention in the context of associative memory and explore memory capacity of such models with different choices of kernels, activation functions, etc., connecting to a rich literature on such investigations dating all the way back to <span class="citation" data-cites="cover1965geometrical">Cover (<a href="#ref-cover1965geometrical" role="doc-biblioref">1965</a>)</span>.</p>
</div></div>
<p>Let length of the sequence <span class="math inline">T</span>. We denote the <span class="math inline">i</span>-th input as <span class="math inline">{x_i \in \mathbb{R}^{d_{\textrm{in}}}}</span>. Similarly the <span class="math inline">j</span>-th output <span class="math inline">{y_j \in \mathbb{R}^{d_{\textrm{out}}}}</span>. Matrices <span class="math inline">{K, Q \in \mathbb{R}^{d_\textrm{key-query} \times d_{\textrm{in}}}}</span> and <span class="math inline">{V \in \mathbb{R}^{d_\textrm{out} \times d_{\textrm{in}}}}</span> are trainable weights.</p>
<p><span class="math display">
\begin{aligned}
k_i, q_i, v_i &amp;= Kx_i, Qx_i, Vx_i &amp;\quad k,q \in \mathbb{R}^{d_{\textrm{key-query}}}, v \in \mathbb{R}^{d_{out}} \\
A_{ij} &amp;= k_i^\top q_j &amp;\quad A \in \mathbb{R}^{T \times T}\\
B_{ij} &amp;= {\exp (A_{ij})} / {\sum_k \exp (A_{ik})} \\
y_i &amp;= \sum_j{B_{ij}v_j}
\end{aligned}
</span></p>
<p><span class="math inline">A</span> is known as the attention matrix. Setting certain entries of the <span class="math inline">A_{ij}</span> to zero is referred to as masking, and it has the effect of preventing multiplicative interaction between the <span class="math inline">i</span>-th and <span class="math inline">j</span>-th inputs in constructing the <span class="math inline">i</span>-th output.</p>
<p>For language, setting the upper triangular entries (excluding the diagonal) to zero is a common choice. This mask pattern ensures that <span class="math inline">y_i</span> is only based on previous and current inputs of the sequence, i.e.&nbsp;<span class="math inline">(x_1, ..., x_{i})</span> (causality for temporal sequences). Since the product of lower triangular matrices remains lower triangular, stacking attention layers with this mask pattern retains such dependence throughout - which can be useful depending on the use case.</p>
<p>I expect that choice of particular masking patterns and sparsity of the attention matrix (and of <span class="math inline">K</span>, <span class="math inline">Q</span>, <span class="math inline">V</span> matrices) are important questions for specific applications.</p>
<p>In the absence of masking, each output takes on the form <span class="math inline">y_j = \sum_i{g_j(x_i)}</span>, for some function <span class="math inline">g_j</span>. This form renders it invariant to ordering i.e.&nbsp;permutations of the input sequence.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="citation" data-cites="zaheer2017deep">Zaheer et al. (<a href="#ref-zaheer2017deep" role="doc-biblioref">2017</a>)</span> have shown that for universal function approximators (e.g.&nbsp;neural networks) <span class="math inline">h</span> and <span class="math inline">g</span>, a function <span class="math inline">f</span> of the form <span class="math display">f(x_1, ... x_n) = h(\sum_i{g(x_i)})</span> is a universal approximator for any permutation invariant function.</p>
</div></div><p>Other than self-attention, the decoder-only transformer architecture includes residual connections, layer normalization. These conceptually straightforward operations turn out to be crucial in practice to prevent gradients from exploding or vanishing. Language models also share weights between the embedding layer <code>(vocabulary_size x embedding_dimension)</code>. It remains to be seen if such tricks could be helpful for other applications.</p>
<p>The transformer in <span class="citation" data-cites="vaswani2017attention">Vaswani et al. (<a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span> also has an encoding arm that is used for cross-attention, without masking. This makes intuitive sense for tasks like translation where the sequence of words in english may not be relevant for sequence of words in a german translation. In general, cross-attention is used to provide context from a different source.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>I highly recommend the GPT implementation walkthrough by Karpathy <span class="citation" data-cites="karpathy2023llm">(<a href="#ref-karpathy2023llm" role="doc-biblioref">2023</a>)</span>. He’s a skilled teacher and watching him live-code is instructive.</p>
</div></div></section>
<section id="multi-task-learning" class="level3">
<h3 class="anchored" data-anchor-id="multi-task-learning">Multi-task learning</h3>
<p>In one version (e.g.&nbsp;GPT and variants), these transformer models are trained to simply predict the next word. That is, the model learns distributions <span class="math inline">p(s_i | s_1, ..., s_{i-1})</span> for all <span class="math inline">i \in (1,...,n)</span>.</p>
<p>So far, a clear reason for why large language models seem magical is missing. Why bother increasing size of models, why invest so much in collecting the data and compute resources?</p>
<p>I think <span class="citation" data-cites="radford2019language">Radford et al. (<a href="#ref-radford2019language" role="doc-biblioref">2019</a>)</span>, and later <span class="citation" data-cites="brown2020language">Brown et al. (<a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span> provide this missing reason. The main realization is that for language, the task itself is a sequence of tokens. Tasks can take the form of “translate to german” or “write python code for” etc. Here the sequence of words that specify the task are themselves part of the input sequence.</p>
<p>Moreover, the underlying input does not have to be “words”, but instead can be more parcellated and abstract, e.g.&nbsp;<a href="https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt">byte pair encodings</a>.</p>
<p>The implication is that while we learn <span class="math inline">p(\textrm{output}|\textrm{input})</span>, we are also learning <span class="math inline">{p(\textrm{output} | \textrm{input}, \textrm{task})}</span>, without needing task-specific architecture or training!</p>
<p>In my view, this is a central requirement for a meaningful notion of foundational models. The implicit multi-task learning that takes place is the reason why non-trivial zero shot performance is possible on a variety of tasks, even though the task is not explicitly specified while training.</p>
</section>
<section id="foundational-models" class="level3">
<h3 class="anchored" data-anchor-id="foundational-models">Foundational models</h3>
<p>The term “foundational models” was popularized by <span class="citation" data-cites="bommasani2021opportunities">Bommasani et al. (<a href="#ref-bommasani2021opportunities" role="doc-biblioref">2021</a>)</span>, and an accompanying <a href="https://crfm.stanford.edu/workshop.html">workshop at Stanford University</a>.</p>
<p><span class="citation" data-cites="bommasani2021opportunities">Bommasani et al. (<a href="#ref-bommasani2021opportunities" role="doc-biblioref">2021</a>)</span> describe the notion of emergence (generalization over tasks, and therefore related to zero-shot performance) and homogenization (learning something useful for a variety of downstream tasks, and therefore the pre-training is amenable to fine-tuning). These essentially echo the central observations of <span class="citation" data-cites="radford2019language">Radford et al. (<a href="#ref-radford2019language" role="doc-biblioref">2019</a>)</span> and <span class="citation" data-cites="brown2020language">Brown et al. (<a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span>.</p>
<p>The idea of fine-tuning and using pre-trained model components on related datasets or tasks (e.g.&nbsp;transfer learning) has been a motivation behind efforts to curate such models in various domains, even before large language models became common e.g.&nbsp;<a href="https://pytorch.org/hub/">https://pytorch.org/hub/</a>. Unlike the multi-task learning in large language models, the scope of tasks for which pre-trained models may be useful in other domains is usually limited.</p>
</section>
<section id="towards-scientific-applications" class="level3">
<h3 class="anchored" data-anchor-id="towards-scientific-applications">Towards scientific applications</h3>
<p>The output of language models can be directly parsed by humans. Let’s say we ask a language model to write science fiction based on a fictional planet, for which we make up the rules of physics. The output from these models can be directly parsed, and evaluated for quality.</p>
<p>This is usually not the case for scientific applications. The model makes predictions, but for tools to make sense of when predictions are to be trusted are lacking.</p>
</section>
<section id="transformers-for-single-cell-resolution-datasets" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="transformers-for-single-cell-resolution-datasets">Transformers for single-cell resolution datasets</h3>
<p>The most developed, least noisy single-cell resolution <em>-omics</em> datasets are from transcriptomics. For most analyses, the dataset is a table where rows are cells, columns are genes, and entries are the gene expression values. Additional metadata, e.g.&nbsp;disease state of the donor from which the cell was sampled, perturbations applied to the cell etc. may be part of the dataset.</p>
<p>Typical tasks include determining identifiable cell types, specifying differentially expressed genes across conditions (aging, disease, perturbations), inferring developmental trajectories of particular cell types, identifying gene regulatory networks etc.</p>
<p>The issue that plagues so many of these analyses are related modeling of noise in such measurements, curse of high-dimensionality, absence of established ground-truth, and often lack of a mathematically precise definition for tasks.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>The team at <a href="https://openproblems.bio/events/">OpenProblems</a> have continued to push for standardization and benchmarking through formal descriptions of tasks, e.g. <span class="citation" data-cites="luecken2021sandbox">Luecken et al. (<a href="#ref-luecken2021sandbox" role="doc-biblioref">2021</a>)</span>. Still, the output of such efforts (both in terms of benchmarking datasets and state-of-the-art models) seems to have had limited influence on subsequent academic research.</p>
</div></div><p>Transcriptomic data contain neither a natural sequential ordering, nor a large enough corpus of tasks to draw parallels with emergence and homogenization described above.</p>
<p>Recent studies applying transformer-based models to single cell genomics have generated quite some buzz:</p>
<ul>
<li><span class="citation" data-cites="le2021transformer">Le et al. (<a href="#ref-le2021transformer" role="doc-biblioref">2021</a>)</span></li>
<li><span class="citation" data-cites="yang2022scbert">Yang et al. (<a href="#ref-yang2022scbert" role="doc-biblioref">2022</a>)</span></li>
<li><span class="citation" data-cites="connell2022genellm">Connell, Khan, and Keiser (<a href="#ref-connell2022genellm" role="doc-biblioref">2022</a>)</span></li>
<li><span class="citation" data-cites="shen2023generative">Shen et al. (<a href="#ref-shen2023generative" role="doc-biblioref">2023</a>)</span></li>
<li><span class="citation" data-cites="ma2023single">Ma et al. (<a href="#ref-ma2023single" role="doc-biblioref">2023</a>)</span></li>
<li><span class="citation" data-cites="cui2022scformer">Cui et al. (<a href="#ref-cui2022scformer" role="doc-biblioref">2022</a>)</span>, rebranded as <span class="citation" data-cites="cui2023scgpt">Cui et al. (<a href="#ref-cui2023scgpt" role="doc-biblioref">2023</a>)</span></li>
<li><span class="citation" data-cites="theodoris2023">Theodoris et al. (<a href="#ref-theodoris2023" role="doc-biblioref">2023</a>)</span></li>
<li><span class="citation" data-cites="hao2023foundation">Hao et al. (<a href="#ref-hao2023foundation" role="doc-biblioref">2023</a>)</span></li>
<li><span class="citation" data-cites="gong2023xtrimogene">Gong et al. (<a href="#ref-gong2023xtrimogene" role="doc-biblioref">2023</a>)</span></li>
</ul>
<p>(to be continued)</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-smola2019attn" class="csl-entry" role="listitem">
Alex Smola, Ashton Zhang. 2019. <span>“A Tutorial on Attention in Deep Learning.”</span> 2019. <a href="https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4343">https://icml.cc/Conferences/2019/ScheduleMultitrack?event=4343</a>.
</div>
<div id="ref-bloem2019transformers" class="csl-entry" role="listitem">
Bloem, Peter. 2019. <span>“Transformers from Scratch.”</span> 2019. <a href="https://peterbloem.nl/blog/transformers">https://peterbloem.nl/blog/transformers</a>.
</div>
<div id="ref-bommasani2021opportunities" class="csl-entry" role="listitem">
Bommasani, Rishi, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, et al. 2021. <span>“On the Opportunities and Risks of Foundation Models.”</span> <em>arXiv Preprint arXiv:2108.07258</em>.
</div>
<div id="ref-brown2020language" class="csl-entry" role="listitem">
Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.
</div>
<div id="ref-connell2022genellm" class="csl-entry" role="listitem">
Connell, William, Umair Khan, and Michael J. Keiser. 2022. <span>“A Single-Cell Gene Expression Language Model.”</span> <em>arXiv Preprint arXiv:2210.14330</em>. <a href="https://doi.org/10.48550/arXiv.2210.14330">https://doi.org/10.48550/arXiv.2210.14330</a>.
</div>
<div id="ref-cover1965geometrical" class="csl-entry" role="listitem">
Cover, Thomas M. 1965. <span>“Geometrical and Statistical Properties of Systems of Linear Inequalities with Applications in Pattern Recognition.”</span> <em>IEEE Transactions on Electronic Computers</em>, no. 3: 326–34.
</div>
<div id="ref-cui2022scformer" class="csl-entry" role="listitem">
Cui, Haotian, Chloe Wang, Hassaan Maan, Nan Duan, and Bo Wang. 2022. <span>“scFormer: A Universal Representation Learning Approach for Single-Cell Data Using Transformers.”</span> <em>bioRxiv</em>, 2022–11.
</div>
<div id="ref-cui2023scgpt" class="csl-entry" role="listitem">
Cui, Haotian, Chloe Wang, Hassaan Maan, and Bo Wang. 2023. <span>“scGPT: Towards Building a Foundation Model for Single-Cell Multi-Omics Using Generative AI.”</span> <em>bioRxiv</em>, 2023–04.
</div>
<div id="ref-gong2023xtrimogene" class="csl-entry" role="listitem">
Gong, Jing, Minsheng Hao, Xin Zeng, Chiming Liu, Jianzhu Ma, Xingyi Cheng, Taifeng Wang, Xuegong Zhang, and Le Song. 2023. <span>“xTrimoGene: An Efficient and Scalable Representation Learner for Single-Cell RNA-Seq Data.”</span> <em>bioRxiv</em>, 2023–03.
</div>
<div id="ref-hao2023foundation" class="csl-entry" role="listitem">
Hao, Minsheng, Jing Gong, Xin Zeng, Chiming Liu, Yucheng Guo, Xingyi Cheng, Taifeng Wang, Jianzhu Ma, Le Song, and Xuegong Zhang. 2023. <span>“Large Scale Foundation Model on Single-Cell Transcriptomics.”</span> <em>bioRxiv</em>. <a href="https://doi.org/10.1101/2023.05.29.542705">https://doi.org/10.1101/2023.05.29.542705</a>.
</div>
<div id="ref-karpathy2023llm" class="csl-entry" role="listitem">
Karpathy, Andrej. 2023. <span>“Neural Networks: Zero to Hero.”</span> 2023. <a href="https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">https://www.youtube.com/playlist?list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ</a>.
</div>
<div id="ref-le2021transformer" class="csl-entry" role="listitem">
Le, Nguyen Quoc Khanh, Quang-Thai Ho, Trinh-Trung-Duong Nguyen, and Yu-Yen Ou. 2021. <span>“A Transformer Architecture Based on BERT and 2D Convolutional Neural Network to Identify DNA Enhancers from Sequence Information.”</span> <em>Briefings in Bioinformatics</em> 22 (5): bbab005.
</div>
<div id="ref-luecken2021sandbox" class="csl-entry" role="listitem">
Luecken, Malte D, Daniel Bernard Burkhardt, Robrecht Cannoodt, Christopher Lance, Aditi Agrawal, Hananeh Aliee, Ann T Chen, et al. 2021. <span>“A Sandbox for Prediction and Integration of Dna, Rna, and Proteins in Single Cells.”</span> In <em>Thirty-Fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2)</em>.
</div>
<div id="ref-ma2023single" class="csl-entry" role="listitem">
Ma, Anjun, Xiaoying Wang, Jingxian Li, Cankun Wang, Tong Xiao, Yuntao Liu, Hao Cheng, et al. 2023. <span>“Single-Cell Biological Network Inference Using a Heterogeneous Graph Transformer.”</span> <em>Nature Communications</em> 14 (1): 964.
</div>
<div id="ref-phuong2022formal" class="csl-entry" role="listitem">
Phuong, Mary, and Marcus Hutter. 2022. <span>“Formal Algorithms for Transformers.”</span> <em>arXiv Preprint arXiv:2207.09238</em>.
</div>
<div id="ref-radford2019language" class="csl-entry" role="listitem">
Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. <span>“Language Models Are Unsupervised Multitask Learners.”</span> <em>OpenAI Blog</em> 1 (8): 9.
</div>
<div id="ref-raschka2023llm" class="csl-entry" role="listitem">
Raschka, Sebastian. 2023. <span>“Understanding Large Language Models.”</span> 2023. <a href="https://magazine.sebastianraschka.com/p/understanding-large-language-models">https://magazine.sebastianraschka.com/p/understanding-large-language-models</a>.
</div>
<div id="ref-schlag2021linear" class="csl-entry" role="listitem">
Schlag, Imanol, Kazuki Irie, and Jürgen Schmidhuber. 2021. <span>“Linear Transformers Are Secretly Fast Weight Programmers.”</span> In <em>International Conference on Machine Learning</em>, 9355–66. PMLR.
</div>
<div id="ref-shen2023generative" class="csl-entry" role="listitem">
Shen, H., J. Liu, J. Hu, X. Shen, C. Zhang, D. Wu, M. Feng, et al. 2023. <span>“Generative Pretraining from Large-Scale Transcriptomes for Single-Cell Deciphering.”</span> <em>iScience</em> 26 (5).
</div>
<div id="ref-theodoris2023" class="csl-entry" role="listitem">
Theodoris, Christina V., Ling Xiao, Anant Chopra, Mark D. Chaffin, Zeina R. Al Sayed, Matthew C. Hill, Helene Mantineo, et al. 2023. <span>“Transfer Learning Enables Predictions in Network Biology.”</span> <em>Nature</em>. <a href="https://doi.org/10.1038/s41586-023-06139-9">https://doi.org/10.1038/s41586-023-06139-9</a>.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-weng2018attention" class="csl-entry" role="listitem">
Weng, Lilian. 2018. <span>“Attention? Attention!”</span> <em>Lilianweng.github.io</em>. <a href="https://lilianweng.github.io/posts/2018-06-24-attention/">https://lilianweng.github.io/posts/2018-06-24-attention/</a>.
</div>
<div id="ref-yang2022scbert" class="csl-entry" role="listitem">
Yang, Fan, Wenchuan Wang, Fang Wang, Yuan Fang, Duyu Tang, Junzhou Huang, Hui Lu, and Jianhua Yao. 2022. <span>“scBERT as a Large-Scale Pretrained Deep Language Model for Cell Type Annotation of Single-Cell RNA-Seq Data.”</span> <em>Nature Machine Intelligence</em> 4 (10): 852–66.
</div>
<div id="ref-zaheer2017deep" class="csl-entry" role="listitem">
Zaheer, Manzil, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and Alexander J Smola. 2017. <span>“Deep Sets.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>