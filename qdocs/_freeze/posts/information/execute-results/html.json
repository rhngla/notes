{
  "hash": "75351b38c810039ea01d38ecea52ac8a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Information, entropy, and KL divergence\"\nauthor: \"Rohan Gala\"\ndate: \"2023-07-13\"\ndate-modified: \"2023-08-01\"\ncategories: [\"fundamentals\", \"math\", \"machine learning\"]\ntoc: true\nnumber-sections: true\nnumber-depth: 2\nhighlight-style: github\nhtml-math-method: katex\nbibliography: \"../refs.bib\"\ncode-fold: true\npage-navigation: false\n---\n\nInformation and entropy are important concepts in modern statistical learning. The origins of these closely related concepts can be traced back to physical systems. Classical thermodynamics and heat engines motivate a continuous perspective, and Shannon's information theory for data encoding and decoding motivate a discrete perspective for these concepts. \n\nWhile these physical systems can help build intuition, leaning hard on analogies can make it confusing for the problem at hand. Here I present a self-contained introduction to these concepts without reference to physical systems.\n\nI'll assume continuous values for our random variables throughout. For the discrete case, the only change is to think of $p_X$ and $q_X$ as a probability mass functions (rather than a density functions), and the rest of the description on this page holds.\n\n| Quantity         | Symbol      | Description    |\n| :----            | :---        | :------------- |\n|`random variable` | $X$         | Function that maps events to values                         |\n|`model`           | $q_X$       | A (controllable) probability density function               |\n|`density`         | $q_X(x)$    | Probability density at a particular value $x$               |\n|`draw`            | $x\\sim q_X$ | A value for a random variable, drawn from the `model` $q_X$ |\n|`observation`     | $x\\sim p_X$ | An observed value for a random variable from $p_X$          |\n: Notation and definitions {#tbl-terminology tbl-colwidths=\"[25,15,55]\"}{.striped .hover}\n\n**Note:** \n\n1. On this page, I use `draw`, `model` and `density` to refer to quantities related to $q_X$ as shown in the table. The plain-text model, density etc. refer to the general concepts.\n\n2. We'll drop the subscript $X$ from the distributions and use $p = p_X$ and $q = q_X$ to simplify notation. \n\n### Set up\n\n - We are given observations $x$ from a distribution $p$.\n - We cannot directly access $p$, only observations from it.\n - We want to find a `model` $q$ that approximates $p$ well.\n - Assume that `model` $q$ can be tuned via parameters $\\theta$ $^\\dagger$.\n\n:::{.column-margin}\n${^\\dagger}$ The $\\theta$ dependence can be explicitly denoted in the `model` as $q_{\\theta}$, or the `density` (and therefore likelihood) as $q(x;\\theta)$. Likelihood can thus be viewed as a function of $\\theta$. The semi-colon indicates that $\\theta$ refers to parameters, and not a random variable that comes with its own distribution.\n\n<br>\n\nIn the Bayesian view, $\\theta$ itself may be considered as a particular value of a random variable $\\Theta$, and this may be indicated in the distribution as $q_{X|\\Theta}$.\n:::\n\n### Likelihood and information\n\n<p></p>\n\nGiven an observation $x$:\n\n - We can interpret $q(x)$ as how _likely_ an `observation` is under the `model`.\n - So we refer to $q(x)$ as the likelihood.\n - Following this, the quantity $-\\log{q(x)}$ is called negative log-likelihood.\n\n<p></p>\n\n - Further, if density $q(x)$ is small, we can interpret the observation $x$ as _surprising_.\n - The smaller the value of $q(x)$, the larger is the value of $-\\log{q(x)}$.\n - Large values of negative log-likelihood thus convey surprise.\n - If we had access to $p$, we could similarly calculate surprise in `observations` under $p$.\n - The quantity $-\\log{p(x)}$ is known as information.\n\n<p></p>\n\n - The quantities $-\\log{q(x)}$ and $-\\log{p(x)}$ look identical, yet have different names.\n - $-\\log{p(x)}$ tells us something about the information content of an observation.\n - $-\\log{q(x)}$ conveys something about our approximation $q$ (and therefore $\\theta$).\n - This is why it's helpful to retain the terminology, and interpret it as such.\n\n:::{.column-margin}\nWhy use the $\\log$ operation to define information?\nA cryptic, partial answer: independence of distributions $\\implies$ addition of information (and of log-likelihoods).\n:::\n\n### Entropy\n\n<p></p>\n\n - As a description of the model, we can calculate the expected surprise over `draws` $x \\sim q$.\n - Expected surprise is simply the avg. negative log-likelihood, as above.\n - Expected surprise with `draws`: $E_{x \\sim q}({-\\log q(x)})$.\n - Expected surprise with `observations`: $E_{x \\sim p}({-\\log q(x)})$.\n\n:::{.column-margin}\nExpectation is usually an operator applied to a function of a random variable, indicated as $E[f(X)]$. The distribution over which the expectation is calculated is often implicit. \n\n<br>\n\nHere we treat expectation as a function of values specified in the argument, drawn from a distribution specified in the subscript, $E_{x \\sim p}(f(x))$. This is a commonly adopted convention that I'll follow here.\n:::\n\n - Expected surprise with `draws` is called `entropy` of $q$.\n - Expected surprise with `observation` is called `cross-entropy` of $q$ w.r.t. $p$.\n - Minimizing `cross-entropy` is simply minimizing the expected surprise in `observations`, and is one way to tune $q$ through $\\theta$.\n - This is equivalent to maximizing the likelihood of `observations` under $q$!\n\n::: {.callout-note appearance=\"simple\" collapse=false icon=false}\n\n## Monte Carlo estimate of Expectations\n\n - Monte Carlo integration enables approximation of integrals over regions as sums over samples drawn from those regions.\n - In particular, for regions that make up a valid probability distribution $p$:\n\n\\begin{aligned}\n\\int f(x) p(x) dx \\approx \\frac{1}{N} \\sum_{i=1}^N f(x_i) \\\\\n\\end{aligned}\n\n - We can thus express expectation over a probability distribution as a sum over samples!\n - This requires that samples are drawn i.i.d.$^{\\dagger\\dagger}$ according to the distribution $p$.\n - The larger $N$ is, the better the approximation is.\n - We often formulate problems so that $p$ is some simple, low-dimensional distribution so that a relatively small number of samples can approximate such integrals well.\n - How to efficiently sample from various kinds of $p$ (e.g. has no explicit functional form, is high dimensional etc.) is an active research area.\n\n <p></p>\nExample:\n\n - Consider cross entropy, where the integral is over the distribution $p$.\n - In this case, we don't have access to $p$ directly, only `observations`.\n - Nevertheless, we'll assume `observations` are i.i.d. according to $p$.\n - Using $i$ to index such observations $x_i \\sim p$, we have an _estimator_ of the cross-entropy as a sum over `observations`:\n\n\n\\begin{aligned}\nE_{x \\sim p}({-\\log q(x)}) &= \\int p(x) (-\\log q(x)) dx \\\\\n&\\approx \\frac{1}{N} \\sum_{i=1}^N -\\log q(x_i)\n\\end{aligned}\n\n$^{\\dagger\\dagger}$ i.i.d: independent and identically distributed.\n:::\n\n### KL Divergence and Gibbs inequality\n\n<p></p>\n\n| Quantity | Refers to |\n| :------- | ----: |\n| $H(q) = E_{x \\sim q}({-\\log q(x)})$ | Entropy of $q$ |\n| $H(p) = E_{x \\sim p}({-\\log p(x)})$ | Entropy of $p$ | \n| $H(p, q) = E_{x \\sim p}({-\\log q(x)})$ | Cross entropy ($q$ w.r.t. $p$) |\n| $D_\\textrm{KL}(p | q) = E_{x \\sim p}({-\\log q(x)} -(-\\log p(x)))$ | KL Divergence of $q$ w.r.t. $p$|\n|$D_\\textrm{KL}(p | q) = H(p,q) - H(p)$ | Relative entropy interpretation |\n|$D_\\textrm{KL}(p | q) = H(p,q) - H(p) \\geq 0$ | Gibbs inequality |\n: Quantities related to information  {.striped .hover}\n\n<p></p>\n\n - Gibb's inequality is a consequence of Jensen's inequality (see proof below).\n - It shows that `cross-entropy` is always greater than or equal to `entropy`.\n - The `cross-entropy` is minimized when $q = p$.\n - KL divergence is zero when $q = p$.\n - $D_\\textrm{KL}(p | q)$ is often presented as $E_{x \\sim p}(\\log \\frac{p(x)}{q(x)})$ where the connection to entropy is harder to see.\n\n::: {.callout-note appearance=\"simple\" collapse=false icon=false}\n\n## Gibb's inequality proof\n\n\\begin{aligned}\nD_\\textrm{KL}(p | q) &= E_{x \\sim p}({-\\log q(x)} -(-\\log p(x))) \\\\\n&= E_{x \\sim p}(-\\log \\frac{q(x)}{p(x)}) \\\\\n\\end{aligned}\n - Jensen's inequality: For a convex function $\\phi$: $E_{x \\sim p}(\\phi(x))~\\geq~\\phi(E_{x \\sim p}(x))$.\n - $-\\log$ is a convex function.\n\\begin{aligned}\nE_{x \\sim p}(-\\log \\frac{q(x)}{p(x)})~&\\geq -\\log E_{x \\sim p}(\\frac{q(x)}{p(x)}) \\\\\n&\\geq -\\log \\int p(x) \\frac{q(x)}{p(x)} dx \\\\\n&\\geq -\\log \\int q(x) dx \\\\\n&\\geq -\\log (1) \\\\\n&\\geq 0 \\\\\n\\implies D_\\textrm{KL}(p | q) &\\geq 0\n\\end{aligned}\n\nFor the discrete case, the integral is replaced by a sum.\n\n:::\n\n<p></p>\n\n### A recap and some intuitions\n\n - We can't access $p$ directly, only `observations` from it.\n - We're proposing the `model` $q$ as an approximation to $p$.\n - Maximizing likelihood, minimizing cross-entropy, and minimizing KL divergence are closely related concepts to find a $q$ that approximates $p$.\n\n<p></p>\n\nRe. entropy:\n\n - The lower the entropy, the less surprising samples are on average.\n - Sampling from a sharply peaked distribution will be confined mostly to a small region.\n - Such samples will have high likelihood (by definition).\n - So a sharply peaked distribution has low entropy.\n - In statistical mechanics, structure in a system is related to 'peaky' distributions.\n - A more structured system means the related distribution has lower entropy.\n - Conversely, a more disordered system means the related distribution has higher entropy.\n\n<p></p>\nA numerical example with Gaussians:\n\n - We consider $p = \\mathcal{N}(0, 1)$\n - $q$ is also gaussian, but parameters $\\mu$ or $\\sigma$ are varied.\n - Here we don't explore algorithms to find $q$.\n - The goal is simply to compare entropy-based quantities, which serve as a basis for such algorithms.\n\n<p></p>\n\n - In the left plot, we fix $\\sigma = 1$ and vary $\\mu$ for $q$.\n - Entropy is related to shape of distributions.\n - $\\sigma$ determines the shape of the Gaussian.\n - Therefore, $H(q)$ remains unchanged.\n - The minimum occurs at $\\mu = 0$, i.e. where $q$ matches $p$.\n - The difference between $H(p,q)$ and $D_\\textrm{KL}(p | q)$ at the minimum is $H(p)$.\n\n<p></p>\n\n - In the right plot, we fix $\\mu = 1$ and vary $\\sigma$ for $q$.\n - Entropy $H(q)$ is small when $q$ is sharply peaked (i.e. small $\\sigma$).\n - As $\\sigma$ inreases, $q$ spreads out and $H(q)$ becomes larger.\n - The minimum occurs at $\\sigma = 1$, i.e. where $q$ matches $p$.\n - $H(p,q) - D_\\textrm{KL}(p | q) = H(p)$ at the minimum.\n\n::: {#d695359d .cell execution_count=1}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport torch\n\ncustom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\nsns.set_theme(style=\"ticks\", font_scale=0.8, rc=custom_params)\n\n# p as a fixed Gaussian\nmu_p, std_p = 0.0, 1.0\np = torch.distributions.Normal(loc=mu_p, scale=std_p)\nx_obs = p.sample((10000,))\n\n# H(p) calculations:\nHp_theory = p.entropy()\nHp_emp = -p.log_prob(x_obs).mean()\n\n# q as Gaussians with varying mu\nDpq = []\nHpq = []\nHq = []\nq_mu = np.arange(-4, 4, 0.4)\nq_sd = 1.0\nfor mu in q_mu:\n    q = torch.distributions.Normal(loc=mu, scale=q_sd)\n    Dpq.append(torch.distributions.kl_divergence(p, q))\n    Hpq.append(-q.log_prob(x_obs).mean())\n    Hq.append(q.entropy())\n\nf, ax = plt.subplots(1, 2, figsize=(6, 3))\nax[0].plot(q_mu[[1, -1]], [Hp_emp, Hp_emp], '-b', label=r'$H(p)$')\nax[0].plot(q_mu, Hq, '.r', label=r'$H(q)$')\nax[0].plot(q_mu, Dpq, '-g', label=r'$D_{KL}(p,q)$')\nax[0].plot(q_mu, Hpq, '-k', alpha=0.5, label=r'$H(p,q)$')\nax[0].set(xlabel=r'$\\mu$', \n          ylabel='Entropy [nats]',\n          title=r'$p=\\mathcal{N}(0,1), q=\\mathcal{N}(\\mu, 1)$')\nax[0].grid()\n\n# q as Gaussians with varying sigma\nDpq = []\nHpq = []\nHq = []\nq_sd = np.arange(0.5, 2, 0.05)\nq_mu = 0.0\nfor sd in q_sd:\n    q = torch.distributions.Normal(loc=q_mu, scale=sd)\n    Dpq.append(torch.distributions.kl_divergence(p, q))\n    Hpq.append(-q.log_prob(x_obs).mean())\n    Hq.append(q.entropy())\nax[1].plot(q_sd[[1, -1]], [Hp_emp, Hp_emp], '-b', label=r'$H(p)$')\nax[1].plot(q_sd, Hq, '.r', label=r'$H(q)$')\nax[1].plot(q_sd, Dpq, '-g', label=r'$D_{KL}(p,q)$')\nax[1].plot(q_sd, Hpq, '-k', alpha=0.5, label=r'$H(p,q)$')\nax[1].set(xlabel=r'$\\sigma$', \n          ylabel='Entropy [nats]',\n          title=r'$p=\\mathcal{N}(0,1), q=\\mathcal{N}(0,\\sigma)$')\nax[1].grid()\nax[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](information_files/figure-html/cell-2-output-1.png){width=566 height=277}\n:::\n:::\n\n\n### Acknowledgments\n\n[Anna Grim](https://annamgrim.com/), [Łukasz Kuśmierz](https://scholar.google.com/citations?user=U6B_HNYAAAAJ&hl=en&oi=ao), [Priyanka Kulkarni](https://scholar.google.com/citations?hl=en&user=ViilpdV5QxsC) and [Shreyas Potnis](https://scholar.google.com/citations?user=WxTzovMAAAAJ) for helpful comments and suggestions.\n\n",
    "supporting": [
      "information_files"
    ],
    "filters": [],
    "includes": {}
  }
}