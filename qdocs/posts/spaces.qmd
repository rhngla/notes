---
title: "Concepts in dimensionality reduction"
author: "Rohan Gala"
date: "2023-04-07"
categories: [math]
image: "image.jpg"
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
---


We'll start with some definitions - the goal is to situate ourselves with:
 
 - `objects` we'll work with, 
 - `spaces` they live in
 - `rules` that govern relationships and operations between them.
 
 Ultimately we want to construct analysis tools i.e. operations to manipulate such objects constructed from data, and discover relationships without violating (or perhaps  knowingly violating) definitions/ underlying assumptions. Further, through these definitions we will become familiar with hierarchies of such objects, spaces, and rules. Some definitions are more abstract, and some are special cases with additional rules. Being aware of where in the abstraction hierarchy we choose to operate is helpful to manage complexity.

# Fundamentals

## Metric and metric space {#sec-metricspace}

A distance metric $d$ is defined on set $M$
 $$
 d:M \times M \rightarrow \mathbb{R}
 $$

A distance metric is sometimes called just "distance" or just "metric". For all $x,y,z \in M$ the following relationships hold for any valid distance metric:
$$
\begin{aligned}
d(x, y) &\geq 0 \\
d(x, y) &=0 \Longleftrightarrow x=y \\
d(x, y) &=d(y, x) \\
d(x, z) &\leq d(x, y)+d(y, z)
\end{aligned}
$${#eq-metric}

A metric space is defined as the ordered pair $(M,d)$, where $M$ is a set, and $d$ is a distance metric. Reference to the metric $d$ is occasionally omitted understood from the context, and the metric space is denoted simply by $(M)$.

::: {.column-margin}

Cosine distance, defined for vectors $x$ and $y$ in Euclidean space as $1-\cos\theta$ where $\theta = \frac{x \cdot y}{\|x\| \|y\|}$, is not a distance metric in general (e.g. $d(x, 2x) = 0$, but $x\neq 2x$). However, it becomes a valid distance metric if we restrict our set of vectors to only those that have a fixed norm ($\| x \| = c$).
:::

## Field {#sec-field}

A field is a set $F$ which includes $0$ and $1$ and binary operations of addition $+$ and multiplication $\cdot$ such that for all elements $x,y,z \in F$ the following hold:

$$
\begin{aligned}
0 + x &= x \\
x + (-x) &= 0, \quad -x \in F \\
x + y &= y + x \\
x + (y + z) &= (x + y) + z \\
1 \cdot x &= 1 \\
x^{-1} \cdot x &= 1, \quad \forall x \neq 0 \\
x \cdot y &= y \cdot x \\
x \cdot (y \cdot z) &= (x \cdot y) \cdot z \\
x \cdot (y + z) &= (x \cdot y) + (x \cdot z) \\
\end{aligned}
$${#eq-vectorspace}

We typically deal with the real field $\mathbb{R}$ or complex field $\mathbb{C}$

## Vector space {#sec-vectorspace}

A vector space over a field $F$ is a set $V$ together with operations of addition and scalar multiplication) such that for all vectors $x,y,z \in V$ and all scalars $\alpha, \beta \in \mathbb{F}$ the following hold:

$$
\begin{aligned}
x + y &= y + x \\
(x + y) + z &= x + (y + z) \\
x + 0 &= x \\
x + (-x) &= 0 \\
\alpha (x + y) &= \alpha x + \alpha y \\
(\alpha + \beta) x &= \alpha x + \beta x \\
\alpha (\beta x) &= (\alpha \beta) x \\
1x &= x
\end{aligned}
$${#eq-vectorspace}

## Norm {#sec-norm}

A norm on a vector space $V$ is a function $w:V \rightarrow \mathbb{R}$ that satisfies:
$$
\begin{aligned}
w(x) &\geq 0 \\
w(x) &= 0 \implies x = 0 \\
w(\alpha x) &= \lvert\alpha\rvert w(x) \\
w(x+y) &\leq w(x) + w(y)
\end{aligned}
$${#eq-norm}

 - Norms induce a distance metric on the vector space, $d (x, y) = w (x âˆ’ y)$, and such a normed vector space is therefore also a metric space.
 - Not all metric spaces are normed vector spaces. That is, there exist metric spaces that are not normed vector spaces.
 - A metric can define an _induced norm_ $w$ over a vector space (via $w(x) = d(x,0)$) if it satisfies the following:
$$
\begin{aligned}
d(w,v) &= d(u+w, u+v) \\
d(\alpha u,\alpha v) &= \lvert\alpha\rvert d(u,v)
\end{aligned}
$${#eq-inducednorm}

## Inner product space {#sec-innerproductspace}

An inner product space is a vector space $V$ over the field $F$ together with an inner product. An inner product is a map $\langle \cdot ,\cdot \rangle :V \times V \to F$, that satisfies the properties for all vectors $x,y,z\in V$ and all scalars $a,b \in F$. 
$$
\begin{aligned}
\langle x,y\rangle &={\overline {\langle y,x\rangle }} \\
\langle ax+by,z\rangle &=a \langle x,z\rangle +b\langle y,z\rangle \\
\langle x,x\rangle &\geq 0 \\
\langle x,x\rangle = 0 &\iff x = 0 \\
\end{aligned}
$${#eq-innerproduct}

 - Conjugate symmetry implies $\langle x,x\rangle \in \mathbb{R}$. 
 - In many applications, $F$ is $\mathbb{R}$, and conjugate symmetry reduces to symmetry.
 - Every inner product space induces a norm, called its _canonical norm_: 
 $$\|x\|={\sqrt {\langle x,x\rangle }}$$


::: {.column-margin}
$p$-norm (or $L_p$-norm) is defined over a vector space as:
$$
\| x \| = \sqrt[p]{\sum_{i=1}^n x_i^p}
$$

The distance induced by the $p$-norm is also known as the Minkowski distance of order $p$.
:::

### Euclidean space {#sec-euc_manifold}

An $n$-dimensional Euclidean space is a particular metric space. In fact, it is a vector space equipped with an inner product. Vectors in Euclidean space are denoted as $x \in \mathbb{R}^n$. 

 - The inner product for two vectors $x$ and $y$ is defined as $\langle x, y \rangle = x \cdot y$, where $x \cdot y = \sum_{i=1}^n x_i y_i$.
 - The inner product induces a norm (also known as 2-norm, following the definition of the more general case of $p$-norm) $\| x \| = \sqrt{x \cdot x}$.
 - The norm induces a distance metric: $d_{\textrm{euclidean}}(x,y) = \| x - y \|_2$. 

We often assume we are working with a Euclidean metric space $(\mathbb{R}^n, d_{\textrm{euclidean}})$ without explicitly stating definitions. However, considering these definitions make it easier to understand the more general case of a Riemannian manifold.

## Reimannian Manifold

Textbooks on differential geometry, e.g. @andrews2020dg would provide a more formal entry point and description for Manifolds. I'd like to point out connections here to Euclidean geometry to build some intuition first. In geometry taught in school, Euclidean geometry is introduced through its [axioms](https://en.wikipedia.org/wiki/Euclidean_geometry). In particular, relaxing the fifth postulate (which states that parallel lines do not meet) leads to ideas of spherical and hyperbolic geometry, which can also be viewed as special cases of Reimannian manifolds that have constant, global _curvature_.

Generalizing this, we can consider Riemannian manifolds that can have local curvature. This local curvature is allowed to vary smoothly from one position to the next along the manifold. The local region around each position can be treated as a Euclidean space (we'll introduce the notion of tangent space for this). We can choose a coordinate system locally to account for the local curvature, and then use formal machinery from Euclidean spaces there.

However, as we move along manifold, we'd like to modify the co-ordinate system itself to account for the local curvature. Because we are working with a smoothly varying curvature, we can do this using smooth transformations of the co-ordinate system - for which we use tools from calculus.

### Jacobians and coordinate transformations



### Notation
A Riemannian manifold is a tuple $(M,g)$ where $M$ is a set of points and $g$ refers to inner products on vector spaces that we define at each point $p \in M$. Vectors that are tangent to the point $p$ make up the tangent space denoted by $T_{p} M$. 

Let point $p$ and a local co-ordinate system denoted as $(x^1,...,x^n)$

The set where we define each object as a differential operator $\mathbf{A} = (A^1\frac{\partial}{\partial x^1},...,A^n\frac{\partial}{\partial x^n})$, s.t. $A^i \in \mathbb{R}$. Such a set satisfies definition of a vector space. Moreover, each object is tangent to the manifold because of the partial derivatives. Thus, we now have a way to specify objects belonging to the tangent space $T_{p} M$.

Consider the change of co-ordinates from $(x^1,...,x^n)$ to $(y^1,...,y^n)$, where $y^i = f^i(x^1,...,x^n)$. The Jacobian matrix is defined as $J_{ij} = \frac{\partial y^i}{\partial x^j}$.



::: {.column-margin}

The differential operators actually define a  _vector field_; each vector field is a vector space on itself. 
:::

We denote the inner product at $p \in M$ as the map: $g_{p}: T_{p} M \times T_{p} M \rightarrow \mathbb{R}$.

For two vectors $\mathbf{A}$ and $\mathbf{B}$ in $T_{p} M$, $\langle \mathbf{A}, \mathbf{B} \rangle _{g_{p}}$, the inner product 

Through the distance metric induced by the  canonical norm, it is easy to work out that Euclidean space is a special case of a Reimannian Manifold where g_{p} is an identity matrix.

### Local
Recall that an inner product induces a norm, and the norm induces a distance metric. Since the inner product is locally defined at each point, we can think of the distance metric as being defined at each point. Using this local notion of distance, we can calculate distances between any two points on the manifold through integration. This we have a set of points $M$, where we can calculate distances between any two points

Note the similarity to definition of a metric space, @sec-metricspace.

Note that the metric tensor is an inner product on the tangent space $T_{p}M$. Moreover, the tangent space is a vector space. Recall that any inner product induces a norm, which in turm induces a distance metric. Thus, there is notion of a distance metric for each point on the manifold (and the reason why $g_p$ is called a metric tensor).

> Note: For the Euclidean manifold, the metric tensor is the identity matrix. 


## Heirarchy of geometric transformations
  - Diffeomorphic: Preserves angles, distances, and shape
  - Isometric: Preserves angles and distances
  - Conformal: Preserves angles

# Dimensionality reduction

Autoencoders have been used for nonlinear dimensionality reduction, manifold learning, and clustering @tschannen2018recent. They have been adapted to learn geometry preserving representations, e.g. @yonghyeon2021regularized, @peterfreund2020local. 

Measuring distortion:
 - @mcqueen2016nearly 


### Measuring distortion for scRNA-seq

#### Structure of the data matters

#### Local similarity

KNN-based measures; pick a neighborhood, and quantify overlap of neighborhood sets in reference space and representation.

#### Global similarity

Clustering based approaches. Cluster data in reference and representation spaces, quantify overlap of clusters across the two spaces. Can use measures like ARI, NMI, PSI (point set index, @rezaei2016set).

## Curse of high dimensionality {#sec-highdim-curse}

Intuition for properties of data in 2d / 3d space don't carry over to high dimensional spaces.

1. Most of the volume is at the surface of the hypersphere in high D
2. This is a consequence of the phenomena of "concentration of measure"

Further reading: @aggarwal2001surprising, Chapter 1 of @lee2007nonlinear.

## Manifold assumption

> Many methods attempt to use point samples in high dimensional space to parametrize a low dimensional manifold.

Paraphrasing @ganea2019non, we assume that data lies on a manifold of much smaller dimension (latent) than the ambient dimension (measured). This enables learning and generalization to unseen data points. We further assume that it is a metric space $(Y, d_Y)$ and has a smooth differentiable structure that allows learning via optimization. 

Thus we are interested in Riemannian manifolds of intrinsic dimension $d \leq n$. The hope is we can achieve this by learning an a mapping function $f : X \rightarrow Y$ from the input high dimensional data space to the low-dimensional representation space.

## Dimensionality reduction

These set of [notes](https://drewwilimitis.github.io/Manifold-Learning/) contain python implementation of common methods. 


 - Many methods to reduce dimensionality in a way that preserves pair-wise distances / similarities
 - MDS is a linear method
 - Isomap preserves geodesic distances
 - Diffusion maps consider a notion of reachability of distant points by constructing a diffusion process
 - Laplacian eigenmaps, T-sne, Umap etc. are other nonlinear methods

Random projections, random sparse projections can be considered as baselines. Constructing such transforms has links to the Johnson Lindenstrauss lemma, and there is research on fast construction of useful random transforms. E.g. see Johnson-Lindenstrauss transforms referred to in this [blog post](https://tevenlescao.github.io/blog/fastpages/jupyter/2020/06/18/JL-Lemma-+-Linformer.html) demonstrating applications to transformers. 

The proof in @dasgupta2003elementary for the Johnson Lindenstrauss lemma seems accessible. 

Some references from [this twitter thread](https://twitter.com/lpachter/status/1431325969411821572?s=21&t=5TsIwpYk5rwp--JHbq_RwA)

  [Nearest component analysis](https://www.cs.toronto.edu/~hinton/absps/nca.pdf)

https://ieeexplore.ieee.org/abstract/document/1661543 - Diffusion maps and compression - Diffusion maps and coarse-graining: a unified framework for dimensionality reduction, graph partitioning, and data set parameterization

[Sparse random projections](https://icml.cc/2011/papers/551_icmlpaper.pdf)

# Evaluating distortion

## Using Jaccard similarity

Consider representations of point $p$ embedded in two metric spaces $(X, d_{X}) and in $(Y, d_{Y})$. We denote the k-nearest neighbors of $p$ in the two metric spaces as $K_{X}(p)$ and $K_{Y}(p)$. The Jaccard similarity between the neighborhood sets is defined as:
$$
J_{XY}(p) = \frac{|K_{X}(p) \cap K_{Y}(p)|}{|K_{X}(p) \cup K_{Y}(p)|}
$$

Such a quantification was recently proposed in the context of scRNA-seq data by @cooley2019novel.

# Isometry in autoencoder representations

Relative representations: https://arxiv.org/pdf/2209.15430.pdf

# Cell types and manifolds
 - We assume that cellular identity and diversity that we would like to interpret can be captured by the position of cells on a low-dimensional manifold.
 - Further, we assume that the (high dimensional) feature set we want to measure (e.g. genome-wide mRNA abundance) is explained well by some stochastic generative process.
 - Such a generative process takes co-ordinates along a low dimensional manifold as input, and produces the feature values one attempts to measure experimentally.
 - The measurement procedure introduces undesirable noise in these feature values. 
 - Our hope is that we have: 
    i. a well-understood noise model with its own parameters to explain variability in the observed values
    ii. a way to learn the manifold from the structure of the data (cells x genes matrix)
    iii. a metric on the manifold that enables analyses of cellular homogeneity and diversity

Identifying appropriate metric for to measure similarity between cells is a fundamental challenge, and at the core of investigations such as the one in @skinnider2019evaluating. 

# Noise models

# Similarities and distances

[see @ganea2019non for a nice exposition]

# Diffusion maps
How to define a metric such that overall structure of the data is preserved better in lower dimensional?

In high dimensional space, a small amount of noise per dimension can adds up very quickly, and distances are not as reliable. How can we still preserve topology of data?

Diffusion maps are designed to achieve exactly that[@coifman2005geometric; @de2008introduction].
"PAGA" [@wolf2019paga] interprets diffusion maps in the context of single-cell RNA-sequencing data. 

$$
P = K^{-1}D
$$

# Unresolved issues

  - Equivalence of metrics?
  - Bregman divergence?

# Further reading

@suarez2021tutorial survey techniques for distance metric learning towards improving classifier performance. 

### References

::: {#refs}
:::