
---
title: "Information, entropy, and KL divergence"
author: "Rohan Gala"
date: "2023-07-13"
date-modified: last-modified
categories: ["fundamentals", "math", "machine learning"]
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
page-navigation: false
---


Information and entropy are important concepts in modern statistical learning. The origins of these closely related concepts can be traced back to physical systems. Classical thermodynamics and heat engines motivate a continuous perspective, and Shannon's information theory for data encoding and decoding motivate a discrete perspective for these concepts. 

While these physical systems can help build intuition, leaning hard on analogies can make it confusing for the problem at hand. Here I present a self-contained introduction to these concepts without reference to physical systems.

I'll assume continuous values for our random variables throughout. For the discrete case, the only change is to think of $p_X$ as a probability mass function (rather than a density function), and the rest of the description on this page holds.

| Quantity | Refers to | Description |
| :---- | :-----    | :----- |
|`random variable` | $X$  |Function that maps events to values |
|`distribution` | $q_X$| A particular probability density function |
|`density`| $q_X(x)$| Probability density of at a particular value $x$, as specified by $q_X$ |
|`sample` | $x\sim q_X$ | A value for a random variable, drawn from the underlying `distribution` $q_X$|
|`observation`| $x\sim p_X$ | An observed value for a random variable. We typically don't know and cannot directly access $p_X$, but want to approximate it with $q_X$|
: Notation and definitions {#tbl-terminology tbl-colwidths="[25,15,55]"}{.striped .hover}

**Note:** 

1. I'll treat `sample`, `distribution` and `density` specifically to referring quantities related to $q_X$ as shown in the table. The plain-text sample, distribution, density etc. will refer to the general concepts.

2. We'll also drop the subscript $X$ from the distributions and use $p = p_X$ and $q = q_X$ since there are no further dependencies to consider (e.g. no conditional distributions).

### Likelihood and information

<p></p>

 - We are given `observations`, $x \sim p(x)$.
 - We assume an underlying `distribution` $q$. 
 - We interpret `density` $q(x)$ as how _likely_ an `observation` is under the `distribution`.
 - So we refer to $q(x)$ as the likelihood.
 - Following this, the expression $\log q(x)$ is called log-likelihood.

<p></p>

 - Further, if `density` $q(x)$ is small, we say the `observation` is _unlikely_.
 - The more unlikely the `observation`, the more _surprising_ it is.
 - The smaller the value of $q(x)$, the larger is the value of $-\log q(x)$.
 - Large values of negative log-likelihood thus convey surprise.
 - Formally, the negative log-likelihood is called _information_.
 - Notion of surprise requires context of an underlying distribution!


:::{.column-margin}
Why use the $\log$ operation to define information?
A cryptic, partial answer: independence of distributions $\implies$ addition of information.
:::
<p></p>

 - Let's play a similar game with `samples`, $x \sim q$
 - Many `samples` have larger values of $q(x)$
 - Some `samples` will have small values of $q(x)$
 - So analogous to `observations`, some `samples` can be viewed as _surprising_.

### Entropy

<p></p>

 - As a summary statistic, we calculate the expected surprise over `samples`
 - Expected surprise is simply the avg. negative log-likelihood, as above.
 - Expected surprise with `samples`: $E_{x \sim q}({-\log q(x)})$
 - Expected surprise with `observations`: $E_{x \sim p}({-\log q(x)})$

:::{.column-margin}
Expectation is usually an operator applied to a function of a random variable, indicated as $E[f(X)]$. The distribution over which the expectation is calculated is often implicit. 

<br>

Here we treat expectation as a function of values specified in the argument, drawn from a distribution specified in the subscript, $E_{x \sim p}(f(x))$. This is a commonly adopted convention that I'll follow here.
:::



 - Expected surprise with `samples` is called `entropy` of $q$
 - Expected surprise with `observation` is called `cross-entropy` of $q$ w.r.t. $p$
 - Minimizing `cross-entropy` is simply minimizing the expected surprise in `observations`, and is one way to find $q$.
 - This is equivalent to maximizing the likelihood of `observations` under $q$!

### KL Divergence and Gibbs inequality

<p></p>

| Quantity | Refers to |
| :------- | ----: |
| $H(q) = E_{x \sim q}({-\log q(x)})$ | Entropy of $q$ |
| $H(p) = E_{x \sim p}({-\log p(x)})$ | Entropy of $p$ | 
| $H(p, q) = E_{x \sim p}({-\log q(x)})$ | Cross entropy ($q$ w.r.t. $p$) |
| $D_\textrm{KL}(p | q) = E_{x \sim p}({-\log q(x)} -(-\log p(x)))$ | KL Divergence of $q$ w.r.t. $p$|
|$D_\textrm{KL}(p | q) = H(p,q) - H(p)$ | Relative entropy interpretation |
|$D_\textrm{KL}(p | q) = H(p,q) - H(p) \geq 0$ | Gibbs inequality |
: Quantities related to information  {.striped .hover}

<p></p>

 - Gibb's inequality is a consequence of Jensen's inequality (see proof below).
 - It shows that `cross-entropy` is always greater than or equal to `entropy`.
 - The `cross-entropy` is minimized when $q_X = p_X$.
 - KL divergence is zero when $q_X = p_X$.
 - $D_\textrm{KL}(p | q)$ is often presented as $E_{x \sim p}(\log \frac{p(x)}{q(x)})$ where the connection to entropy is harder to see.

::: {.callout-note appearance="simple" collapse=false icon=false}

## Gibb's inequality proof

\begin{aligned}
D_\textrm{KL}(p | q) &= E_{x \sim p}({-\log q(x)} -(-\log p(x))) \\
&= E_{x \sim p}(-\log \frac{q(x)}{p(x)}) \\
\end{aligned}
 - Jensen's inequality: For a convex function $\phi$: $E_{x~p}(\phi(x))~\geq~\phi(E_{x \sim p}(x)$.
 - $-\log$ is a convex function.
\begin{aligned}
E_{x \sim p}(-\log \frac{q(x)}{p(x)})~&\geq -\log E_{x \sim p}(\frac{q(x)}{p(x)}) \\
&\geq -\log \int p(x) \frac{q(x)}{p(x)} dx \\
&\geq -\log \int q(x) dx \\
&\geq -\log (1) \\
&\geq 0 \\
\implies D_\textrm{KL}(p | q) &\geq 0
\end{aligned}

For the discrete case, the integral is replaced by a sum.

:::

<p></p>

### A recap and some intuitions

 - We can't access $p$ directly, only `observations` from it.
 - We're proposing the `distribution` $q$ as an approximation to $p$.
 - Maximizing likelihood, minimizing cross-entropy, and minimizing KL divergence are closely related concepts to find a $q$ that approximates $p$.

<p></p>

Re. entropy:

 - The lower the entropy, the less surprising are samples.
 - A sharply peaked distribution will lead to samples from a small region.
 - Moreover, each of those samples will have high likelihood (by definition).
 - So a sharply peaked distribution has low entropy
 - Loosely, a more structured distribution $\implies$ lower entropy
 - Conversely, a more disordered distribution $\implies$ higher entropy

<p></p>
A numerical example with Gaussians:

 - We consider $p = \mathcal{N}(0, 1)$
 - $q$ is also gaussian, but with different $\mu$ and $\sigma$

<p></p>

 - In the left plot, we fix $\sigma = 1$ and vary $\mu$ for $q$.
 - Entropy is related to shape of distributions.
 - $\sigma$ determines the shape of the Gaussian.
 - Therefore, $H(q)$ remains unchanged.
 - The minimum occurs at $\mu = 0$, i.e. where $q$ matches $p$.
 - The difference between $H(p,q)$ and $D_\textrm{KL}(p | q)$ at the minimum is $H(p)$.

<p></p>

 - In the right plot, we fix $\mu = 1$ and vary $\sigma$ for $q$.
 - Entropy $H(q)$ is small when $q$ is sharply peaked (i.e. small $\sigma$).
 - As $\sigma$ inreases, $q$ spreads out and $H(q)$ becomes larger.
 - The minimum occurs at $\sigma = 1$, i.e. where $q$ matches $p$.
 - $H(p,q) - D_\textrm{KL}(p | q) = H(p)$ at the minimum.

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import torch

custom_params = {"axes.spines.right": False, "axes.spines.top": False}
sns.set_theme(style="ticks", font_scale=0.8, rc=custom_params)

mu_p, std_p = 0.0, 1.0
p = torch.distributions.Normal(loc=mu_p, scale=std_p)
x_obs = p.sample((10000,))

Hp_theory = p.entropy()
Hp_emp = -p.log_prob(x_obs).mean()

# init
Dpq = []
Hpq = []
Hq = []

q_mu = np.arange(-4, 4, 0.4)
q_sd = 1.0

for mu in q_mu:
    q = torch.distributions.Normal(loc=mu, scale=q_sd)
    Dpq.append(torch.distributions.kl_divergence(p, q))
    Hpq.append(-q.log_prob(x_obs).mean())
    Hq.append(q.entropy())

f, ax = plt.subplots(1, 2, figsize=(6, 2))
ax[0].plot(q_mu[[1, -1]], [Hp_emp, Hp_emp], '-b', label=r'$H(p)$')
ax[0].plot(q_mu, Hq, '.r', label=r'$H(q)$')
ax[0].plot(q_mu, Dpq, '-g', label=r'$D_{KL}(p,q)$')
ax[0].plot(q_mu, Hpq, '-k', alpha=0.5, label=r'$H(p,q)$')

ax[0].set(xlabel=r'$\mu$', 
          ylabel='Entropy [nats]',
          title=r'$p=\mathcal{N}(0,1), q=\mathcal{N}(\mu, 0)$')
ax[0].grid()

# init

Dpq = []
Hpq = []
Hq = []

q_sd = np.arange(0.5, 2, 0.05)
q_mu = 0.0

for sd in q_sd:
    q = torch.distributions.Normal(loc=q_mu, scale=sd)
    Dpq.append(torch.distributions.kl_divergence(p, q))
    Hpq.append(-q.log_prob(x_obs).mean())
    Hq.append(q.entropy())
ax[1].plot(q_sd[[1, -1]], [Hp_emp, Hp_emp], '-b', label=r'$H(p)$')
ax[1].plot(q_sd, Hq, '.r', label=r'$H(q)$')
ax[1].plot(q_sd, Dpq, '-g', label=r'$D_{KL}(p,q)$')
ax[1].plot(q_sd, Hpq, '-k', alpha=0.5, label=r'$H(p,q)$')


ax[1].set(xlabel=r'$\sigma$', ylabel='Entropy [nats]',
          title=r'$p=\mathcal{N}(0,1), q=\mathcal{N}(0,\sigma)$')
ax[1].grid()
ax[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.show()
```