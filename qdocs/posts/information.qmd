
---
title: "Information, entropy, and KL divergence"
author: "Rohan Gala"
date: "2023-07-13"
date-modified: "2023-08-01"
categories: ["fundamentals", "math", "machine learning"]
toc: true
number-sections: true
number-depth: 2
highlight-style: github
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
page-navigation: false
---


Information and entropy are important concepts in modern statistical learning. The origins of these closely related concepts can be traced back to physical systems. Classical thermodynamics and heat engines motivate a continuous perspective, and Shannon's information theory for data encoding and decoding motivate a discrete perspective for these concepts. 

While these physical systems can help build intuition, leaning hard on analogies can make it confusing for the problem at hand. Here I present a self-contained introduction to these concepts without reference to physical systems.

I'll assume continuous values for our random variables throughout. For the discrete case, the only change is to think of $p_X$ as a probability mass function (rather than a density function), and the rest of the description on this page holds.

| Quantity | Refers to | Description |
| :---- | :-----    | :----- |
|`random variable` | $X$  |Function that maps events to values |
|`distribution` | $q_X$| A particular probability density function |
|`density`| $q_X(x)$| Probability density at a particular value $x$, as specified by $q_X$ |
|`sample` | $x\sim q_X$ | A value for a random variable, drawn from the underlying `distribution` $q_X$|
|`observation`| $x\sim p_X$ | An observed value for a random variable. We typically don't know and cannot directly access $p_X$, but want to approximate it with $q_X$|
: Notation and definitions {#tbl-terminology tbl-colwidths="[25,15,55]"}{.striped .hover}

**Note:** 

1. `sample`, `distribution` and `density` specifically refer to quantities related to $q_X$ as shown in the table. The plain-text sample, distribution, density etc. refer to the general concepts.

2. We'll drop the subscript $X$ from the distributions and use $p = p_X$ and $q = q_X$ to simplify notation. 

### Likelihood and information

<p></p>

 - We are given `observations`, $x \sim p(x)$.
 - We assume an underlying `distribution` $q$ that can be tuned via parameters $\theta$ $^\dagger$. 
 - We interpret `density` $q(x)$ as how _likely_ an `observation` is under the `distribution`.
 - So we refer to $q(x)$ as the likelihood.
 - Following this, the quantity $-\log{q(x)}$ is called negative log-likelihood.

:::{.column-margin}
${^\dagger}$ The $\theta$ dependence can be explicitly denoted in the `distribution` as $q_{X|\theta}$, or the `density` (and therefore likelihood) as $q(x|\theta)$. Likelihood can thus be viewed as a function of $\theta$.

<br>

In the Bayesian view, $\theta$ itself may be considered as a particular value of a random variable $\Theta$, and this may be indicated in the distribution as $q_{X|\Theta}$.
:::


<p></p>

 - Further, if `density` $q(x)$ is small, we say the `observation` is _unlikely_.
 - The more unlikely the `observation`, the more _surprising_ it is.
 - The smaller the value of $q(x)$, the larger is the value of $-\log{q(x)}$.
 - Large values of negative log-likelihood thus convey surprise.
 - If we had access to $p$, we could similarly calculate surprise in `observations` under $p$.
 - The quantity $-\log{p(x)}$ is known as information.

<p></p>

 - The quantities $-\log{q(x)}$ and $-\log{p(x)}$ look identical, yet have different names.
 - $-\log{p(x)}$ tells us something about the information content of an observation.
 - $-\log{q(x)}$ conveys something about our approximation $q$ (and therefore $\theta$).
 - This is why it's helpful to retain the terminology, and interpret it as such.


:::{.column-margin}
Why use the $\log$ operation to define information?
A cryptic, partial answer: independence of distributions $\implies$ addition of information (and of log-likelihoods).
:::

### Entropy

<p></p>

 - As a summary statistic, we calculate the expected surprise over `samples`.
 - Expected surprise is simply the avg. negative log-likelihood, as above.
 - Expected surprise with `samples`: $E_{x \sim q}({-\log q(x)})$.
 - Expected surprise with `observations`: $E_{x \sim p}({-\log q(x)})$.

:::{.column-margin}
Expectation is usually an operator applied to a function of a random variable, indicated as $E[f(X)]$. The distribution over which the expectation is calculated is often implicit. 

<br>

Here we treat expectation as a function of values specified in the argument, drawn from a distribution specified in the subscript, $E_{x \sim p}(f(x))$. This is a commonly adopted convention that I'll follow here.
:::



 - Expected surprise with `samples` is called `entropy` of $q$.
 - Expected surprise with `observation` is called `cross-entropy` of $q$ w.r.t. $p$.
 - Minimizing `cross-entropy` is simply minimizing the expected surprise in `observations`, and is one way to tune $q$.
 - This is equivalent to maximizing the likelihood of `observations` under $q$!

::: {.callout-note appearance="simple" collapse=false icon=false}

## Monte Carlo estimate of Expectations

 - Monte Carlo integration enables approximation of integrals over regions as sums over samples drawn from those regions.
 - In particular, for regions that make up a valid probability distribution $p$:

\begin{aligned}
\int f(x) p(x) dx \approx \frac{1}{N} \sum_{i=1}^N f(x_i) \\
\end{aligned}

 - We can thus express expectation over a probability distribution as a sum over samples!
 - This requires that samples are drawn i.i.d.$^{\dagger\dagger}$ according to the distribution $p$.
 - The larger $N$ is, the better the approximation is.
 - We often formulate problems so that $p$ is some simple, low-dimensional distribution so that a relatively small number of samples can approximate such integrals well.
 - The subject efficient sampling from $p$ of various classes (e.g. no explicit functional form, high dimensional etc.) is a subject of active research.

 <p></p>
Example:

 - Consider cross entropy, where the integral is over the distribution $p$.
 - In this case, we don't have access to $p$ directly, only `observations`.
 - Nevertheless, we'll assume `observations` are i.i.d. according to $p$.
 - Using $i$ to index such observations $x_i \sim p$, we have an _estimator_ of the cross-entropy as a sum over `observations`:


\begin{aligned}
E_{x \sim p}({-\log q(x)}) &= \int p(x) (-\log q(x)) dx \\
&\approx \frac{1}{N} \sum_{i=1}^N f(x_i)
\end{aligned}

$^{\dagger\dagger}$ i.i.d: independent and identically distributed.
:::

### KL Divergence and Gibbs inequality

<p></p>

| Quantity | Refers to |
| :------- | ----: |
| $H(q) = E_{x \sim q}({-\log q(x)})$ | Entropy of $q$ |
| $H(p) = E_{x \sim p}({-\log p(x)})$ | Entropy of $p$ | 
| $H(p, q) = E_{x \sim p}({-\log q(x)})$ | Cross entropy ($q$ w.r.t. $p$) |
| $D_\textrm{KL}(p | q) = E_{x \sim p}({-\log q(x)} -(-\log p(x)))$ | KL Divergence of $q$ w.r.t. $p$|
|$D_\textrm{KL}(p | q) = H(p,q) - H(p)$ | Relative entropy interpretation |
|$D_\textrm{KL}(p | q) = H(p,q) - H(p) \geq 0$ | Gibbs inequality |
: Quantities related to information  {.striped .hover}

<p></p>

 - Gibb's inequality is a consequence of Jensen's inequality (see proof below).
 - It shows that `cross-entropy` is always greater than or equal to `entropy`.
 - The `cross-entropy` is minimized when $q = p$.
 - KL divergence is zero when $q = p$.
 - $D_\textrm{KL}(p | q)$ is often presented as $E_{x \sim p}(\log \frac{p(x)}{q(x)})$ where the connection to entropy is harder to see.

::: {.callout-note appearance="simple" collapse=false icon=false}

## Gibb's inequality proof

\begin{aligned}
D_\textrm{KL}(p | q) &= E_{x \sim p}({-\log q(x)} -(-\log p(x))) \\
&= E_{x \sim p}(-\log \frac{q(x)}{p(x)}) \\
\end{aligned}
 - Jensen's inequality: For a convex function $\phi$: $E_{x \sim p}(\phi(x))~\geq~\phi(E_{x \sim p}(x))$.
 - $-\log$ is a convex function.
\begin{aligned}
E_{x \sim p}(-\log \frac{q(x)}{p(x)})~&\geq -\log E_{x \sim p}(\frac{q(x)}{p(x)}) \\
&\geq -\log \int p(x) \frac{q(x)}{p(x)} dx \\
&\geq -\log \int q(x) dx \\
&\geq -\log (1) \\
&\geq 0 \\
\implies D_\textrm{KL}(p | q) &\geq 0
\end{aligned}

For the discrete case, the integral is replaced by a sum.

:::

<p></p>

### A recap and some intuitions

 - We can't access $p$ directly, only `observations` from it.
 - We're proposing the `distribution` $q$ as an approximation to $p$.
 - Maximizing likelihood, minimizing cross-entropy, and minimizing KL divergence are closely related concepts to find a $q$ that approximates $p$.

<p></p>

Re. entropy:

 - The lower the entropy, the less surprising samples are on average.
 - Sampling from a sharply peaked distribution will be confined mostly to a small region.
 - Such samples will have high likelihood (by definition).
 - So a sharply peaked distribution has low entropy.
 - In statistical mechanics, structure in a system is related to 'peaky' distributions.
 - A more structured system means the related distribution has lower entropy.
 - Conversely, a more disordered system means the related distribution has higher entropy.

<p></p>
A numerical example with Gaussians:

 - We consider $p = \mathcal{N}(0, 1)$
 - $q$ is also gaussian, but parameters $\mu$ or $\sigma$ are varied.
 - Here we don't explore algorithms to find $q$.
 - The goal is simply to compare entropy-based quantities, which serve as a basis for such algorithms.

<p></p>

 - In the left plot, we fix $\sigma = 1$ and vary $\mu$ for $q$.
 - Entropy is related to shape of distributions.
 - $\sigma$ determines the shape of the Gaussian.
 - Therefore, $H(q)$ remains unchanged.
 - The minimum occurs at $\mu = 0$, i.e. where $q$ matches $p$.
 - The difference between $H(p,q)$ and $D_\textrm{KL}(p | q)$ at the minimum is $H(p)$.

<p></p>

 - In the right plot, we fix $\mu = 1$ and vary $\sigma$ for $q$.
 - Entropy $H(q)$ is small when $q$ is sharply peaked (i.e. small $\sigma$).
 - As $\sigma$ inreases, $q$ spreads out and $H(q)$ becomes larger.
 - The minimum occurs at $\sigma = 1$, i.e. where $q$ matches $p$.
 - $H(p,q) - D_\textrm{KL}(p | q) = H(p)$ at the minimum.

```{python}
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import torch

custom_params = {"axes.spines.right": False, "axes.spines.top": False}
sns.set_theme(style="ticks", font_scale=0.8, rc=custom_params)

# p as a fixed Gaussian
mu_p, std_p = 0.0, 1.0
p = torch.distributions.Normal(loc=mu_p, scale=std_p)
x_obs = p.sample((10000,))

# H(p) calculations:
Hp_theory = p.entropy()
Hp_emp = -p.log_prob(x_obs).mean()

# q as Gaussians with varying mu
Dpq = []
Hpq = []
Hq = []
q_mu = np.arange(-4, 4, 0.4)
q_sd = 1.0
for mu in q_mu:
    q = torch.distributions.Normal(loc=mu, scale=q_sd)
    Dpq.append(torch.distributions.kl_divergence(p, q))
    Hpq.append(-q.log_prob(x_obs).mean())
    Hq.append(q.entropy())

f, ax = plt.subplots(1, 2, figsize=(6, 3))
ax[0].plot(q_mu[[1, -1]], [Hp_emp, Hp_emp], '-b', label=r'$H(p)$')
ax[0].plot(q_mu, Hq, '.r', label=r'$H(q)$')
ax[0].plot(q_mu, Dpq, '-g', label=r'$D_{KL}(p,q)$')
ax[0].plot(q_mu, Hpq, '-k', alpha=0.5, label=r'$H(p,q)$')
ax[0].set(xlabel=r'$\mu$', 
          ylabel='Entropy [nats]',
          title=r'$p=\mathcal{N}(0,1), q=\mathcal{N}(\mu, 1)$')
ax[0].grid()

# q as Gaussians with varying sigma
Dpq = []
Hpq = []
Hq = []
q_sd = np.arange(0.5, 2, 0.05)
q_mu = 0.0
for sd in q_sd:
    q = torch.distributions.Normal(loc=q_mu, scale=sd)
    Dpq.append(torch.distributions.kl_divergence(p, q))
    Hpq.append(-q.log_prob(x_obs).mean())
    Hq.append(q.entropy())
ax[1].plot(q_sd[[1, -1]], [Hp_emp, Hp_emp], '-b', label=r'$H(p)$')
ax[1].plot(q_sd, Hq, '.r', label=r'$H(q)$')
ax[1].plot(q_sd, Dpq, '-g', label=r'$D_{KL}(p,q)$')
ax[1].plot(q_sd, Hpq, '-k', alpha=0.5, label=r'$H(p,q)$')
ax[1].set(xlabel=r'$\sigma$', 
          ylabel='Entropy [nats]',
          title=r'$p=\mathcal{N}(0,1), q=\mathcal{N}(0,\sigma)$')
ax[1].grid()
ax[1].legend(loc='center left', bbox_to_anchor=(1, 0.5))
plt.tight_layout()
plt.show()
```

### Acknowledgments

[Anna Grim](https://annamgrim.com/), [Łukasz Kuśmierz](https://scholar.google.com/citations?user=U6B_HNYAAAAJ&hl=en&oi=ao), [Priyanka Kulkarni](https://scholar.google.com/citations?hl=en&user=ViilpdV5QxsC) and [Shreyas Potnis](https://scholar.google.com/citations?user=WxTzovMAAAAJ) for helpful comments and suggestions.