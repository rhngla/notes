
---
title: "Information, entropy, and KL divergence"
author: "Rohan Gala"
date: "2023-07-13"
date-modified: last-modified
categories: ["fundamentals", "math", "machine learning"]
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
page-navigation: false
---


Information and entropy are important concepts in modern statistical learning. The origins of these closely related concepts can be traced back to physical systems. Classical thermodynamics and heat engines motivate a continuous perspective, and Shannon's information theory for data encoding and decoding motivate a discrete perspective for these concepts. 

While these physical systems can help build intuition, leaning hard on analogies can make it confusing for the problem at hand. Here I present a self-contained introduction to these concepts without reference to physical systems.

I'll assume continuous values for our random variables throughout. For the discrete case, the only change is to think of $p_X$ as a probability mass function (rather than a density function), and the rest of the description on this page holds.

| Quantity | Refers to | Description |
| :---- | :-----    | :----- |
|`random variable` | $X$  |Function that maps events to values |
|`distribution` | $q_X$| A particular probability density function |
|`density`| $q_X(x)$| Probability density of at a particular value $x$, as specified by $q_X$ |
|`sample` | $x\sim q_X$ | A value for a random variable, drawn from the underlying `distribution` $q_X$|
|`observation`| $x\sim p_X$ | An observed value for a random variable. We typically don't know and cannot directly access $p_X$, but want to approximate it with $q_X$|
: Notation and definitions {#tbl-terminology tbl-colwidths="[25,15,55]"}{.striped .hover}

**Note:** 

1. I'll treat `sample`, `distribution` and `density` specifically to referring quantities related to $q_X$ as shown in the table. The plain-text sample, distribution, density etc. will refer to the general concepts.

2. We'll also drop the subscript $X$ from the distributions and use $p = p_X$ and $q = q_X$ since there are no further dependencies to consider (e.g. no conditional distributions).

### Likelihood and information

<p></p>

 - We are given `observations`, $x \sim p(x)$.
 - We assume an underlying `distribution` $q$. 
 - We interpret `density` $q(x)$ as how _likely_ an `observation` is under the `distribution`.
 - So we refer to $q(x)$ as the likelihood.
 - Following this, the expression $\log q(x)$ is called log-likelihood.

<p></p>

 - Further, if `density` $q(x)$ is small, we say the `observation` is _unlikely_.
 - The more unlikely the `observation`, the more _surprising_ it is.
 - The smaller the value of $q(x)$, the larger is the value of $-\log q(x)$.
 - Large values of negative log-likelihood thus convey surprise.
 - Formally, the negative log-likelihood is called _information_.
 - Notion of surprise requires context of an underlying distribution!


:::{.column-margin}
Why use the $\log$ operation to define information?
A cryptic, partial answer: independence of distributions $\implies$ addition of information.
:::
<p></p>

 - Let's play a similar game with `samples`, $x \sim q$
 - Many `samples` have larger values of $q(x)$
 - Some `samples` will have small values of $q(x)$
 - So analogous to `observations`, some `samples` can be viewed as _surprising_.

### Entropy

<p></p>

 - As a summary statistic, we calculate the expected surprise over `samples`
 - Expected surprise is simply the avg. negative log-likelihood, as above.
 - Expected surprise with `samples`: $E_{x \sim q}({-\log q(x)})$
 - Expected surprise with `observations`: $E_{x \sim p}({-\log q(x)})$

:::{.column-margin}
Expectation is usually an operator applied to a function of a random variable, indicated as $E[f(X)]$. The distribution over which the expectation is calculated is often implicit. 

<br>

Here we treat expectation as a function of values specified in the argument, drawn from a distribution specified in the subscript, $E_{x \sim p}(f(x))$. This is a commonly adopted convention that I'll follow here.
:::



 - Expected surprise with `samples` is called `entropy` of $q$
 - Expected surprise with `observation` is called `cross-entropy` of $q$ w.r.t. $p$
 - Minimizing `cross-entropy` is simply minimizing the expected surprise in `observations`, and is one way to find $q$.
 - This is equivalent to maximizing the likelihood of `observations` under $q$!

### KL Divergence and Gibbs inequality

<p></p>

| Quantity | Refers to |
| :------- | ----: |
| $H(q) = E_{x \sim q}({-\log q(x)})$ | Entropy of $q$ |
| $H(p) = E_{x \sim p}({-\log p(x)})$ | Entropy of $p$ | 
| $H(p, q) = E_{x \sim p}({-\log q(x)})$ | Cross entropy ($q$ w.r.t. $p$) |
| $D_\textrm{KL}(p | q) = E_{x \sim p}({-\log q(x)} -(-\log p(x)))$ | KL Divergence of $q$ w.r.t. $p$|
|$D_\textrm{KL}(p | q) = H(p,q) - H(p)$ | Relative entropy interpretation |
|$D_\textrm{KL}(p | q) = H(p,q) - H(p) \geq 0$ | Gibbs inequality |
: Quantities related to information  {.striped .hover}

<p></p>

 - Gibb's inequality is a consequence of Jensen's inequality (see proof below).
 - It shows that `cross-entropy` is always greater than or equal to `entropy`.
 - The `cross-entropy` is minimized when $q_X = p_X$.
 - KL divergence is zero when $q_X = p_X$.
 - $D_\textrm{KL}(p | q)$ is often presented as $E_{x \sim p}(\log \frac{p(x)}{q(x)})$ where the connection to entropy is harder to see.

::: {.callout-note appearance="simple" collapse=false icon=false}

## Gibb's inequality proof

\begin{aligned}
D_\textrm{KL}(p | q) &= E_{x \sim p}({-\log q(x)} -(-\log p(x))) \\
&= E_{x \sim p}(-\log \frac{q(x)}{p(x)}) \\
\end{aligned}
 - Jensen's inequality: For a convex function $\phi$: $E_{x~p}(\phi(x))~\geq~\phi(E_{x \sim p}(x)$.
 - $-\log$ is a convex function.
\begin{aligned}
E_{x \sim p}(-\log \frac{q(x)}{p(x)})~&\geq -\log E_{x \sim p}(\frac{q(x)}{p(x)}) \\
&\geq -\log \int p(x) \frac{q(x)}{p(x)} dx \\
&\geq -\log \int q(x) dx \\
&\geq -\log (1) \\
&\geq 0 \\
\implies D_\textrm{KL}(p | q) &\geq 0
\end{aligned}

For the discrete case, the integral is replaced by a sum.

:::

<p></p>

### A recap and some intuitions

 - We can't access $p$ directly, only `observations` from it.
 - We're proposing the `distribution` $q$ as an approximation to $p$.
 - Maximizing likelihood, minimizing cross-entropy, and minimizing KL divergence are closely related concepts to find a $q$ that approximates $p$.

<p></p>

Re. entropy:

 - The lower the entropy, the less surprising are samples.
 - A sharply peaked distribution will lead to samples from a small region.
 - Moreover, each of those samples will have high likelihood (by definition).
 - So a sharply peaked distribution has low entropy
 - Loosely, a more structured distribution $\implies$ lower entropy
 - Conversely, a more disordered distribution $\implies$ higher entropy
