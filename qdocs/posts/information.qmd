
---
title: "Information, entropy, and KL divergence"
author: "Rohan Gala"
date: "2023-07-13"
date-modified: last-modified
categories: ["fundamentals", "math", "machine learning"]
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
---


Information and entropy are important concepts in modern statistical learning. The origins of these closely related concepts can be traced back to physical systems. Classical thermodynamics and heat engines motivate a continuous perspective, and Shannon's information theory for data encoding and decoding motivate a discrete perspective for these concepts. 

While these physical systems can help build intuition, leaning hard on analogies can make it confusing for the problem at hand. Here I present a self-contained introduction to these concepts without reference to physical systems.

I'll assume continuous values for our random variables throughout. For the discrete case, the only change is to think of $p_X$ as a probability mass function (rather than a density function), and the rest of the description on this page holds.

| Quantity | Refers to | Description |
| :---- | :-----    | :----- |
|`random variable` | $X$  |Function that maps events to values |
|`distribution` | $q_X$| A particular probability density function |
|`density`| $q_X(x)$| Probability density of at a particular value $x$, as specified by $q_X$ |
|`sample` | $x\sim q_X$ | A value for a random variable, drawn from the underlying `distribution` $q_X$|
|`observation`| $x\sim p_X$ | An observed value for a random variable. We typically don't know and cannot directly access $p_X$, but want to approximate it with $q_X$|
: Notation and definitions {#tbl-terminology tbl-colwidths="[25,15,55]"}{.striped .hover}

**Note:** I'll treat `sample`, `distribution` and `density` specifically to referring quantities related to $q_X$ as shown in the table. The plain-text sample, distribution, density etc. will refer to the general concepts.

####

 - We are given `observations`, $x \sim p_X(x)$.
 - We assume an underlying `distribution` $q_X$. 
 - We interpret `density` $q_X(x)$ as how _likely_ an `observation` is under the `distribution`.
 - So we refer to $q_X(x)$ as the likelihood.
 - Following this, the expression $\log q_X(x)$ is called log-likelihood.

####

 - Further, if `density` $q_X(x)$ is small, we say the `observation` is _unlikely_.
 - The more unlikely the `observation`, the more _surprising_ it is.
 - The smaller the value of $q_X(x)$, the larger is the value of $-\log q_X(x)$.
 - Large values of negative log-likelihood thus convey surprise.
 - Formally, the negative log-likelihood is called _information_.
 - Notion of surprise requires context of an underlying `distribution`!


:::{.column-margin}
Why use the $\log$ operation to define information?
A cryptic, partial answer: independence of distributions $\implies$ addition of information, and this serves as partial motivation.
:::

####

 - Let's play a similar game with `samples`, $x \sim q_X$
 - Many `samples` have larger values of $q_X(x)$
 - Some `samples` will have small values of $q_X(x)$
 - So analogous to `observations`, some `samples` can be viewed  as _surprising_.

####
 - As a baseline summary statistic, we calculate the expected surprise over `samples`
 - Expected surprise is simply the avg. negative log-likelihood, as above.
 - Expected surprise with `samples`: $E_{x \sim q_X}({-\log q_X(x)})$
 - Expected surprise with `observations`: $E_{x \sim p_X}({-\log q_X(x)})$

:::{.column-margin}
Expectation is usually an operator applied to random variable, but here we treat it as a function of values taken by the random variable. This is a commonly adopted convention that I'll follow here.
:::

#### 

 - Expected surprise with `samples` is called `entropy` of $q_X$
 - Expected surprise with `observation` is called `cross-entropy` of $q_X$ w.r.t. $p_X$
 - Minimizing `cross-entropy` is simply minimizing the expected surprise in `observations`, and is one way to find $q_X$.
 - This is equivalent to maximizing the likelihood of `observations` under $q_X$!

####

| Quantity | Refers to |
| :------- | ----: |
| $H(q) = E_{x \sim q}({-\log q(x)})$ | Entropy of $q$ |
| $H(p) = E_{x \sim p}({-\log p(x)})$ | Entropy of $p$ | 
| $H(p, q) = E_{x \sim p}({-\log q(x)})$ | Cross entropy ($q$ w.r.t. $p$) |
| $D_\textrm{KL}(p | q) = E_{x \sim p}({-\log q(x)} -(-\log p(x)))$ | KL Divergence of $q$ w.r.t. $p$|
|$D_\textrm{KL}(p | q) = H(p,q) - H(p)$ | Relative entropy interpretation |
|$D_\textrm{KL}(p | q) = H(p,q) - H(p) \geq 0$ | Gibbs inequality |
: Quantities related to information  {.striped .hover}

 - Gibb's inequality is a consequence of Jensen's inequality
 - It shows that `cross-entropy` is always greater than or equal to `entropy`
 - The `cross-entropy` is minimized when $q_X = p_X$

:::{.column-margin}
**Jensen's inequality** <br> 
For convex $\phi$: $E[\phi(X)]~\geq~\phi(E[X])$
:::

####

 - A recap:
 - We can't access $p_X$ directly, only `observations` from it.
 - We're proposing the `distribution` $q_X$ as an approximation to $p_X$.
 - Maximum likelihood, minimum cross-entropy, and minimum KL divergence are closely related concepts for finding $q_X$.

####

 - Some intuitions for entropy:
 - The lower the entropy, the less surprising are samples.
 - A sharply peaked distribution will lead to samples from a small region.
 - Moreover, each of those samples will have high likelihood (by definition).
 - So a sharply peaked distribution has low entropy
 - Loosely, a more structured distribution $\implies$ lower entropy
 - Conversely, a more disordered distribution $\implies$ higher entropy
