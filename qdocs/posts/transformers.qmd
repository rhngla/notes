---
title: "Transformers and foundational models for X"
author: "Rohan Gala"
date: "2023-06-20"
categories: ["machine learning"]
#image: "image.jpg"
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
---

Here I keep track of sources that have helped me understand this topic better.

In my view, the main concepts to be familiar with are:

 - [Generative modeling for sequences](#generative-sequence-modeling)
 - [Attention mechanism, and transformer architecture](#attention-mechanism-and-transformer-architecture)
 - [Multi-task learning](#multi-task-learning)

@raschka2023llm outlines some key papers and developments in the context of large language models, and @weng2018attention provides an overview of the attention mechanism, and some historical notes for the peculiar keys, values, queries nomenclature used in the transformer. 

### Generative sequence modeling

Language has a natural sequential structure. The generative modeling task is to learn a probability distribution over sequences of words. 

Let different words be specific values of $n$ random variables. Then the distribution we'd like to learn from data is $p(s_1, s_2, ..., s_n)$. The sequential ordering motivates a particular factorization of this joint distribution:

$$p(s_1, s_2, ..., s_n) = p(s_1) p(s_2 | s_1) p(s_3 | s_1, s_2) ... p(s_n | s_1, s_2, ..., s_{n-1})$$

It's important to understand that the notion of 'words' is arbitrary for such modeling. Even for language, once can break up words at the character or even byte level (e.g. see [byte pair encoding](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt)). This is a crucial think to keep in mind for the notions of multi-task learning and foundational models. 

When generating new sequences, we are recursively sampling particular conditional distributions, e.g. $p(s_i | s_1, ... s_{i-1})$ to ultimately look like we are sampling from $p(s_i, ... s_n | s_{i-k}, ..., s_{i-1})$.

This looks like $p(\textrm{outputs}|\textrm{inputs})$. We'll refer to this in the section on multi-task learning. 

@phuong2022formal describe transformer-based-algorithms without implementation details, instead focusing on descriptions with consistent and formal notation. With the maze of transformer variants that exist, this resource offers a coherent picture of the landscape.

### Attention mechanism and transformer architecture

@smola2019attn motivates the attention mechanism with kernel regression, and @schlag2021linear link attention to fast weight programming.

@schlag2021linear explore the linear version of attention in the context of associative memory and explore memory capacity of such models with different choices of kernels, activation functions, etc., connecting to a rich literature on such investigations dating all the way back to @cover1965geometrical.

A self-attention layer maps an input sequence of length $T$, denoted as $(x_1,...,x_T), x_i \in \mathbb{R}^{d_\textrm{in} \times 1}$ to an output sequence $(y_1,...,y_T), y_i \in \mathbb{R}^{d_\textrm{out} \times 1}$ as

$$
\begin{align}
k_i, v_i, q_i &= Kx_i,Vx_i,Qx_i \\
A_{ij} &= \textrm{softmax}(k_i^\top q_j) \\
y_{i} &= \sum_j{A_{ij}v_j}
\end{align}
$$

$K, Q \in \mathbb{R}^{T \times d_\textrm{kq}}$ and $V \in \mathbb{R}^{T \times d_\textrm{out}}$, are trainable weights. $\textrm{softmax}$ ensures $\sum_j{A_{ij}=1} \forall i$,  and $A \in \mathbb{R}_{\geq 0}^{T \times T}$ is the attention matrix. 

Setting certain entries of the $A_{ij}$ to zero is referred to as masking, and it has the effect of disallowing particular pairwise-interactions between the input sequence elements in contributing to a particular output.

For language, setting the upper triangular entries (including the diagonal) to be zero is a common choice. This mask pattern ensures that $y_i$ is only based on $(x_1, ..., x_{i-1})$.

In the absence of masking, each output takes on the form $y_j  = \sum_i{g_j(x_i)}$, for some function $g_j$. This form renders it invariant to ordering i.e. permutations of the input sequence. 

On a related note, @zaheer2017deep had shown that for universal function approximators (e.g. neural networks) $h$ and $g$, a function $f$ of the form $f(x_1, ... x_n) = h(\sum_i(g(x_i)))$ is a universal approximator for any permutation invariant function.

I've ignored scaling and nonlinearities for simplicity in the description above. Aside from the attention mechanism, the transformer architecture includes residual connections, layer normalization, and scaling in the calculation of the attention matrix. These conceptually straightforward operations turn out to be crucial in practice to prevent gradients from exploding or vanishing.

At this point, I'd highly recommend the implementation walkthrough by Karpathy [-@karpathy2023llm]. He's a skilled teacher and watching him live-code is instructive.

### Multi-task learning

In one version (e.g. GPT and variants), these transformer models are trained to simply predict the next word. That is, the model learns distributions $p(s_i | s_1, ..., s_{i-1})$ for all $i \in (1,...,n)$.

So far, a clear reason for why large language models seem magical is missing. Why bother increasing size of models, why invest so much in collecting the data and compute resources?

I think @radford2019language, and later @brown2020language provide this missing reason. The main realization is that for language, the task itself is a sequence of tokens. Tasks can take the form of "translate to german" or "write python code for" etc. Here the sequence of words that specify the task are themselves part of the input sequence.

Moreover, the underlying input does not have to be "words", but instead can be more parcellated and abstract [byte pair encodings](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt).

The implication is that while we learn $p(output|input)$, we are also learning $p(output | input, task)$, without needing task-specific architecture or training!

In my view, this is a central requirement for a meaningful notion of foundational models. The implicit multi-task learning that takes place is the reason why non-trivial zero shot performance is possible on a variety of tasks, even though the task is not explicitly specified while training.

### Foundation models

The term "foundational models" was popularized by @bommasani2021opportunities, and an accompanying [workshop at Stanford University](https://crfm.stanford.edu/workshop.html). 

@bommasani2021opportunities describe the notion of emergence (generalization over tasks, and therefore related to zero-shot performance) and homogenization (learning something useful for a variety of downstream tasks, and therefore the pre-training is amenable to fine-tuning).

In my view this is essentially echoing the central observations of @radford2019language and @brown2020language.

With the hype about large language models gripping other domains, it is worth asking if the these ideas are applicable models in other domains that do not have the special place of language.

### Transformers for single-cell resolution datasets

The most developed, least noisy single-cell resolution _-omics_ datasets are from transcriptomics. For most analyses, the dataset is a table rows are cells,  columns are genes, and entries are the gene expression values. Additional metadata, e.g. disease state of the donor from which the cell was sampled, perturbations applied to the cell etc. may be part of the dataset.

Typical tasks include determining identifiable cell types, specifying differentially expressed genes across conditions (aging, disease, perturbations), inferring developmental trajectories of particular cell types, identifying gene regulatory networks etc.

The issue that plagues so many of these analyses are related modeling of noise in such measurements, curse of high-dimensionality, absence of established ground-truth and benchmarks, and ultimately mathematical definitions of tasks.

The team at [OpenProblems](https://openproblems.bio/events/) have continued to push for standardization and benchmarking, but the output (both in terms of benchmarking datasets and state-of-the-art models) seems to have limited influence on academic research on the these topics.

Transcriptomic data contain neither a natural sequential ordering, nor a large enough corpus of tasks to draw parallels with emergence and homogenization described above. 

Recent papers have applied transformer architectures to single cell genomics:

 - @yang2022scbert
 - @cui2022scformer, rebranded as @cui2023scgpt
 - @shen2023generative
 - @theodoris2023
 - @hao2023foundation

 (to be continued)

### References

::: {#refs}
:::