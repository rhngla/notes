---
title: "Transformers and foundational models for X"
author: "Rohan Gala"
date: "2023-06-20"
categories: ["machine learning"]
#image: "image.jpg"
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
---

Here I keep track of sources that have helped me understand this topic better.

In my view, the main concepts to be familiar with are:

 - [Generative modeling for sequences](#generative-sequence-modeling)
 - [Attention mechanism, and transformer architecture](#attention-mechanism-and-transformer-architecture)
 - [Multi-task learning](#multi-task-learning)

::: {.column-margin}
@raschka2023llm outlines some key papers and developments in the context of large language models, and @weng2018attention provides an overview of the attention mechanism, and some historical notes for the peculiar keys, values, queries nomenclature used in @vaswani2017attention. 
:::

### Generative sequence modeling

Language has a natural sequential structure. The generative modeling task is to learn a probability distribution over sequences of words. 

Let different words be specific values of $n$ random variables. Then the distribution we'd like to learn from data is $p(s_1, s_2, ..., s_n)$. The sequential ordering motivates a particular factorization of this joint distribution:

$$p(s_1, s_2, ..., s_n) = p(s_1) p(s_2 | s_1) p(s_3 | s_1, s_2) ... p(s_n | s_1, s_2, ..., s_{n-1})$$

It's important to understand that the notion of 'words' is arbitrary for such modeling. Even for language, one can break up words at the character or even byte level. This is a crucial think to keep in mind for the notions of multi-task learning and foundational models. 

When generating new sequences, we are recursively sampling particular conditional distributions, e.g. $p(s_i | s_1, ... s_{i-1})$ to ultimately look like we are sampling from $p(s_i, ... s_n | s_{i-k}, ..., s_{i-1})$.

This looks like $p(\textrm{outputs}|\textrm{inputs})$. We'll refer to this in the section on multi-task learning. 

::: {.column-margin}
@phuong2022formal describe transformer-based-algorithms without implementation details, instead focusing on descriptions with consistent and formal notation. With the maze of transformer variants that exist, this resource offers a coherent picture of the landscape.
:::

### Attention mechanism and transformer architecture

::: {.column-margin}
@smola2019attn motivates the attention mechanism with kernel regression
:::

::: {.column-margin}
@schlag2021linear link attention to fast weight programming. They also explore the linear version of attention in the context of associative memory and explore memory capacity of such models with different choices of kernels, activation functions, etc., connecting to a rich literature on such investigations dating all the way back to @cover1965geometrical.
:::

I mainly want to highlight how attention captures dependence of the output on particular entries of the input sequence. This bare-bones version of self-attention includes no bias terms, no scaling of the dot products etc., that are part of standard implementations and described well in @phuong2022formal.

Let length of the sequence $T$. We denote the $i$-th input as $x_i \in \mathbb{R}^{d_{\textrm{in}}}$. Similarly the $j$-th output $y_j \in \mathbb{R}^{d_{\textrm{out}}}$. $K, Q \in \mathbb{R}^{d_\textrm{key-query} \times d_{\textrm{in}}}$ and $V \in \mathbb{R}^{d_\textrm{out} \times d_{\textrm{in}}}$, are trainable weight matrices. 

$$
\begin{aligned}
k_i, q_i, v_i &= Kx_i, Qx_i, Vx_i &\quad k,q \in \mathbb{R}^{d_{\textrm{key-query}}}, v \in \mathbb{R}^{d_{out}} \\
A_{ij} &= k_i^\top q_j &\quad A \in \mathbb{R}^{T \times T}\\
B_{ij} &= {\exp (A_{ij})} / {\sum_k \exp (A_{ik})} \\
y_i &= \sum_j{B_{ij}v_j} 
\end{aligned}
$$

$A$ is known as the attention matrix. Setting certain entries of the $A_{ij}$ to zero is referred to as masking, and it has the effect of preventing interaction between the $i$-th and $j$-th inputs in constructing the $i$-th output.

For language, setting the upper triangular entries (excluding the diagonal) to zero is a common choice. This mask pattern ensures that $y_i$ is only based on previous and current inputs of the sequence, i.e. $(x_1, ..., x_{i})$ (causality for temporal sequences). Since the product of lower triangular matrices remains lower triangular, stacking attention layers with this mask pattern retains such dependence throughout - which can be useful depending on the use case.  

I expect that choice of particular masking patterns and sparsity of the attention matrix (and of $K$, $Q$, $V$ matrices) are important questions for specific applications. 

In the absence of masking, each output takes on the form $y_j  = \sum_i{g_j(x_i)}$, for some function $g_j$. This form renders it invariant to ordering i.e. permutations of the input sequence. 

::: {.column-margin}
On a related note, @zaheer2017deep had shown that for universal function approximators (e.g. neural networks) $h$ and $g$, a function $f$ of the form $$f(x_1, ... x_n) = h(\sum_i(g(x_i)))$$ is a universal approximator for any permutation invariant function.
:::

Other than self-attention, the decoder-only transformer architecture includes residual connections, layer normalization. These conceptually straightforward operations turn out to be crucial in practice to prevent gradients from exploding or vanishing. 

The transformer in @vaswani2017attention also has an encoding arm that is used for cross-attention, without masking. This makes intuitive sense for tasks like translation where the sequence of words in english may not be relevant for sequence of words in a german translation. In general, cross-attention is used to provide context from a different source - and I suspect this is key for multimodal applications.

::: {.column-margin}
I highly recommend the GPT implementation walkthrough by Karpathy [-@karpathy2023llm]. He's a skilled teacher and watching him live-code is instructive.
:::

### Multi-task learning

In one version (e.g. GPT and variants), these transformer models are trained to simply predict the next word. That is, the model learns distributions $p(s_i | s_1, ..., s_{i-1})$ for all $i \in (1,...,n)$.

So far, a clear reason for why large language models seem magical is missing. Why bother increasing size of models, why invest so much in collecting the data and compute resources?

I think @radford2019language, and later @brown2020language provide this missing reason. The main realization is that for language, the task itself is a sequence of tokens. Tasks can take the form of "translate to german" or "write python code for" etc. Here the sequence of words that specify the task are themselves part of the input sequence.

Moreover, the underlying input does not have to be "words", but instead can be more parcellated and abstract [byte pair encodings](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt).

The implication is that while we learn $p(output|input)$, we are also learning $p(output | input, task)$, without needing task-specific architecture or training!

In my view, this is a central requirement for a meaningful notion of foundational models. The implicit multi-task learning that takes place is the reason why non-trivial zero shot performance is possible on a variety of tasks, even though the task is not explicitly specified while training.

### Foundation models

The term "foundational models" was popularized by @bommasani2021opportunities, and an accompanying [workshop at Stanford University](https://crfm.stanford.edu/workshop.html). 

@bommasani2021opportunities describe the notion of emergence (generalization over tasks, and therefore related to zero-shot performance) and homogenization (learning something useful for a variety of downstream tasks, and therefore the pre-training is amenable to fine-tuning).

In my view this is essentially echoing the central observations of @radford2019language and @brown2020language.

With the hype about large language models gripping other domains, it is worth asking if the these ideas are applicable models in other domains that do not have the special place of language.

### Transformers for single-cell resolution datasets

The most developed, least noisy single-cell resolution _-omics_ datasets are from transcriptomics. For most analyses, the dataset is a table rows are cells,  columns are genes, and entries are the gene expression values. Additional metadata, e.g. disease state of the donor from which the cell was sampled, perturbations applied to the cell etc. may be part of the dataset.

Typical tasks include determining identifiable cell types, specifying differentially expressed genes across conditions (aging, disease, perturbations), inferring developmental trajectories of particular cell types, identifying gene regulatory networks etc.

The issue that plagues so many of these analyses are related modeling of noise in such measurements, curse of high-dimensionality, absence of established ground-truth, and often lack of a mathematically precise definition for tasks.

::: {.column-margin}
The team at [OpenProblems](https://openproblems.bio/events/) have continued to push for standardization and benchmarking through formal descriptions of tasks, e.g. @luecken2021sandbox. Still, the output of such efforts (both in terms of benchmarking datasets and state-of-the-art models) seems to have had limited influence on subsequent academic research.
:::

Transcriptomic data contain neither a natural sequential ordering, nor a large enough corpus of tasks to draw parallels with emergence and homogenization described above.

Recent studies applying transformer-based models to single cell genomics have generated quite some buzz:

 - @yang2022scbert
 - @cui2022scformer, rebranded as @cui2023scgpt
 - @shen2023generative
 - @theodoris2023
 - @hao2023foundation

 (to be continued)

### References

::: {#refs}
:::