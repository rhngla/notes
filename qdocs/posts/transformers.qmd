---
title: "Transformers"
author: "Rohan Gala"
date: "2023-06-20"
date-modified: "2024-01-10"
categories: ["machine learning"]
#image: "image.jpg"
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
---

Here I keep track of sources that have helped me understand this topic better. I think the main concepts to be familiar with are:

 - [Generative modeling for sequences](#generative-sequence-modeling)
 - [Attention mechanism, and transformer architecture](#attention-mechanism-and-transformer-architecture)
 - [Multi-task learning](#multi-task-learning)

::: {.column-margin}
@raschka2023llm outlines some key papers and developments in the context of large language models.
:::

::: {.column-margin}
@weng2018attention provides an overview of the attention mechanism, and some historical notes for the peculiar keys, values, queries nomenclature used in @vaswani2017attention.
:::

::: {.column-margin}
@bloem2019transformers provides a more pedagogical view of the transformer architecture, and has nice visualizations as well.
:::

### Generative sequence modeling

Language has a natural sequential structure. The generative modeling task is to learn a probability distribution over word sequences. 

Let $s_i$ denote the random variable for the $i$-th word in the sequence. Each random variable can take on values from the entire vocabulary. Then the distribution (a.k.a. generative model) we'd like to learn from data is $p(s_1, s_2, ..., s_n)$. The sequential ordering motivates a particular factorization of this joint distribution:

$$p(s_1, s_2, ..., s_n) = p(s_1) p(s_2 | s_1) p(s_3 | s_1, s_2) ... p(s_n | s_1, s_2, ..., s_{n-1})$$

Note that for language modeling, the vocabulary typically consists of character substrings, or even related byte level representations. As the vocabulary becomes more fine-grained, the distributions that must be learnt become more expressive. The sequence length to cover the same semantic meaning becomes larger, and the amount of data required to learn the distributions increases.

When generating new sequences, one can sample successively from the conditional distributions, e.g. $p(s_{i+1} | s_1, ... s_{i})$ to construct a length $k$ sequence from given a length $i$ sequence. 

Given a sequence of length $i$, constructing a length $k$ sequence can be viewed as sampling from $p(s_{i+1}, ... s_{i+k} | s_{1}, ..., s_{i})$. This looks like $p(\textrm{outputs}|\textrm{inputs})$. We'll revisit this view in the section on multi-task learning. 

::: {.column-margin}
@phuong2022formal describe transformer-based-algorithms without implementation details, instead focusing on descriptions with consistent and formal notation. With the maze of transformer variants that exist, this resource offers a coherent picture of the landscape.
:::

### Attention mechanism and transformer architecture

Here, I'll highlight how attention captures dependence of the output on particular entries of the input sequence. This bare-bones version of self-attention includes no bias terms, no scaling of the dot products etc., that are part of standard implementations and described well in @phuong2022formal.

::: {.column-margin}
@smola2019attn motivates the attention mechanism with a kernel regression view.
:::

::: {.column-margin}
@schlag2021linear link attention to fast weight programming. They also explore the linear version of attention in the context of associative memory and explore memory capacity of such models with different choices of kernels, activation functions, etc., connecting to a rich literature on such investigations dating all the way back to @cover1965geometrical.
:::

Let length of the sequence $T$. We denote the $i$-th input as ${x_i \in \mathbb{R}^{d_{\textrm{in}}}}$. Similarly the $j$-th output ${y_j \in \mathbb{R}^{d_{\textrm{out}}}}$. Matrices ${K, Q \in \mathbb{R}^{d_\textrm{key-query} \times d_{\textrm{in}}}}$ and ${V \in \mathbb{R}^{d_\textrm{out} \times d_{\textrm{in}}}}$ are trainable weights. 

$$
\begin{aligned}
k_i, q_i, v_i &= Kx_i, Qx_i, Vx_i &\quad k,q \in \mathbb{R}^{d_{\textrm{key-query}}}, v \in \mathbb{R}^{d_{out}} \\
A_{ij} &= k_i^\top q_j &\quad A \in \mathbb{R}^{T \times T}\\
B_{ij} &= {\exp (A_{ij})} / {\sum_k \exp (A_{ik})} \\
y_i &= \sum_j{B_{ij}v_j} 
\end{aligned}
$$

$A$ is known as the attention matrix. Setting certain entries of the $A_{ij}$ to zero is referred to as masking, and it has the effect of preventing multiplicative interaction between the $i$-th and $j$-th inputs in constructing the $i$-th output.

For language, setting the upper triangular entries (excluding the diagonal) to zero is a common choice. This mask pattern ensures that $y_i$ is only based on previous and current inputs of the sequence, i.e. $(x_1, ..., x_{i})$ (causality for temporal sequences). Since the product of lower triangular matrices remains lower triangular, stacking attention layers with this mask pattern retains such dependence throughout - which can be useful depending on the use case.

Choice of particular masking patterns and sparsity of the attention matrix (and of $K$, $Q$, $V$ matrices) are important questions for specific applications. 

In the absence of masking, each output takes on the form $y_j  = \sum_i{g_j(x_i)}$, for some function $g_j$. This form renders it invariant to ordering i.e. permutations of the input sequence. 

::: {.column-margin}
@zaheer2017deep have shown that for universal function approximators (e.g. neural networks) $h$ and $g$, a function $f$ of the form $$f(x_1, ... x_n) = h(\sum_i{g(x_i)})$$ is a universal approximator for any permutation invariant function.
:::

Other than self-attention, the decoder-only transformer architecture includes residual connections, layer normalization, weight sharing across embedding laters etc. These conceptually straightforward operations turn out to be crucial in practice to prevent gradients from exploding or vanishing (see e.g. @zhang2022set), and to reduce memory footprint of large models.

The transformer in @vaswani2017attention also has an encoding arm that is used for cross-attention, without masking. This makes intuitive sense for tasks like translation where the sequence of words in english may not be relevant for sequence of words in a german translation. In general, cross-attention is used to utilize context from a different source.

See @weng2023transformer for a more comprehensive review of innovations in transformer-like models.

::: {.column-margin}
I highly recommend the GPT implementation walkthrough by Karpathy [-@karpathy2023llm]. He's a skilled teacher and watching him live-code is instructive.
:::

### Multi-task learning

In one class of transformers (e.g. GPT and variants), the model is trained to simply predict the next word. That is, the model learns distributions $p(s_i | s_1, ..., s_{i-1})$ for all $i \in (1,...,n)$.

So far, a clear reason for why large language models seem magical is missing. Why bother increasing size of models, why invest so much in collecting the data and compute resources?

I think @radford2019language, and later @brown2020language provide the motivation. The main realization is that for language, the task itself is a sequence of tokens. Tasks can take the form of "translate to german" or "write python code for" etc. Here the sequence of words that specify the task are themselves part of the input sequence. Moreover, the underlying input can be more parcellated and abstract, e.g. [byte pair encodings](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt).

The implication is that while we learn $p(\textrm{output}|\textrm{input})$, we are also learning ${p(\textrm{output} | \textrm{input}, \textrm{task})}$, without requiring task-specific architecture or training.

In my view, this is a central requirement for a meaningful notion of foundational models. The implicit multi-task learning that takes place is the reason why non-trivial zero shot performance may be possible on a variety of tasks, even though the task is not explicitly specified while training.

### Foundational models

The term "foundational models" was popularized by @bommasani2021opportunities, and an accompanying [workshop at Stanford University](https://crfm.stanford.edu/workshop.html). 

@bommasani2021opportunities describe the notion of emergence (generalization over tasks, and therefore related to zero-shot performance) and homogenization (learning something useful for a variety of downstream tasks, and therefore the pre-training is amenable to fine-tuning). These essentially echo the central observations of @radford2019language and @brown2020language.

The idea of fine-tuning and using pre-trained model components on related datasets or tasks (e.g. transfer learning) has been a motivation behind efforts to curate such models in various domains, even before large language models became common e.g. [https://pytorch.org/hub/](https://pytorch.org/hub/). Unlike the multi-task learning in large language models, the scope of tasks for which pre-trained models might be useful is much more constrained.

### Parting notes

The output of language models can be directly parsed and evaluated for quality by humans. This fact is exploited for public facing language models such as chatGPT. In this talk titled the _State of GPT_ [-@karpathy2023stategpt], Karpathy describes finetuning, reward modeling, and reinforcement learning using the reward model; all of these hinge on the fact that humans can understand and evaluate outputs of such language models.

Scientific applications will certainly find use for the attention mechanism and the transformer architecture. However, claims in papers from scientific domains that draw analogies with language modeling and co-opt jargon around foundational models deserve a more critical look.

### References

::: {#refs}
:::