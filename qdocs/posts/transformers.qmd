---
title: "Transformers and foundational models for X"
author: "Rohan Gala"
date: "2023-06-20"
date-modified: last-modified
categories: ["machine learning"]
#image: "image.jpg"
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
---

Here I keep track of sources that have helped me understand this topic better.

In my view, the main concepts to be familiar with are:

 - [Generative modeling for sequences](#generative-sequence-modeling)
 - [Attention mechanism, and transformer architecture](#attention-mechanism-and-transformer-architecture)
 - [Multi-task learning](#multi-task-learning)

::: {.column-margin}
@raschka2023llm outlines some key papers and developments in the context of large language models.  
:::

::: {.column-margin}
@weng2018attention provides an overview of the attention mechanism, and some historical notes for the peculiar keys, values, queries nomenclature used in @vaswani2017attention.
:::

::: {.column-margin}
@bloem2019transformers provides a more pedagogical view of the transformer architecture, and has nice visualizations as well.
:::

### Generative sequence modeling

Language has a natural sequential structure. The generative modeling task is to learn a probability distribution over sequences of words. 

Let different words be specific values of $n$ random variables. Then the distribution we'd like to learn from data is $p(s_1, s_2, ..., s_n)$. The sequential ordering motivates a particular factorization of this joint distribution:

$$p(s_1, s_2, ..., s_n) = p(s_1) p(s_2 | s_1) p(s_3 | s_1, s_2) ... p(s_n | s_1, s_2, ..., s_{n-1})$$

It's important to understand that the notion of 'words' is arbitrary for such modeling. Even for language, one can break up words at the character or even byte level. This idea is important for notions of multi-task learning and foundational models.

When generating new sequences, we are recursively sampling particular conditional distributions, e.g. $p(s_i | s_1, ... s_{i-1})$ to ultimately look like we are sampling from $p(s_i, ... s_n | s_{i-k}, ..., s_{i-1})$.

This looks like $p(\textrm{outputs}|\textrm{inputs})$. We'll revisit this view in the section on multi-task learning. 

::: {.column-margin}
@phuong2022formal describe transformer-based-algorithms without implementation details, instead focusing on descriptions with consistent and formal notation. With the maze of transformer variants that exist, this resource offers a coherent picture of the landscape.
:::

### Attention mechanism and transformer architecture

I mainly want to highlight how attention captures dependence of the output on particular entries of the input sequence. This bare-bones version of self-attention includes no bias terms, no scaling of the dot products etc., that are part of standard implementations and described well in @phuong2022formal.

::: {.column-margin}
@smola2019attn motivates the attention mechanism with kernel regression.
:::

::: {.column-margin}
@schlag2021linear link attention to fast weight programming. They also explore the linear version of attention in the context of associative memory and explore memory capacity of such models with different choices of kernels, activation functions, etc., connecting to a rich literature on such investigations dating all the way back to @cover1965geometrical.
:::

Let length of the sequence $T$. We denote the $i$-th input as ${x_i \in \mathbb{R}^{d_{\textrm{in}}}}$. Similarly the $j$-th output ${y_j \in \mathbb{R}^{d_{\textrm{out}}}}$. Matrices ${K, Q \in \mathbb{R}^{d_\textrm{key-query} \times d_{\textrm{in}}}}$ and ${V \in \mathbb{R}^{d_\textrm{out} \times d_{\textrm{in}}}}$ are trainable weights. 

$$
\begin{aligned}
k_i, q_i, v_i &= Kx_i, Qx_i, Vx_i &\quad k,q \in \mathbb{R}^{d_{\textrm{key-query}}}, v \in \mathbb{R}^{d_{out}} \\
A_{ij} &= k_i^\top q_j &\quad A \in \mathbb{R}^{T \times T}\\
B_{ij} &= {\exp (A_{ij})} / {\sum_k \exp (A_{ik})} \\
y_i &= \sum_j{B_{ij}v_j} 
\end{aligned}
$$

$A$ is known as the attention matrix. Setting certain entries of the $A_{ij}$ to zero is referred to as masking, and it has the effect of preventing multiplicative interaction between the $i$-th and $j$-th inputs in constructing the $i$-th output.

For language, setting the upper triangular entries (excluding the diagonal) to zero is a common choice. This mask pattern ensures that $y_i$ is only based on previous and current inputs of the sequence, i.e. $(x_1, ..., x_{i})$ (causality for temporal sequences). Since the product of lower triangular matrices remains lower triangular, stacking attention layers with this mask pattern retains such dependence throughout - which can be useful depending on the use case.  

I expect that choice of particular masking patterns and sparsity of the attention matrix (and of $K$, $Q$, $V$ matrices) are important questions for specific applications. 

In the absence of masking, each output takes on the form $y_j  = \sum_i{g_j(x_i)}$, for some function $g_j$. This form renders it invariant to ordering i.e. permutations of the input sequence. 

::: {.column-margin}
@zaheer2017deep have shown that for universal function approximators (e.g. neural networks) $h$ and $g$, a function $f$ of the form $$f(x_1, ... x_n) = h(\sum_i{g(x_i)})$$ is a universal approximator for any permutation invariant function.
:::

Other than self-attention, the decoder-only transformer architecture includes residual connections, layer normalization. These conceptually straightforward operations turn out to be crucial in practice to prevent gradients from exploding or vanishing. Language models also share weights between the embedding layer `(vocabulary_size x embedding_dimension)`. It remains to be seen if such tricks could be helpful for other applications.

The transformer in @vaswani2017attention also has an encoding arm that is used for cross-attention, without masking. This makes intuitive sense for tasks like translation where the sequence of words in english may not be relevant for sequence of words in a german translation. In general, cross-attention is used to provide context from a different source.

::: {.column-margin}
I highly recommend the GPT implementation walkthrough by Karpathy [-@karpathy2023llm]. He's a skilled teacher and watching him live-code is instructive.
:::

### Multi-task learning

In one version (e.g. GPT and variants), these transformer models are trained to simply predict the next word. That is, the model learns distributions $p(s_i | s_1, ..., s_{i-1})$ for all $i \in (1,...,n)$.

So far, a clear reason for why large language models seem magical is missing. Why bother increasing size of models, why invest so much in collecting the data and compute resources?

I think @radford2019language, and later @brown2020language provide this missing reason. The main realization is that for language, the task itself is a sequence of tokens. Tasks can take the form of "translate to german" or "write python code for" etc. Here the sequence of words that specify the task are themselves part of the input sequence.

Moreover, the underlying input does not have to be "words", but instead can be more parcellated and abstract, e.g. [byte pair encodings](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt).

The implication is that while we learn $p(\textrm{output}|\textrm{input})$, we are also learning ${p(\textrm{output} | \textrm{input}, \textrm{task})}$, without needing task-specific architecture or training!

In my view, this is a central requirement for a meaningful notion of foundational models. The implicit multi-task learning that takes place is the reason why non-trivial zero shot performance is possible on a variety of tasks, even though the task is not explicitly specified while training.

### Foundational models

The term "foundational models" was popularized by @bommasani2021opportunities, and an accompanying [workshop at Stanford University](https://crfm.stanford.edu/workshop.html). 

@bommasani2021opportunities describe the notion of emergence (generalization over tasks, and therefore related to zero-shot performance) and homogenization (learning something useful for a variety of downstream tasks, and therefore the pre-training is amenable to fine-tuning). These essentially echo the central observations of @radford2019language and @brown2020language.

The idea of fine-tuning and using pre-trained model components on related datasets or tasks (e.g. transfer learning) has been a motivation behind efforts to curate such models in various domains, even before large language models became common e.g. [https://pytorch.org/hub/](https://pytorch.org/hub/). Unlike the multi-task learning in large language models, the scope of tasks for which pre-trained models may be useful in other domains is usually limited.

### Towards scientific applications

The output of language models can be directly parsed by humans. Let's say we ask a language model to write science fiction based on a fictional planet, for which we make up the rules of physics. The output from these models can be directly parsed, and evaluated for quality.

This fact is exploited for public facing language models such as chatGPT. In this talk titled the [State of GPT @karpathy2023stategpt], Karpathy describes finetuning, reward modeling, and reinforcement learning using the reward model; all of these hinge on the fact that humans can understand and evaluate outputs of such language models.

This is usually not the case for scientific applications. Models trained on vast amounts of data can certainly make predictions, but when to trust and invest in such predictions (for scientific applications) is generally a very hard, unsolved problem for now in my opinion.

### Transformers for single-cell resolution datasets

The most developed, least noisy single-cell resolution _-omics_ datasets are from transcriptomics. For most analyses, the dataset is a table where rows are cells, columns are genes, and entries are the gene expression values. Additional metadata, e.g. disease state of the donor from which the cell was sampled, perturbations applied to the cell etc. may be part of the dataset.

Typical tasks include determining identifiable cell types, specifying differentially expressed genes across conditions (aging, disease, perturbations), inferring developmental trajectories of particular cell types, identifying gene regulatory networks etc.

The issue that plagues so many of these analyses are related modeling of noise in such measurements, curse of high-dimensionality, absence of established ground-truth, and lack of mathematically precise definition of tasks.

::: {.column-margin}
The team at [OpenProblems](https://openproblems.bio/events/) have continued to push for standardization and benchmarking through formal descriptions of tasks, e.g. @luecken2021sandbox. Still, the output of such efforts (both in terms of benchmarking datasets and state-of-the-art models) seems to have had limited influence on subsequent academic research.
:::

Transcriptomic data contain neither a natural sequential ordering, nor a large enough corpus of tasks to draw parallels with emergence and homogenization described above.

Recent studies applying transformer-based models to single cell genomics consider similar pre-training tasks and model architectures. 

 - @le2021transformer
 - @yang2022scbert
 - @connell2022genellm
 - @shen2023generative
 - @ma2023single
 - @cui2022scformer, rebranded as @cui2023scgpt
 - @theodoris2023
 - @hao2023foundation
 - @gong2023xtrimogene

 (to be continued)

### References

::: {#refs}
:::