---
title: "Kernel regression"
author: "Rohan Gala"
date: "2023-06-01"
categories: [math]
image: "image.jpg"
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
---

#### Kernel regression

$$
\gdef\data{\textrm{data}}
$$

The Watson-Nadaraya estimator offers an intuitive approach to intuitively understand kernel regression. Let's say the training data consists of $(x,y)$ pairs. When a query $x_q$ is provided, one calculates distance of $x_q$ from all the $x$ values in the training data. In the simplest version, we can just return the $y$ value for the closest $x$ in the training data.

 - This requires a notion of distance (or similarity) between $x$'s.
 - One can expect this to work pretty well if the sampling density is high.

If the $y$'s are noisy, one might think to take a weighted sum of $y$'s corresponding to the $x$'s that are close $x_q$. The weighting scheme is a choice, and could be based on the distance between $x$ and $x_q$ - with points $x$'s closer to $x_q$ contributing more to the sum. 

There are different distance metrics one may consider for this. 

Choosing a way to calculate distance, and a weighting scheme that uses it is the core idea behind kernel regression. 



In choosing these, we can introduce hyperparameters that control the distance and weighting scheme. The notion of distance, similarity, inner-products are closely related, and it'll be compare their formal definitions, and even memorize them to keep them handy.

---

For a linear binary classifier (perceptron), the weight vector lies in the same subspace as the one spanned by the data. Classification is achieved by projecting the data vectors on to the weight vector and reading out the sign of the projection.

---


#### References

::: {#refs}
:::