---
title: "Gradients through stochastic nodes"
author: "Rohan Gala"
date: "2023-07-03"
date-modified: last-modified
categories: ["machine learning"]
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
---

Calculating gradients through nodes in a computational graph that involve sampling from a distribution (for which we want to learn parameters) is non-trivial. Intuitively, this is because the samples of the distribution don't maintain a dependency with the distribution parameters. This prevents application of the chain rule used for backpropagation to obtain gradients w.r.t the distribution parameters.

There are two main strategies to get around this, one involving _score function_ estimators, and another involving reparametrization. See @schulman2015gradient for a more formal treatment.

Roughly, the score function estimator-based approach is more general but more noisy. It leads to a surrogate loss function specified at the stochastic node, which permits gradient updates to the parameters of the distribution.

The reparametrization "trick" (when possible) allows reformulating the distribution such that distribution parameters become part of a deterministic function applied to samples obtained from a fixed distribution. 

### The set up

Consider a stochastic node, where we sample values $x$ of a random variable $X$ from a distribution $p_{X|\phi}$ and $\phi$ are distribution parameters. Note that $\phi$ which themselves be specified by an upstream neural network.

Let's also assume a deterministic and differentiable function $f_{\theta}$ with parameters $\theta$ applied to samples $x$, and we are interested in parameters $\theta$ and $\phi$ that optimize an objective that consists of an average over the samples drawn from $p_{X|\phi}$ as follows:

$$
\begin{aligned}
\mathcal{L}(\theta, \phi) &= \mathbb{E}_{x \sim p_{X|\phi}}[f_{\theta}(x)] = \int{f_{\theta}(x)p_{\phi}(x)dx}
\end{aligned}
$$ {#eq-objective}

### Updates to $\theta$

First we evaluate the gradient $\nabla_{\theta}\mathcal{L}$ required to update parameters $\theta$ of the deterministic function $f_{\theta}$:
$$
\begin{aligned}
\nabla_{\theta}\mathcal{L} &= \nabla_{\theta}\mathbb{E}_{x \sim p_{X|\phi}}[f_{\theta}(x)] \\
&=  \int{\nabla_{\theta} f_{\theta}(x) p_{\phi}(x)dx} \\
&=  \mathbb{E}_{x \sim p_{X|\phi}}[\nabla_{\theta}f_{\theta}(x)]
\end{aligned}
$$ {#eq-gradtheta}

We see that the gradient in [Eq. @eq-gradtheta] can be cast as an expectation over samples from the distribution $p_{X|\phi}$. So we can simply draw samples from the distribution and estimate the gradient, because $\nabla_{\theta}f_{\theta}$ is well-defined. In the extreme case, the estimate (albeit a noisy one) is obtained simply from a single sample $x$. So far so good.

### Updates to $\phi$ 

$$
\begin{aligned}
\nabla_{\phi}\mathcal{L} &= \nabla_{\phi}\mathbb{E}_{x \sim p_{X|\phi}}[f_{\theta}(x)] \\
&= \int{f_{\theta}(x) \nabla_{\phi} p_{\phi}(x)dx}
\end{aligned}
$$ {#eq-gradphi}

It may be tempting think of [Eq. @eq-gradphi] as $\mathbb{E}_{x \sim \nabla_{\phi} p_{X|\phi}}[f_{\theta}(x)]$ in an analogous manner - this would be wrong, because $\nabla_{\phi} p_{\phi}(x)$ not a valid probability density function.

:::{.column-margin}
A valid probability density function should be non-negative everywhere, and integrate to unity over its domain.
:::

Instead we'll use an identity that is often referred to as the log-derivative trick to rewrite [Eq. @eq-gradphi] as follows:

:::{.column-margin}
Log-derivative trick:
$$
\nabla_{x}h(x) = h(x)\nabla_x{\log(x)}
$$
:::

$$
\begin{aligned}
\int{f_{\theta}(x) \nabla_{\phi} p_{\phi}(x)dx} &= \int{(f_{\theta}(x)  \nabla_{\phi} \log{p_{\phi}(x)}) p_{\phi}(x) dx} \\
\implies \nabla_{\phi}\mathcal{L} &=\mathbb{E}_{x \sim p_{X|\phi}}[f_{\theta}(x) \nabla_{\phi}{\log{p_{\phi}(x)}}] 
\end{aligned}
$$ {#eq-score}

[Eq. @eq-score] shows that the gradient is once again over samples from the distribution $p_{X|\phi}$. As before, in the extreme case this may be estimated with a single sample.

If we view $p_{\phi}(x)$ as a likelihood function, then $\nabla_{\phi}{\log{p_{\phi}(x)}}$ is referred to as the _score function_. The estimate of the gradient includes the score function, and so it is referred to as a _score function estimator_.

This estimator suggests $\mathcal{L}_{\textrm{surrogate}}=~\mathbb{E}_{x \sim p_{X|\phi}}{[f_{\theta}(x)\log{p_{\phi}(x)}]}$ as a surrogate loss function where $f_{\theta}(x)$ is treated as a constant (because it does not depend on $\phi$).

It is worth thinking about the extreme case, when $f_{\theta}(x)$ itself is a constant independent of $x$. This would suggest that the parameter $\phi$ does not influence the loss, and so we would expect the gradient for $\phi$ to be zero in expectation.

This is indeed the case, 

:::{.column-margin}
The bias and variance over different samples of the data are an important characterization of estimators. The score function is of relevance here, e.g. see these [Lecture notes](https://cseweb.ucsd.edu/~elkan/291winter2005/lect09.pdf) and discussion about the Cram√©r-Rao bound.
:::

Another way to estimate the gradient through stochastic nodes is to use the _reparametrization trick_. This is possible when the distribution $p_{X|\phi}$ can be composed with a deterministic function $g_\psi$ applied to samples $z$ drawn from a fixed distribution $p_{Z}$. 

### References

::: {#refs}
:::