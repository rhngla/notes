---
title: "Variational Autoencoders"
author: Rohan Gala
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
bibliography: ../refs.bib

format:
  html:
    theme: journal
    code-fold: true
    html-math-method: katex

editor:
    render-on-save: true
---

There is a class of inference problems for which we use approximate methods. Approximate methods are grouped into 
1. deterministic approximations, which include maximum likelihood and variational methods
2. Monte Carlo methods

Some resources I used to compile this post:

 - [Matthew Bernstein's blog](https://mbernste.github.io/posts/vae/)
 - [Yuge Shi's blog](https://yugeten.github.io/posts/2020/06/elbo/)
 - [Adam Kosiorek's blog](http://akosiorek.github.io/what_is_wrong_with_vaes/)
 - @hoffman2016elbo

Notation:

Latent variable models:

We view individual data points as particular values $x$ of random variable $X$ drawn from it's distribution $p_X$.

_Evidence_ refers to probability of the data under the model, $p_X(x)$. Using the low of total probability:

$$p_X(x) = \int p_{X,Z}(x, z) dz = \int p_{X|Z}(x|z) p_Z(z) dz$${#eq-total-prob}

We do not know $p(z)$ or $p(x|z)$. Integrating over $z$ is intractable for many problems. 

We introduce a known distribution $q(z)$, which is supposed to be our best guess of the true distribution $p(z)$. 

Then we introduce parameters $\theta$ to specify the dependence of $X$ on $Z$ $p(x|z)$ in a flexible manner $p(x|z;\theta)$ (equivalently written as $p_{\theta}(x|z)$). 

Then:

$$
\begin{aligned}
p(x) &= \int p_{\theta}(x|z) p(z) \frac{q(z)}{q(z)} dz \\
\log p(x) &= \log \int p_{\theta}(x|z) p(z) \frac{q(z)}{q(z)} dz \\
&=  \log \mathbf{E}_{z \sim q} \bigg[ \frac{p_{\theta}(x|z) p(z)}{q(z)} \bigg] \\
&\geq \mathbf{E}_{z \sim q} \log  \bigg[ \frac{p_{\theta}(x|z) p(z)}{q(z)} \bigg] \\
&\geq \mathbf{E}_{z \sim q} [ \log  p_{\theta}(x|z)] - D_{KL}(q(z)\|p(z)) 
\end{aligned}
$$

::: {.column-margin}
This appears in the margin
:::

# Named section {#sec-named_sec}

### References

::: {#refs}
:::