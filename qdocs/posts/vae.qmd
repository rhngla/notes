
---
title: "Variational Autoencoders"
author: "Rohan Gala"
date: "2023-06-20"
date-modified: last-modified
categories: ["math"]
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
format:
  html:
    theme: cosmo
    css: styles.css
---

\gdef\colb#1{\textcolor{63a7ff}{#1}}
\gdef\colr#1{\textcolor{ff6e63}{#1}}
\gdef\colg#1{\textcolor{40bf9a}{#1}}
\gdef\argmax{\textrm{argmax}\,}
\gdef\Eqz{\mathbb{E}_{\mathbf{z}\sim q(\mathbf{z})}}
\gdef\Eqzi{\mathbb{E}_{{z_i}\sim q(z_i)}}


### Background
 
 - An earlier post on [entropy and KL divergence](https://rhngla.github.io/notes/posts/information.html), would be helpful context for this post. Notation conventions vary across communities, and can cause confusion. 
 - This [post on notation](https://rhngla.github.io/notes/posts/notation_dist.html) may be helpful.
 - The Expectation maximization (EM) algorithm is closely related predecessor, see [`CS229` lecture notes](https://cs229.stanford.edu/notes2020spring/cs229-notes8.pdf) and related discussion in @minka1998expectation.
 - Variational inference (VI) is a generalization of the EM algorithm, see @blei2017variational. See this [toy implementation](https://towardsdatascience.com/variational-inference-the-basics-f70ac511bcea) of a VI algorithm
 - Variational autoencoders are a special case of stochastic variational inference


### EM algorithm

**Formulation**

::: {.customtable style="font-size: 90%; width: 80%; margin: auto auto;"}


| Notation         |       What we mean                               |
|:--------:        |:------------------:                              |
| $X_i$            | r.v. for $i$<sup>th</sup> observation            |
| $\mathbf{X}$     | vector of r.v.'s $(X_1, X_2, \ldots, X_n)$       |
| $x_i$            | value for $i$<sup>th</sup> observation           |
| $\mathbf{x}$     | vector of values $(x_1, x_2, \ldots, x_n)$       |
| $Z_i$            | r.v. for unobserved component of $i$<sup>th</sup> observation |
| $\mathbf{z}$     | set given by $\{z_1, z_2, \ldots, z_n \}$        |
| $q(\mathbf{z})$  | distribution for $\mathbf{z}$                    |
| $q(z_i)$         | distribution for $z_i$                           |
:::

**Set up**

 - We have observations of high-dimensional data $(x_1, \cdots, x_n)$
 - Assuming i.i.d observations the following consequences:
      1. independence: $p(X_1, \cdots , X_n)$ factorizes as $\prod_i{p(X_i)}$. 
      2. identically distributed: each $p(X_i)$ has the same underlying parameters $\theta$
 
 - Then according to the maximum likelihood principle learning this distribution amounts to obtaining $\theta_{\tt{MLE}} = \underset{\theta}{\argmax} \sum_i{\log p(X_i=x_i ; \theta)}$

**The challenge**

 - We think that the data has underlying explanatory factors $Z$.
 - $p(X_i=x_i ; \theta)$ is not easy to evaluate


\begin{aligned}
\log {p(x;\theta)} &= \log \int {p(x,z) dz}  \\
\log {p(x;\theta)} &= \log \int {p(x|z) p(z) dz} \quad &&\text{Bayes rule} \\

\end{aligned}


We don't have access to $p(z)$. So we insert a different $z$ distribution, $q(z|x)$. This is motivated by viewing $p(x|z)$ as the decoder, and $q(z|x)$ as the encoder.

::: {.column-margin}
Introducing a distribution for the integration variable allows us to view the integral as an expectation over that distribution. 
:::

$$
\begin{aligned}
\log {p(x;\theta)} &= \log \int {p(x|z) p(z) \frac{q(z|x)}{q(z|x)} dz}  \\
                   &=  \log E_{z \sim q(.|x)} \bigg[ \frac{p(x|z) p(z)}{q(z|x)} \bigg] \\
                   &\geq E_{z \sim q(.|x)} \log  \bigg[ \frac{p(x|z) p(z)}{q(z|x)} \bigg] \quad &&\text{Jensen's inequality} \\
                   &\geq E_{z \sim q(.|x)} [ \log  p(x|z)] - D_{KL}(q(z|x)\|p(z)) \quad &&\text{ELBO}
\end{aligned}
$$

::: {.column-margin}
Expection within $\log$ is a recurring clue in different contexts to use Jensen's inequality.
:::




[comment]: Link to Manifold hypothesis.

  - We think that the high dimensional data $X$ depend on low dimensional factors $Z$
  - These low dimensional factors can be viewed as a representation of the data






We can approach deriving the variational infrence objective from two perspectives:
 - Representation learning $p(z|x)$
 - Missing data $p(x,z)$

:::{}
 - We are interested in $\theta$ and unobserved $z_i$.
 - Maximum likelihood estimate for the parameters $\theta_{\tt{MLE}} = \underset{\theta}{\argmax} \log p(\mathbf{x} ; \theta)$
 - Given: computing $p(x_i ; \theta)$ is difficult but computing $p(x_i, z_i ; \theta)$ is easy.
 - But $z_i$ is unobserved, so we must get around that somehow.
:::

:::{.callout-note title="Assuming observations are i.i.d." collapse=false icon=false}

We can view $\mathbf{x}$ as the _training set_. Then likelihood $p(\mathbf{x} ; \theta)$ is the joint density $p(x_1, x_2, \ldots, x_n ; \theta)$. 

 - Independence $\implies$ joint distribution is a product of the individual distributions.
 - Identically distributed $\implies$ the distribution for all $X_i$'s is the same.
 - In other words, each value $x_i$ can be treated as a draw from a distribution of a single random variable $X$, that is parameterized by the same $\theta$.

$$
\begin{aligned}
p(\mathbf{x} ; \theta) &= \prod_{i=1}^n p(x_i ; \theta) \\
\log p(\mathbf{x} ; \theta) &= \sum_{i=1}^n \log p(x_i ; \theta)
\end{aligned}
$$

The maximum likelihood estimate for $\theta$ can then be simplified to a sum of log-densities at individual observations $x_i$:

$$
\theta_{\tt{MLE}} = \underset{\theta}{\argmax} (\sum_{i=1}^n \log p(x_i ; \theta))
$$

This can be viewed as the Monte Carlo estimate of an expectation over the data distribution $p(X; \theta)$.
$$
\theta_{\tt{MLE}} = \underset{\theta}{\argmax} E_{x \sim p(X;\theta)}( \log p(x ; \theta) )
$$
:::

For any value $x$ we would want to know the corresponding $z$. So we assume some distribution for $z$ and work with expectations. 

\begin{aligned}
\log p(x ; \theta) &= \log \int_{z} p(x, z ; \theta) dz &\\
                   &= \log \int_{z} \frac{p(x, z ; \theta) q(z)}{q(z) } dz &\\
                   &= \log E_{z \sim q(z)} \bigg[ \frac{p(x, z ; \theta)}{q( z)} \bigg] & \\
\end{aligned}

We pause here and make some observations. 

1. Writing a lower bound for the log likelihood, aka ELBO. 
\begin{aligned}
\log E_{z \sim q(z)} \bigg[ \frac{p(x, z ; \theta)}{q( z)} \bigg] &\geq E_{z \sim q(z)} \log \bigg[ \frac{p(x, z ; \theta)}{q(z)} \bigg] &\quad \textrm{Jensen's inequality} \\
\end{aligned}

We can use Baye's rule to write $p(x, z ; \theta)$ as a product of $p(z|x; \theta)$ and $p(x ; \theta)$. Then equality holds if $q(z) = p(z|x; \theta)$, where $p(z|x; \theta)$ is the 'true' posterior distribution of $z$ given $x$.

2. Introducing the KL divergence between $q(z)$ and $p(z|x; \theta)$

\begin{aligned}
\log E_{z \sim q(z)} \bigg[ \frac{p(x, z ; \theta)}{q( z)} \bigg] &= E_{z \sim q(z)} \log \bigg[ \frac{p(x, z ; \theta)}{q(z)} \bigg] \\
&= E_{z \sim q(z)} \log p(x, z ; \theta) - E_{z \sim q(z)} \log q(z) \\
&= \mathcal{L}(q, \theta) + D_{\textrm{KL}}(q(z) | p(z|x; \theta)) \\
\end{aligned}







$$
\begin{aligned}
\underbrace{p(x ; \theta)}_{\text{does not depend on } z} &= 
\quad \underbrace{p(x, z ; \theta)}_{\text{easy to compute}} \quad / 
\underbrace{p(z|x; \theta)}_{q(z)\text{ approximates this}} \quad &&\text{Bayes rule} \\
\\
\log{p(x; \theta)} &= \log p(x, z ; \theta)  - \log p(\mathbf{z}|\mathbf{x}; \theta) \quad &&\text{log transform} \\
\Eqz \log{p(\mathbf{x}|\theta)} &= \Eqz \log p(\mathbf{x}, \mathbf{z} | \theta)  - \Eqz \log  p(\mathbf{z}|\mathbf{x},\theta)  \quad &&\text{expectation over}~q(\mathbf{z})  \\
\implies \log{p(\mathbf{x}|\theta)} &= \underbrace{\Eqz \log p(\mathbf{x}, \mathbf{z} | \theta)}_{\mathcal{L}(q,\theta)} + \underbrace{D_{\textrm{KL}}(q(\mathbf{z}) | p(\mathbf{z}|\mathbf{x},\theta)}_{\geq 0}
\end{aligned}
$$

 - Thus, maximizing $\mathcal{L}(q,\theta)$ offers a lower bound for $\log{p(\mathbf{x}|\theta)}$.
 - The $\theta$ learnt in this way is a local maximum of the likelihood function.



**Algorithm**

:::{}
 - guess an initial $\theta$.
 - iteratively perform the E and M steps until convergence into a local maximum.
 - In the E step, update $q$ to maximizes $\mathcal{L}(q,\theta)$ at current $\theta$.
 - In the M step, update $\theta$ to maximize $\mathcal{L}(q,\theta)$ at current $q$.
 - This iterative scheme is guaranteed to converge to a local maximum.
:::

**Common simplifications and approximations**

 - i.i.d samples $x_i$ $\implies p(\mathbf{x} | \theta) = \prod_{i=1}^n p(x_i| \theta)$
 - mean field approximation $\implies q(\mathbf{z}) = \prod_{i=1}^n q(z_i)$
 - These approximations decoupled the objective for EM steps across data points. 

$$
\begin{aligned}
\log{p(\mathbf{x}|\theta)} = \sum_i \log{p(x_i|\theta)} &= \sum_{i=1}^n [ \Eqzi \log p(x_i, z_i | \theta)  + D_{\textrm{KL}}(q(z_i) | p(z_i|x_i,\theta) ] \\
&= \sum_{i=1}^n \mathcal{L}(q(z_i),\theta)
\end{aligned}
$$

:::{.callout-note title="Choice of $q$" collapse=false icon=false}

The notation $q(z_i)$ is can be confusing here. Here we explicitly mean the density of $z_i$ in the distribution $q$.

However, once can model q as a conditional distribution, and indicate it as $q(z_i|x_i)$. This is a common choice in variational inference, and is used in the VAE as well.

Answer: In the gaussian mixtures case, it is indeed shared across all $z_i$. In a VAE, it is different for each $z_i$, parametrized by a neural network and $x_i$.
:::


### Set up
 
- We have access to data observations $x$ drawn from distribution $p_X$.
- We think that this data distribution depends on a different random variable $Z$.
- Alternatively, we can think that the data can be _represented_ by a different random variable $Z$.

<p></p>

- In a probabilistic sense, we'd like to know the distribution $p_{Z|X}$.
- Learning such a representation (of the data) means we seek a mapping from $X$ to $Z$.
- We cannot directly observe $Z$, and so we want to _infer_ it.
- The distribution $p_{Z|X}$ is called the _true posterior_.
- The goal is to approximate $p_{Z|X}$.

<p></p>

- To be _Bayesian_ means to formally specify our beliefs about unknowns, i.e. $Z$ here.
- We do this by specifying a distribution $p_Z$ referred to as the _prior_ for $Z$.
- This prior reflects our belief about the representation.
- We can view this prior as a _regularizer_ in the optimization sense.
- Here we focus on low dimensional representation $Z$ of high dimensional data $X$. 

 
### Variational and amortized inference

<p></p> 
:::{.column-margin}
E.g. If we choose $\mathcal{Q}$ to be the family of Gaussian distributions, then $q_{Z;\omega}$ is a particular gaussian distribution specified through $\omega = (\mu, \Sigma)$.
:::

 - Let $q_{Z;\omega}$ be our approximation of $p_{Z|X}$.
 - Here $\omega$ are parameters (i.e. we don't bother with a prior distribution for $\omega$).
 - The family of distributions parametrized by $\omega$ is denoted by $\mathcal{Q}$.
 - We call this _variational_ inference because the parameters to approximate the posterior (within $\mathcal{Q}$) are learnt through optimization.

<p></p>

 - For the VAE, these parameters are chosen to be data dependent, i.e. $\omega = f(X)$.
 - A single $f$ is learned across all observations $x$ - this is called _amortized_ inference.
 - Since $q_{Z;\omega}$ depends on $X$ through $\omega = f(X)$, we can view it as $q_{Z|X}$.


<p></p>
 - We now swap out some notation, and denote distributions using densities.
 - We do this to match notation commonly found in machine learning papers. 
 - So instead of $p_X, p_{Z|X}$ etc., we'll use $p(x), p(z|x)$ etc.
 - In VAE, a neural network with weights $\phi$ specifies $f$.
 - We indicate this in $q(z|x)$ as a subscript by writing $q_{\phi}(z|x)$.

<p></p>
 
 
 - At any point, we can draw samples from $q_{\phi}(z|x)$.
 - We would like this to be close to the _true posterior_ distribution $p(z|x)$.
 - $D_\textrm{KL}(q_{\phi}(z|x) | p(z|x))$ is a measure of how far $q(z|x)$ is from $p(z|x)$.
 - $p(z|x)$ is inaccessible to us (that is what we are trying to approximate!)
 - So we rearrange terms to get something we can work with.

<p></p>

 - In doing so, we will see that we need access to $p(x|z)$
 - We don't know $p(x|z)$ either, but we can learn this simultaneously.
 - We assume a form $p_{\psi}(x|z)$, where $\psi$ are parameters of the assumed distribution.
 - The standard VAE takes $\psi$ to be a deterministic function $g$ of $Z$.
 - $g$ is a neural network with weights $\theta$, and shared across all $z$. 
 - We therefore write $p_{\psi}(x|z)$ as $p_{\theta}(x|z)$.

<p></p>

 - Our basic modeling assumption is that $X$ is generated by $Z$.
 - Therefore the density $p_{\theta}(x|z)$ should be large for observed data points $x$.
 - In other words, the likelihood (of $\theta$ and $z$) should be high for all observed $x$.
 - Observed data $x$ is itself noisy (e.g. camera noise in image data, _dropout_ in scRNA-seq measurements).
 - The form of $p_{\theta}(x|z)$ can be chosen to model noise in the data.
 - $p_{\theta}(x|z)$ can thus be thought of as an observation model.

:::{.callout-note title="Fully factorized Gaussian observation model" collapse=false icon=false}

We use $x \in \mathbb{R}^n$ to denote one sample, and $x_i$ to denote the $i^{th}$ feature.

We must then define an observation model to calculate the expected likelihood of eq **xxx**. We also usually work with log transformed likelihoods (notice that it is the log likelihood that appears in the ELBO eq **xxx**). There are two commonly encountered modeling choices:

1. The observation model is fully factorizable, i.e. $p(x|z) = \prod_{i=1}^n p(x_i|z)$. That is, components $x_i$ are conditionally independent given $z$.
2. Each $p(x_i|z)$ is a gaussian distribution, with unit variance, i.e. $p(x_i|z) = \mathcal{N}(\mu_i, 1)$.

We do not know the true parameters $\mu_i$, and these must be learned. So what we learn are _parameters of the observation model_, $\mu \in \mathbb{R}^n$ in this case.

Now to calculate the expected log likelihood, we plug in the $x$ into the observation model and calculate the expectation based on samples of $z$ drawn from the approximate posterior $q_{\phi}(z|x)$.

For the fully factorized gaussian with unit variance observation model, the expected log likelihood can be expressed as:

$$
\begin{aligned}
E_{q_{\phi}(z|x)} (\log p(x|z)) &= E_{q_{\phi}(z|x)} \bigg[ \sum_{i=1}^n \log p(x_i|z) \bigg] \\
&= E_{q_{\phi}(z|x)} \bigg[ \sum_{i=1}^n \log \mathcal{N}(x_i|\mu_i, 1) \bigg] \\
&= E_{q_{\phi}(z|x)} \bigg[ \sum_{i=1}^n -\frac{1}{2} \log(2\pi) - \frac{1}{2} (x_i - \mu_i)^2 \bigg] \\
&= -\frac{n}{2} \log(2\pi) - \frac{n}{2} E_{q_{\phi}(z|x)} \underbrace{\colb{\bigg[\frac{1}{n} \lVert x - \mu \rVert^2 \bigg]}}_{\text{mean squared error}}
\end{aligned}
$$

This is the familiar looking mean squared error calculated across features of the data.

With stochastic learning algorithms, we maximize the expected ELBO over the data distribution (i.e. training samples).

:::

 - The reverse KL divergence $D_\textrm{KL}(q(z|x) | p(z|x))$ is not the same as the forward KL divergence $D_\textrm{KL}(p(z|x) | q(z|x))$. 
 - A practical reason to use the reverse version us that we can sample from $q(z|x)$, but not $p(z|x)$.

:::

\begin{aligned}
D_\textrm{KL}(q_{\phi}(z|x)|p(z|x)) &= E_{q_{\phi}(z|x)} (-\log{p(z|x)}) - E_{q_{\phi}(z|x)} (-\log{q_{\phi}(z|x)}) \\
                             &= E_{q_{\phi}(z|x)} (-\log{\frac{\colr{p(x|z)}\colb{p(z)}}{\colg{p(x)}}}) - E_{q_{\phi}(z|x)} (-\log{\colb{q_{\phi}(z|x)}}) \\
                             &= D_\textrm{KL}(\colb{q_{\phi}(z|x)|p(z)}) - E_{q_{\phi}(z|x)} (\log{\colr{p(x|z)}}) - \log{\colg{p(x)}} \\
\end{aligned}

 - $D_\textrm{KL}(\colb{q_{\phi}(z|x)|p(z)})$ is a KL divergence between our approximate posterior and the prior. 
 - $E_{q_{\phi}(z|x)} (\log{\colr{p(x|z)}})$ is the expected likelihood of the generative model parameters.
 - In fact, $\colr{p(x|z)}$ the generative model that takes $z$ as input and generates $x$.
 - We don't know this, but we can learn this simultaneously. 
 - We parametrize this $p(x|z)$
 - $\log{\colg{p(x)}}$ is log evidence. 
 - log evidence cannot be computed, since it requires access to the data distribution $p$.

To minimize the $D_\textrm{KL}(q_{\phi}(z|x)|p(z|x))$, we then:

 - minimize $D_\textrm{KL}(\colb{q_{\phi}(z|x)|p(z)})$
 - maximize $E_{q_{\phi}(z|x)} (\log{\colr{p(x|z)}})$
 - ignore $\log{\colg{p(x)}}$ (since it does not depend on $z$)

:::{.callout-note collapse=false icon=false}
The term $E_{q(z|x)} (\log{\colr{p(x|z)}}) - D_\textrm{KL}(\colb{q(z|x)|p(z)})$ is given the name _evidence lower bound_ (ELBO). To see why, we can think about the problem from a different perspective.

Consider the `model` to be $p$, parameterized by $\theta$. Then the evidence is given by:

\begin{aligned}
\log p(x) &= \log \int p(x|z) p(z) dz \\
&= \log sE_{z \sim p(z)} \bigg[ p(x|z) \bigg] \\
&\geq E_{z \sim p(z)} \log  \bigg[ p(x|z) \bigg] \\
&\geq E_{z \sim p(z)} [ \log  p(x|z)] - D_{KL}(p(z)\|q(z))
\end{aligned}

:::

<p></p>

 Quoting part of the introduction in @blei2017variational. 

 > A Bayesian model draws the latent variables from a prior density $p(z)$ and then relates them to the observations through the likelihood $p(x|z)$. Inference in a Bayesian model amounts to conditioning on data and computing the posterior $p(z | x)$. In complex Bayesian models, this computation often requires approximate inference.

 >Rather than use sampling, the main idea behind variational inference is to use optimization. First, we posit a family of approximate densities $\mathcal{Q}$. This is a set of densities over the latent variables. Then, we try to find the member of that family that minimizes the Kullback-Leibler (KL) divergence to the exact posterior, $q∗(z) = \argmin {D_{\textrm{KL}}(q(z) | p(z | x))}$

See Eqn 8 in https://www.jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf, and equivalence to the KL



 - Problem set up:
 - We have access to observations $x$, that come from some unknown distribution $p_X$.
 - We assume there is a simpler distribution $p(z)$ that generates the data $x$ through a mapping specified by $p(x|z)$.
 - $z$ is a latent variable

<p></p>

 - We want to infer the latent variable $z$ that generated $x$
 given the data, what is the latent variable that generated it?
 The goal is to approximate the posterior density $p(z|x)$ with a tunable distribution $q_{\phi}(z|x)$
 - We assume that $Z$ is the random variable that generates data $X$
 - Further, $Z$ is a latent variable (not observed directly)
 - We have have access to or control prior density $p(z)$, the likelihood $p(x|z)$, and family of distributions $q_{\phi}(z|x)$

<p></p>

 - By Bayes rule, $p(z|x) = \frac{p(x|z)p(z)}{p(x)}$
 - Further, $p(x) = \int {p(x|z)p(z) dz}$
 - In general, this integral is intractable (how to proceed when it is actually tractable?)

<p></p>

 - For two distributions $p$ and $q$, the KL divergence can be viewed as a measure of how different $p$ and $q$ are.
 - The definition of KL divergence is not symmetric
 - We'll consider $D_\textrm{KL}(q(z|x)|p(z|x))$
 - Remember, the goal is to tune a family of distributions $q_{\phi}(z|x)$ such that it is close to $p(z|x)$





$$
D(q_{\phi}(z|x)|p(z|x)) = - E_{q_{\phi}(z|x)} (\log {p(x|z)}) {\color{#758EB7} + D(q_{\phi}(z|x)|p(z))} {\color{#C06C84} + \log {p(x)}}
$$

How we parametrize $\phi$ expressive we make $q_{\phi}$



 - In other words, we want a model we want to sample from, and the samples should look like the data
 - We of course have samples of _some_ data


There is a class of inference problems for which we use approximate methods. Approximate methods are grouped into 

1. Deterministic approximations
    - Maximum likelihood estimation 
    - Variational inference
2. Monte Carlo methods

Some resources I used to compile this post:

 - [Jaan Altosaar's blog](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/): This is one of the clearest blogposts for those with less of a background in probabilistic methodology.
 - [Dustin Tran](https://dustintran.com/blog/variational-auto-encoders-do-not-train-complex-generative-models): I found this to be an insightful comment about expressivity in generative models trained through the VAE framework.
 - [Yuge Shi's blog](https://yugeten.github.io/posts/2020/06/elbo/) She shares newer developments, and has also published a multi-modal version of VAEs.
 
 - [Eric Jang's blog](https://blog.evjang.com/2016/08/variational-bayes.html). 
 
 - [Adam Kosiorek's blog](http://akosiorek.github.io/what_is_wrong_with_vaes/)
 - [Matthew Bernstein's blog](https://mbernste.github.io/posts/vae/)
 - @hoffman2016elbo
 - [Black box variational inference](http://proceedings.mlr.press/v33/ranganath14)
 - [Schemes other than ELBO](https://bochang.me/blog/posts/iwae/)
 - [Andriy Mnih lecture](https://www.youtube.com/watch?v=7Pcvdo4EJeo&t=544s): This is a good lecture with an overview of where Latent variable models are placed within the broader context of machine learning. Highly recommend this as an advanced revision of the topic. 
- https://jaketae.github.io/study/gan-math/

I'll use the following notational convention: 

 | Notation     |       What we mean     |
 |:--------:    |:------------------:    |
 | $p(x)$       | $p_{X}(X=x)$           |
 | $q(x)$       | $q_{X}(X=x)$           |
 | $p(x|z)$     | $p_{X|Z}(X=x|Z=z)$     |
 | $p(x,z)$     | $p_{X,Z}(X=x,Z=z)$     |
 | $E_{q(z|x)}$ | $E_{z \sim q_{Z|X=x}}$ |


::: {.callout-note}
According notation indicated in the table, for a continuous random variable $X$, $p(x)$ is a probability density evaluated at the point $X=x$ based on the distribution $p_X$.

In blog posts, research papers and even textbooks, $p(x)$ is used to refer to either the probability density $p_X(X=x)$, or the distribution itself $p_X$ depending on context.

This eventually becomes easier to parse from context, but was a source of confusion for me. Such [_abuse of notation_](https://statmodeling.stat.columbia.edu/2020/02/05/abuse-of-expectation-notation/) is unfortunately common in literature.
:::

We view individual data points as particular values $x$ of random variable $X$ drawn from it's distribution $p_X$, indicated as $x \sim p_X$. 

When $p_X$ is being modeled by some distribution with parameters $\theta$, we'll write $p_{X|\theta}$. Here we'll refer to parameter as objects whose values are we want to determine, e.g. through an optimization procedure.

When we are trying to estimate the distribution, but only have access to a bunch of samples, it makes sense to think of the $p(X=x| \theta)$ as _evidence_. 


In such a scenario, it makes sense to refer to the likelihood of observing _Evidence_ refers to probability of the data under the model, $p_X(x)$. Using the law of total probability:

$$p_X(x) = \int p_{X,Z}(x, z) dz = \int p_{X|Z}(x|z) p_Z(z) dz$${#eq-total-prob}

We do not know $p_Z$ or $p_{X|Z}$. Integrating over $z$ is intractable for many problems. 

We introduce a known distribution $q(z)$, which will be our best guess of the true distribution $p(z)$. 

Then we introduce parameters $\theta$ to specify the dependence of $X$ on $Z$, i.e. $p_X|Z$ in a flexible manner $p_{X|Z;\theta}(x|z;\theta)$ (equivalently written as $p_{\theta}(x|z)$). 

Then:

$$
\begin{aligned}
p(x) &= \int p_{\theta}(x|z) p(z) \frac{q(z)}{q(z)} dz \\
\log p(x) &= \log \int p_{\theta}(x|z) p(z) \frac{q(z)}{q(z)} dz \\
&=  \log \mathbf{E}_{z \sim q} \bigg[ \frac{p_{\theta}(x|z) p(z)}{q(z)} \bigg] \\
&\geq \mathbf{E}_{z \sim q} \log  \bigg[ \frac{p_{\theta}(x|z) p(z)}{q(z)} \bigg] \\
&\geq \mathbf{E}_{z \sim q} [ \log  p_{\theta}(x|z)] - D_{KL}(q(z)\|p(z)) 
\end{aligned}
$$

### Further reading
- [A unified view of different objectives in latent variable generative models](https://arxiv.org/pdf/1806.06514)

### References

::: {#refs}
:::