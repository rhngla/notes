
---
title: "Variational Autoencoders"
author: "Rohan Gala"
date: "2023-06-20"
date-modified: last-modified
categories: ["math"]
toc: true
number-sections: true
number-depth: 2
highlight-style: pygments
html-math-method: katex
bibliography: "../refs.bib"
code-fold: true
format:
  html:
    theme: cosmo
    css: styles.css
---

\gdef\colb#1{\textcolor{63a7ff}{#1}}
\gdef\colr#1{\textcolor{ff6e63}{#1}}
\gdef\colg#1{\textcolor{40bf9a}{#1}}


An earlier post on [entropy and KL divergence](https://rhngla.github.io/notes/posts/information.html), would be helpful context for this post about the variational autoencoder formulation. Notation conventions used in probabilistic machine learning papers vary quite a bit, and this [note](https://rhngla.github.io/notes/posts/notation_dist.html) documents resolution to my confusion around this.

### Set up
 
- We have access to data observations $x$ drawn from distribution $p_X$.
- We think that this data distribution depends on a different random variable $Z$.
- Alternatively, we can think that the data can be _represented_ by a different random variable $Z$.

<p></p>

- In a probabilistic sense, we'd like to know the distribution $p_{Z|X}$.
- Learning such a representation (of the data) means we seek a mapping from $X$ to $Z$.
- We cannot directly observe $Z$, and so we want to _infer_ it.
- The distribution $p_{Z|X}$ is called the _true posterior_.
- The goal is to approximate $p_{Z|X}$.

<p></p>

- To be _Bayesian_ means to formally specify our beliefs about unknowns, i.e. $Z$ here.
- We do this by specifying a distribution $p_Z$ referred to as the _prior_ for $Z$.
- This prior reflects our belief about the representation.
- We can view this prior as a _regularizer_ in the optimization sense.
- Here we focus on low dimensional representation $Z$ of high dimensional data $X$. 

 
### Variational and amortized inference

<p></p> 
:::{.column-margin}
E.g. If we choose $\mathcal{Q}$ to be the family of Gaussian distributions, then $q_{Z;\omega}$ is a particular gaussian distribution specified through $\omega = (\mu, \Sigma)$.
:::

 - Let $q_{Z;\omega}$ be our approximation of $p_{Z|X}$.
 - Here $\omega$ are parameters (i.e. we don't bother with a prior distribution for $\omega$).
 - The family of distributions parametrized by $\omega$ is denoted by $\mathcal{Q}$.
 - We call this _variational_ inference because the parameters to approximate the posterior are learnt through optimization.

<p></p>

 - For the VAE, these parameters are chosen to be data dependent, i.e. $\omega = f(X)$.
 - A single $f$ is learned across all observations $x$ - this is called _amortized_ inference.
 - Since $q_{Z;\omega}$ depends on $X$ through $\omega = f(X)$, we can view it as $q_{Z|X}$.


<p></p>
 - We now swap out some notation, and denote distributions using densities.
 - We do this to match notation commonly found in machine learning papers. 
 - So instead of $p_X, p_{Z|X}$ etc., we'll use $p(x), p(z|x)$ etc.
 - In VAE, a neural network with weights $\phi$ specifies $f$.
 - We indicate this in $q(z|x)$ as a subscript by writing $q_{\phi}(z|x)$.

<p></p>
 
 
 - At any point, we can draw samples from $q_{\phi}(z|x)$.
 - We would like this to be close to the _true posterior_ distribution $p(z|x)$.
 - $D_\textrm{KL}(q_{\phi}(z|x) | p(z|x))$ is a measure of how far $q(z|x)$ is from $p(z|x)$.
 - $p(z|x)$ is inaccessible to us (that is what we are trying to approximate!)
 - So we rearrange terms to get something we can work with.

<p></p>

 - In doing so, we will see that we need access to $p(x|z)$
 - This is the likelihood of the generative model parameters.
 - We don't know p(x|z) either, but we can learn this simultaneously.

The term $p(x|z)$ is nuanced, and it goes against intuition if we've mainly worked with _mean squared error_ as a _reconstruction loss_. I'll first explain the idea in words, which might be a bit hard to follow but I'd encourage reading it first even if it does not make complete sense at first pass. Take a look at the example, and revisiting the second time around will hopefully help understand it better. 

We first present a likelihood perspective.

 - The distribution $p(x|z)$ (here the notation actually refers to $p_{X|Z}$) is unknown. 
 - We assume a form $p_{\psi}(x|z)$, where $\psi$ are parameters of the assumed distribution.
 - Our basic modeling assumption is that $X$ is generated by $Z$.
 - Therefore the density $p_{\psi}(x|z)$ should be large for observed data points $x$.
 - In other words, the likelihood (of $\psi$ and $z$) should be high for observed $x$.

<p></p>

Now we present a perspective that accounts for noise in the data.

 - Observed data $x$ is itself noisy (e.g. camera noise in image data, _dropout_ in scRNA-seq measurements).
 - The form of $p_{\psi}(x|z)$ can be chosen to model noise in the data.
 - $p_{\psi}(x|z)$ can thus be thought of as an observation model.



:::{.callout-note title="Fully factorized Gaussian observation model" collapse=false icon=false}

We use $x \in \mathbb{R}^n$ to denote one sample, and $x_i$ to denote the $i^{th}$ feature.

We must then define an observation model to calculate the expected likelihood of eq **xxx**. We also usually work with log transformed likelihoods (notice that it is the log likelihood that appears in the ELBO eq **xxx**). There are two commonly encountered modeling choices:

1. The observation model is fully factorizable, i.e. $p(x|z) = \prod_{i=1}^n p(x_i|z)$. That is, components $x_i$ are conditionally independent given $z$.
2. Each $p(x_i|z)$ is a gaussian distribution, with unit variance, i.e. $p(x_i|z) = \mathcal{N}(\mu_i, 1)$.

We do not know the true parameters $\mu_i$, and these must be learned. So what we learn are _parameters of the observation model_, $\mu \in \mathbb{R}^n$ in this case.

Now to calculate the expected log likelihood, we plug in the $x$ into the observation model and calculate the expectation based on samples of $z$ drawn from the approximate posterior $q_{\phi}(z|x)$.

For the fully factorized gaussian with unit variance observation model, the expected log likelihood can be expressed as:

$$
\begin{aligned}
E_{q_{\phi}(z|x)} (\log p(x|z)) &= E_{q_{\phi}(z|x)} \bigg[ \sum_{i=1}^n \log p(x_i|z) \bigg] \\
&= E_{q_{\phi}(z|x)} \bigg[ \sum_{i=1}^n \log \mathcal{N}(x_i|\mu_i, 1) \bigg] \\
&= E_{q_{\phi}(z|x)} \bigg[ \sum_{i=1}^n -\frac{1}{2} \log(2\pi) - \frac{1}{2} (x_i - \mu_i)^2 \bigg] \\
&= -\frac{n}{2} \log(2\pi) - \frac{n}{2} E_{q_{\phi}(z|x)} \underbrace{\colb{\bigg[\frac{1}{n} \lVert x - \mu \rVert^2 \bigg]}}_{\text{mean squared error}}
\end{aligned}
$$

This is the familiar looking mean squared error calculated across features of the data. 

With stochastic learning algorithms, we maximize the expected ELBO over the data distribution (i.e. training samples).
:::

KL divergence is not symmetric; 

 - $D_\textrm{KL}(q(z|x) | p(z|x))$ is known as the reverse KL divergence
 - $D_\textrm{KL}(p(z|x) | q(z|x))$ is known as the forward KL divergence. 

Why use one over the other? A practical reason here is that the reverse KL divergence leads to averaging over samples from $q(z|x)$, which we can access easily; in the forward KL divergence case, we'd have to work with samples from $p(z|x)$ which is unknown and inaccessible.

:::

\begin{aligned}
D_\textrm{KL}(q_{\phi}(z|x)|p(z|x)) &= E_{q_{\phi}(z|x)} (-\log{p(z|x)}) - E_{q_{\phi}(z|x)} (-\log{q_{\phi}(z|x)}) \\
                             &= E_{q_{\phi}(z|x)} (-\log{\frac{\colr{p(x|z)}\colb{p(z)}}{\colg{p(x)}}}) - E_{q_{\phi}(z|x)} (-\log{\colb{q_{\phi}(z|x)}}) \\
                             &= D_\textrm{KL}(\colb{q_{\phi}(z|x)|p(z)}) - E_{q_{\phi}(z|x)} (\log{\colr{p(x|z)}}) - \log{\colg{p(x)}} \\
\end{aligned}

 - $D_\textrm{KL}(\colb{q_{\phi}(z|x)|p(z)})$ is a KL divergence between our approximate posterior and the prior. 
 - $E_{q_{\phi}(z|x)} (\log{\colr{p(x|z)}})$ is the expected likelihood of the generative model parameters.
 - In fact, $\colr{p(x|z)}$ the generative model that takes $z$ as input and generates $x$.
 - We don't know this, but we can learn this simultaneously. 
 - We parametrize this $p(x|z)$
 - $\log{\colg{p(x)}}$ is log evidence. 
 - log evidence cannot be computed, since it requires access to the data distribution $p$.

To minimize the $D_\textrm{KL}(q_{\phi}(z|x)|p(z|x))$, we then:

 - minimize $D_\textrm{KL}(\colb{q_{\phi}(z|x)|p(z)})$
 - maximize $E_{q_{\phi}(z|x)} (\log{\colr{p(x|z)}})$
 - ignore $\log{\colg{p(x)}}$ (since it does not depend on $z$)

:::{.callout-note collapse=false icon=false}
The term $E_{q(z|x)} (\log{\colr{p(x|z)}}) - D_\textrm{KL}(\colb{q(z|x)|p(z)})$ is given the name _evidence lower bound_ (ELBO). To see why, we can think about the problem from a different perspective.

Consider the `model` to be $p$, parameterized by $\theta$. Then the evidence is given by:

\begin{aligned}
\log p(x) &= \log \int p(x|z) p(z) dz \\
&= \log sE_{z \sim p(z)} \bigg[ p(x|z) \bigg] \\
&\geq E_{z \sim p(z)} \log  \bigg[ p(x|z) \bigg] \\
&\geq E_{z \sim p(z)} [ \log  p(x|z)] - D_{KL}(p(z)\|q(z))
\end{aligned}

:::

<p></p>

 Quoting part of the introduction in @blei2017variational. 

 > A Bayesian model draws the latent variables from a prior density $p(z)$ and then relates them to the observations through the likelihood $p(x|z)$. Inference in a Bayesian model amounts to conditioning on data and computing the posterior $p(z | x)$. In complex Bayesian models, this computation often requires approximate inference.

 >Rather than use sampling, the main idea behind variational inference is to use optimization. First, we posit a family of approximate densities $\mathcal{Q}$. This is a set of densities over the latent variables. Then, we try to find the member of that family that minimizes the Kullback-Leibler (KL) divergence to the exact posterior, $qâˆ—(z) = \argmin {D_{\textrm{KL}}(q(z) | p(z | x))}$

See Eqn 8 in https://www.jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf, and equivalence to the KL



 - Problem set up:
 - We have access to observations $x$, that come from some unknown distribution $p_X$.
 - We assume there is a simpler distribution $p(z)$ that generates the data $x$ through a mapping specified by $p(x|z)$.
 - $z$ is a latent variable

<p></p>

 - We want to infer the latent variable $z$ that generated $x$
 given the data, what is the latent variable that generated it?
 The goal is to approximate the posterior density $p(z|x)$ with a tunable distribution $q_{\phi}(z|x)$
 - We assume that $Z$ is the random variable that generates data $X$
 - Further, $Z$ is a latent variable (not observed directly)
 - We have have access to or control prior density $p(z)$, the likelihood $p(x|z)$, and family of distributions $q_{\phi}(z|x)$

<p></p>

 - By Bayes rule, $p(z|x) = \frac{p(x|z)p(z)}{p(x)}$
 - Further, $p(x) = \int {p(x|z)p(z) dz}$
 - In general, this integral is intractable (how to proceed when it is actually tractable?)

<p></p>

 - For two distributions $p$ and $q$, the KL divergence can be viewed as a measure of how different $p$ and $q$ are.
 - The definition of KL divergence is not symmetric
 - We'll consider $D_\textrm{KL}(q(z|x)|p(z|x))$
 - Remember, the goal is to tune a family of distributions $q_{\phi}(z|x)$ such that it is close to $p(z|x)$





$$
D(q_{\phi}(z|x)|p(z|x)) = - E_{q_{\phi}(z|x)} (\log {p(x|z)}) {\color{#758EB7} + D(q_{\phi}(z|x)|p(z))} {\color{#C06C84} + \log {p(x)}}
$$

How we parametrize $\phi$ expressive we make $q_{\phi}$



 - In other words, we want a model we want to sample from, and the samples should look like the data
 - We of course have samples of _some_ data


There is a class of inference problems for which we use approximate methods. Approximate methods are grouped into 

1. Deterministic approximations
    - Maximum likelihood estimation 
    - Variational inference
2. Monte Carlo methods

Some resources I used to compile this post:

 - [Jaan Altosaar's blog](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)
 - [Eric Jang's blog](https://blog.evjang.com/2016/08/variational-bayes.html)
 - [Yuge Shi's blog](https://yugeten.github.io/posts/2020/06/elbo/)
 - [Adam Kosiorek's blog](http://akosiorek.github.io/what_is_wrong_with_vaes/)
 - [Matthew Bernstein's blog](https://mbernste.github.io/posts/vae/)
 - @hoffman2016elbo
 - [Black box variational inference](http://proceedings.mlr.press/v33/ranganath14)
 - [Schemes other than ELBO](https://bochang.me/blog/posts/iwae/)
 - [Andriy Mnih lecture](https://www.youtube.com/watch?v=7Pcvdo4EJeo&t=544s): This is a good lecture with an overview of where Latent variable models are placed within the broader context of machine learning. Highly recommend this as an advanced revision of the topic. 

I'll use the following notational convention: 

 | Notation     |       What we mean     |
 |:--------:    |:------------------:    |
 | $p(x)$       | $p_{X}(X=x)$           |
 | $q(x)$       | $q_{X}(X=x)$           |
 | $p(x|z)$     | $p_{X|Z}(X=x|Z=z)$     |
 | $p(x,z)$     | $p_{X,Z}(X=x,Z=z)$     |
 | $E_{q(z|x)}$ | $E_{z \sim q_{Z|X=x}}$ |


::: {.callout-note}
According notation indicated in the table, for a continuous random variable $X$, $p(x)$ is a probability density evaluated at the point $X=x$ based on the distribution $p_X$.

In blog posts, research papers and even textbooks, $p(x)$ is used to refer to either the probability density $p_X(X=x)$, or the distribution itself $p_X$ depending on context.

This eventually becomes easier to parse from context, but was a source of confusion for me. Such [_abuse of notation_](https://statmodeling.stat.columbia.edu/2020/02/05/abuse-of-expectation-notation/) is unfortunately common in literature.
:::

We view individual data points as particular values $x$ of random variable $X$ drawn from it's distribution $p_X$, indicated as $x \sim p_X$. 

When $p_X$ is being modeled by some distribution with parameters $\theta$, we'll write $p_{X|\theta}$. Here we'll refer to parameter as objects whose values are we want to determine, e.g. through an optimization procedure.

When we are trying to estimate the distribution, but only have access to a bunch of samples, it makes sense to think of the $p(X=x| \theta)$ as _evidence_. 


In such a scenario, it makes sense to refer to the likelihood of observing _Evidence_ refers to probability of the data under the model, $p_X(x)$. Using the law of total probability:

$$p_X(x) = \int p_{X,Z}(x, z) dz = \int p_{X|Z}(x|z) p_Z(z) dz$${#eq-total-prob}

We do not know $p_Z$ or $p_{X|Z}$. Integrating over $z$ is intractable for many problems. 

We introduce a known distribution $q(z)$, which will be our best guess of the true distribution $p(z)$. 

Then we introduce parameters $\theta$ to specify the dependence of $X$ on $Z$, i.e. $p_X|Z$ in a flexible manner $p_{X|Z;\theta}(x|z;\theta)$ (equivalently written as $p_{\theta}(x|z)$). 

Then:

$$
\begin{aligned}
p(x) &= \int p_{\theta}(x|z) p(z) \frac{q(z)}{q(z)} dz \\
\log p(x) &= \log \int p_{\theta}(x|z) p(z) \frac{q(z)}{q(z)} dz \\
&=  \log \mathbf{E}_{z \sim q} \bigg[ \frac{p_{\theta}(x|z) p(z)}{q(z)} \bigg] \\
&\geq \mathbf{E}_{z \sim q} \log  \bigg[ \frac{p_{\theta}(x|z) p(z)}{q(z)} \bigg] \\
&\geq \mathbf{E}_{z \sim q} [ \log  p_{\theta}(x|z)] - D_{KL}(q(z)\|p(z)) 
\end{aligned}
$$


This paper explores the use of AI to augment and enhance music education by enabling personalized learning experiences tailored to a human learner's unique interests, skills, and goals. Here we focus on generating curricula and related exercises that adapt to the learner's intent and progress in real time. We present a new ear training application that can analyze and reduce piano scores to generate piano method books for the modern age. Weaving the learner's current tastes and interests into the fabric of their education in this way lowers barriers, supports effort, and has the potential to fuel the new wave of creative expression.


### References

::: {#refs}
:::